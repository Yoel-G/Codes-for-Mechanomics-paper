{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoel-G/Codes-for-Mechanomics-paper/blob/main/Population_analysis_model_training_testing_and_batch_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author information:\n",
        "Yoel Goldstein, Lab of Prof. Ofra Benny\n",
        "Hebrew University, School of Pharmacy\n",
        "\n",
        "This code uses FACS data to assess the classification performance in distinguishing between cancer cell populations with varying phenotypes by aggregating X number of cell values.\n",
        "\n",
        "The code is generic and can be used for all cell lines. The current code uses H460 cell types as en example.\n",
        "\n",
        "Once the models are trained they are saved as a pickle.\n",
        "\n",
        "For excuting the code you can use any kind of data for any cell line:\n",
        "* cells with particle\n",
        "* cells without particles\n",
        "* Batch samples\n",
        "\n",
        "It does require to fit some parts of the preprocessing of the code to adjust it specifically to the type of the data, such as: number of samples and thresholds for cleaning.  \n",
        "\n",
        "Code description:\n",
        "*   Requires file directory pointing to a folder containing raw .csv files\n",
        "*   Includes preprocessing of the raw data: cleaning, log transformation, and normalization.  \n",
        "*  Multiple datasets are examined, including: physiological parameters alone (SSC and FSC), physiological parameters divided into bins, cell size alone (FSC), cell granularity alone (SSC), particle uptake parameters alone, particle uptake divided into bins, and a combined dataset containing all parameters (physiological and particle uptake).  \n",
        "*   The code includes Random forest, XGBoost and SVM classifiers and study their performance in classifying between two cell subpopulations.  \n",
        "*   Unseen data as a second blind test (Batch data)\n",
        "*   PCA based dimeention reduction and its effect on the performances\n",
        "\n",
        "\n",
        "To analyze only one pasrticle uptake run only the rows and sections of the specific particle\n",
        "\n"
      ],
      "metadata": {
        "id": "K3bRFJek5sT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P2pJEdN61A_"
      },
      "outputs": [],
      "source": [
        "pip install eli5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXGg6fES61BB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pylab import *\n",
        "import eli5\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "from pathlib import Path\n",
        "from pandas import DataFrame\n",
        "from collections import Counter\n",
        "from functools import reduce\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "sns.set_style('white')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB531ZnqfCyN",
        "outputId": "a0d6207f-cce8-484e-ad4a-83fd70bcac55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = r'' # enter the correct path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# functions for analysis"
      ],
      "metadata": {
        "id": "kO9YjCkOnknT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiwV40GZ61BB"
      },
      "outputs": [],
      "source": [
        "# getting the index of the row in the dataset\n",
        "def get_index(df):\n",
        "    index = df.index\n",
        "    t = index.array\n",
        "    t_dict = Counter(t).most_common()\n",
        "    t_dict = dict(t_dict)\n",
        "    t_keys = t_dict.keys()\n",
        "    return (t_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_18aO9Fx61BC"
      },
      "outputs": [],
      "source": [
        "# splitting the dataset into groups and returning for each row its group number\n",
        "# choos the number of cells per group\n",
        "def split_df(index):\n",
        "    #pop = (floor(index/100) + 1).astype(int)\n",
        "    #pop = (floor(index/200) + 1).astype(int)\n",
        "    pop = (floor(index/500) + 1).astype(int)\n",
        "    #pop = (floor(index/50) + 1).astype(int)\n",
        "    return pd.Series([(pop)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DNI5pGR8FON"
      },
      "outputs": [],
      "source": [
        "# split the uptake intensity range into 11 bins and return a list of these bins.\n",
        "# For each row, this function defaults to a value of 0, except for the bin where the intensity falls,\n",
        "# which is assigned a value of 1. It returns 11 columns of 0 and 1.\n",
        "\n",
        "def bins (intensity):\n",
        "    bins_list = [0,0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "    if intensity < 4:\n",
        "        bins_list[0] = 1\n",
        "    elif intensity >= 4 and intensity < 5:\n",
        "        bins_list[1] = 1\n",
        "    elif intensity >=5 and intensity < 6:\n",
        "        bins_list[2] = 1\n",
        "    elif intensity >=6 and intensity < 7:\n",
        "        bins_list[3] = 1\n",
        "    elif intensity >=7 and intensity < 8:\n",
        "        bins_list[4] = 1\n",
        "    elif intensity >=8 and intensity < 9:\n",
        "        bins_list[5] = 1\n",
        "    elif intensity >= 9 and intensity < 10:\n",
        "        bins_list[6] = 1\n",
        "    elif intensity >= 10 and intensity < 10.5:\n",
        "        bins_list[7] = 1\n",
        "    elif intensity >= 10.5 and intensity < 11:\n",
        "        bins_list[8] = 1\n",
        "    elif intensity >= 11 and intensity < 11.5:\n",
        "        bins_list[9] = 1\n",
        "    else:\n",
        "        bins_list[10] = 1\n",
        "\n",
        "    return pd.Series([bins_list[0],bins_list[1],bins_list[2],bins_list[3],bins_list[4],bins_list[5],bins_list[6],\n",
        "                     bins_list[7], bins_list[8], bins_list[9], bins_list[10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq6Dz5he61BC"
      },
      "outputs": [],
      "source": [
        "# split the pyshiological (size (FSC) and granularity (SSC)) range into 7 bins and return a list of these bins.\n",
        "# For each row, this function defaults to a value of 0, except for the bin where the intensity falls,\n",
        "# which is assigned a value of 1. It returns 7 columns of 0 and 1.\n",
        "\n",
        "def bins_phys (intensity):\n",
        "    bins_list = [0,0,0,0,0,0,0]\n",
        "\n",
        "    if intensity < 10:\n",
        "        bins_list[0] = 1\n",
        "    elif intensity >= 10 and intensity < 10.5:\n",
        "        bins_list[1] = 1\n",
        "    elif intensity >= 10.5 and intensity < 11:\n",
        "        bins_list[2] = 1\n",
        "    elif intensity >= 11 and intensity < 11.5:\n",
        "        bins_list[3] = 1\n",
        "    elif intensity >= 11.5 and intensity < 12:\n",
        "        bins_list[4] = 1\n",
        "    elif intensity >= 12 and intensity < 12.5:\n",
        "        bins_list[5] = 1\n",
        "    else:\n",
        "        bins_list[6] = 1\n",
        "\n",
        "    return pd.Series([bins_list[0],bins_list[1],bins_list[2],bins_list[3],bins_list[4],bins_list[5],bins_list[6]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKilaFmp61BC"
      },
      "outputs": [],
      "source": [
        "# multiplying the row uptake intensity with the bins coloumns (0 or 1)\n",
        "def bins_intensity (intensity, A_1, A_2, A_3, A_4, A_5, A_6, A_7, A_8, A_9, A_10, A_11):\n",
        "    return pd.Series([intensity*A_1, intensity*A_2, intensity*A_3, intensity*A_4,\n",
        "                     intensity*A_5, intensity*A_6, intensity*A_7, intensity*A_8,\n",
        "                     intensity*A_9, intensity*A_10, intensity*A_11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4kw0bIa61BD"
      },
      "outputs": [],
      "source": [
        "# multiplying the row physiological intensity with the bins coloumns (0 or 1)\n",
        "def bins_intensity_phys (intensity, A_1, A_2, A_3, A_4, A_5, A_6, A_7):\n",
        "    return pd.Series([intensity*A_1, intensity*A_2, intensity*A_3, intensity*A_4,\n",
        "                     intensity*A_5, intensity*A_6, intensity*A_7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WolRpwoK61BD"
      },
      "outputs": [],
      "source": [
        "# normalizing the uptake intensities with the mean intansity of the group\n",
        "def normalized_intensity (intensity_1, intensity_2, intensity_3, intensity_4, intensity_5, intensity_6, intensity_7,intensity_8,intensity_9,intensity_10,intensity_11,mean_A):\n",
        "    return pd.Series([intensity_1/mean_A, intensity_2/mean_A, intensity_3/mean_A, intensity_4/mean_A,\n",
        "                      intensity_5/mean_A, intensity_6/mean_A, intensity_7/mean_A, intensity_8/mean_A,\n",
        "                      intensity_9/mean_A, intensity_10/mean_A, intensity_11/mean_A])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybclsE0H61BD"
      },
      "outputs": [],
      "source": [
        "# normalizing the physiological intensities with the mean intansity of the group\n",
        "def normalized_intensity_phys (intensity_1, intensity_2, intensity_3, intensity_4, intensity_5, intensity_6, intensity_7, mean_A):\n",
        "    return pd.Series([intensity_1/mean_A, intensity_2/mean_A, intensity_3/mean_A, intensity_4/mean_A,\n",
        "                      intensity_5/mean_A, intensity_6/mean_A, intensity_7/mean_A])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6bBKozW61BD"
      },
      "outputs": [],
      "source": [
        "# finds and returns the group mean uptake intensities of each bin\n",
        "def average (intensity_1, intensity_2, intensity_3, intensity_4, intensity_5, intensity_6, intensity_7,intensity_8,\n",
        "             intensity_9, intensity_10, intensity_11, count_1, count_2, count_3, count_4, count_5, count_6, count_7,\n",
        "             count_8, count_9, count_10, count_11):\n",
        "    return pd.Series([intensity_1/count_1, intensity_2/count_2, intensity_3/count_3, intensity_4/count_4,\n",
        "                      intensity_5/count_5, intensity_6/count_6, intensity_7/count_7, intensity_8/count_8,\n",
        "                      intensity_9/count_9, intensity_10/count_10, intensity_11/count_11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGyppC5q61BE"
      },
      "outputs": [],
      "source": [
        "# finds and returns the group mean physiological intensities of each bin\n",
        "def average_phys (intensity_1, intensity_2, intensity_3, intensity_4, intensity_5, intensity_6, intensity_7,\n",
        "             count_1, count_2, count_3, count_4, count_5, count_6, count_7):\n",
        "    return pd.Series([intensity_1/count_1, intensity_2/count_2, intensity_3/count_3, intensity_4/count_4,\n",
        "                      intensity_5/count_5, intensity_6/count_6, intensity_7/count_7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RZgkalKUdhV"
      },
      "outputs": [],
      "source": [
        "# finds and returns mising values\n",
        "def fix_median (pop_number_origin, pop_number, intensity):\n",
        "    pop_dict = {pop_number[i]: intensity[i] for i in range(len(pop_number))}\n",
        "    pop_dict_origin ={}\n",
        "    if pop_number_origin in pop_dict:\n",
        "        pop_dict_origin[pop_number_origin] = pop_dict[pop_number_origin]\n",
        "    else:\n",
        "        pop_dict_origin[pop_number_origin] = 0\n",
        "\n",
        "    return pd.Series(pop_dict_origin[pop_number_origin])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "979qSGwu61BE"
      },
      "outputs": [],
      "source": [
        "# building a function for analysis visualization\n",
        "def heatconmat(y_true,y_pred):\n",
        "    sns.set_context('talk')\n",
        "    plt.figure(figsize=(9,6))\n",
        "    sns.heatmap(confusion_matrix(y_true,y_pred),\n",
        "                annot=True,\n",
        "                fmt='d',\n",
        "                cbar=False,\n",
        "                cmap='gist_earth_r',\n",
        "                yticklabels=sorted(y_true.unique()))\n",
        "    plt.show()\n",
        "    print(classification_report(y_true,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwOU96Al61BE"
      },
      "outputs": [],
      "source": [
        "# displays the model accuracy performance of the training and validation data\n",
        "def display_classification_results(classifier, X_train, y_train, x_val, y_val):\n",
        "    print('Classifier: {0:s}\\nParameters:'.format(classifier.__class__.__name__))\n",
        "    for k,v in classifier.get_params().items():\n",
        "        print('\\t{0} : {1}'.format(k,v))\n",
        "    training_set_score = classifier.score(X_train,y_train)\n",
        "    print('Testing on TRAINING set: {}'.format(training_set_score))\n",
        "    val_set_score = classifier.score(x_val,y_val)\n",
        "    print('Validation on VALIDATION set: {}'.format(val_set_score))\n",
        "    return (training_set_score,val_set_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing -  preparing the data for analysis\n",
        "\n",
        "Here we demonstrate the analysis using the H460 cells."
      ],
      "metadata": {
        "id": "ObsgYpmDtCmH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3wdW7L061BE"
      },
      "outputs": [],
      "source": [
        "# read the H460 cells csv files from directory\n",
        "\n",
        "df_1_H460 = pd.read_csv(data_path +'H460_1.csv')\n",
        "df_2_H460 = pd.read_csv(data_path +'H460_2.csv')\n",
        "df_3_H460 = pd.read_csv(data_path +'H460_3.csv')\n",
        "df_4_H460 = pd.read_csv(data_path +'H460_4.csv')\n",
        "df_5_H460 = pd.read_csv(data_path +'H460_5.csv')\n",
        "df_6_H460 = pd.read_csv(data_path +'H460_6.csv')\n",
        "df_7_H460 = pd.read_csv(data_path +'H460_7.csv')\n",
        "df_8_H460 = pd.read_csv(data_path +'H460_8.csv')\n",
        "df_9_H460 = pd.read_csv(data_path +'H460_9.csv')\n",
        "df_10_H460 = pd.read_csv(data_path +'H460_10.csv')\n",
        "df_11_H460 = pd.read_csv(data_path +'H460_11.csv')\n",
        "df_12_H460 = pd.read_csv(data_path +'H460_12.csv')\n",
        "df_13_H460 = pd.read_csv(data_path +'H460_13.csv')\n",
        "df_14_H460 = pd.read_csv(data_path +'H460_14.csv')\n",
        "df_15_H460 = pd.read_csv(data_path +'H460_15.csv')\n",
        "df_16_H460 = pd.read_csv(data_path +'H460_16.csv')\n",
        "df_17_H460 = pd.read_csv(data_path +'H460_17.csv')\n",
        "df_18_H460 = pd.read_csv(data_path +'H460_18.csv')\n",
        "df_19_H460 = pd.read_csv(data_path +'H460_19.csv')\n",
        "df_20_H460 = pd.read_csv(data_path +'H460_20.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the data containes one sample only for each population use from now on only pop 1 train and pop 2 train and scilence pop 1 test and pop 2 test"
      ],
      "metadata": {
        "id": "IBXnT-Ohqslo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkq0eEDH61BF"
      },
      "outputs": [],
      "source": [
        "# asigning training and testing datasets in the case of multiple samples\n",
        "\n",
        "# training pop 1 - buliding a list with the chosen files for training\n",
        "df_list_pop_1 = [df_1_H460, df_2_H460, df_3_H460, df_4_H460, df_5_H460,\n",
        "          df_6_H460, df_7_H460, df_8_H460, df_9_H460, df_10_H460,\n",
        "          df_11_H460, df_12_H460, df_13_H460, df_14_H460, df_15_H460]\n",
        "\n",
        "# adding the file number - group in each of the training files\n",
        "group = 1\n",
        "for df in df_list_pop_1:\n",
        "    df['group'] = group\n",
        "    group += 1\n",
        "\n",
        "# testing pop 1 - buliding a list with the chosen files for testing\n",
        "df_list_pop_1_test = [df_16_H460, df_17_H460, df_18_H460, df_19_H460, df_20_H460]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case where there is only one sample for the next preprocessing steps use only pop 1"
      ],
      "metadata": {
        "id": "vmChE84CbElP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oxjrtiQ61BF"
      },
      "outputs": [],
      "source": [
        "# read the H460 cis-res cells csv files fro directory\n",
        "\n",
        "df_1_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_1.csv')\n",
        "df_2_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_2.csv')\n",
        "df_3_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_3.csv')\n",
        "df_4_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_4.csv')\n",
        "df_5_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_5.csv')\n",
        "df_6_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_6.csv')\n",
        "df_7_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_7.csv')\n",
        "df_8_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_8.csv')\n",
        "df_9_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_9.csv')\n",
        "df_10_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_10.csv')\n",
        "df_11_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_11.csv')\n",
        "df_12_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_12.csv')\n",
        "df_13_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_13.csv')\n",
        "df_14_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_14.csv')\n",
        "df_15_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_15.csv')\n",
        "df_16_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_16.csv')\n",
        "df_17_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_17.csv')\n",
        "df_18_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_18.csv')\n",
        "df_19_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_19.csv')\n",
        "df_20_H460_cis_res = pd.read_csv(data_path +'H460_cis_res_20.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FkK8KHi61BF"
      },
      "outputs": [],
      "source": [
        "# asigning training and testing datasets in the case of multiple samples\n",
        "\n",
        "# training pop 2 - buliding a list with the chosen files for training\n",
        "df_list_pop_2 = [df_1_H460_cis_res, df_2_H460_cis_res, df_3_H460_cis_res, df_4_H460_cis_res, df_5_H460_cis_res,\n",
        "          df_6_H460_cis_res, df_7_H460_cis_res, df_8_H460_cis_res, df_9_H460_cis_res, df_10_H460_cis_res,\n",
        "          df_11_H460_cis_res, df_12_H460_cis_res, df_13_H460_cis_res, df_14_H460_cis_res, df_15_H460_cis_res]\n",
        "\n",
        "# adding the file number - group in each of the training files\n",
        "for df in df_list_pop_2:\n",
        "    df['group'] = group\n",
        "    group += 1\n",
        "\n",
        "# testing pop 2 - buliding a list with the chosen files for testing\n",
        "df_list_pop_2_test = [df_16_H460_cis_res, df_17_H460_cis_res, df_18_H460_cis_res, df_19_H460_cis_res, df_20_H460_cis_res]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Xy-Q5Y61BG"
      },
      "outputs": [],
      "source": [
        "# merging the datasets into one dataset of training and one dataset of testing for both cell lines\n",
        "# not needed if there is only one sample of each population\n",
        "\n",
        "# merged df train pop 1\n",
        "Total_pop_1 = pd.DataFrame()\n",
        "for df in df_list_pop_1:\n",
        "    Total_pop_1 =  pd.concat([Total_pop_1, df], ignore_index=True)\n",
        "\n",
        "# merged df test pop 1\n",
        "Total_pop_1_test = pd.DataFrame()\n",
        "for df in df_list_pop_1_test:\n",
        "    Total_pop_1_test =  pd.concat([Total_pop_1_test, df], ignore_index=True)\n",
        "\n",
        "\n",
        "# merged df train pop 2\n",
        "Total_pop_2 = pd.DataFrame()\n",
        "for df in df_list_pop_2:\n",
        "    Total_pop_2 =  pd.concat([Total_pop_2, df], ignore_index=True)\n",
        "\n",
        "# merged df test pop 2\n",
        "Total_pop_2_test = pd.DataFrame()\n",
        "for df in df_list_pop_2_test:\n",
        "    Total_pop_2_test =  pd.concat([Total_pop_2_test, df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm1ppcd761BG"
      },
      "outputs": [],
      "source": [
        "# renaming train pop 1\n",
        "Total_pop_1.drop(['Unnamed: 0','Time'], axis = 1, inplace = True)\n",
        "Total_pop_1.rename(columns={\"FSC-A\": \"FSC_A\", \"FSC-H\": \"FSC_H\", \"FSC-W\": \"FSC_W\", \"SSC-A\": \"SSC_A\",\n",
        "                                  \"SSC-H\": \"SSC_H\", \"SSC-W\": \"SSC_W\", \"BV421-A\": \"BV421_A\", \"FITC-A\": \"FITC_A\",\n",
        "                                  \"PerCP-A\": \"PerCP_A\", \"PE-Texas Red-A\": \"PE_Texas_Red_A\", \"BV421-A\": \"BV421_A\",\n",
        "                                   \"Alexa Fluor 700-A\": \"Alexa_Fluor_700_A\"}, inplace = True)\n",
        "\n",
        "# renaming test pop 1\n",
        "Total_pop_1_test.drop(['Unnamed: 0','Time'], axis = 1, inplace = True)\n",
        "Total_pop_1_test.rename(columns={\"FSC-A\": \"FSC_A\", \"FSC-H\": \"FSC_H\", \"FSC-W\": \"FSC_W\", \"SSC-A\": \"SSC_A\",\n",
        "                                  \"SSC-H\": \"SSC_H\", \"SSC-W\": \"SSC_W\", \"BV421-A\": \"BV421_A\", \"FITC-A\": \"FITC_A\",\n",
        "                                  \"PerCP-A\": \"PerCP_A\", \"PE-Texas Red-A\": \"PE_Texas_Red_A\", \"BV421-A\": \"BV421_A\",\n",
        "                                   \"Alexa Fluor 700-A\": \"Alexa_Fluor_700_A\"}, inplace = True)\n",
        "\n",
        "# renaming train pop 2\n",
        "Total_pop_2.drop(['Unnamed: 0','Time'], axis = 1, inplace = True)\n",
        "Total_pop_2.rename(columns={\"FSC-A\": \"FSC_A\", \"FSC-H\": \"FSC_H\", \"FSC-W\": \"FSC_W\", \"SSC-A\": \"SSC_A\",\n",
        "                                  \"SSC-H\": \"SSC_H\", \"SSC-W\": \"SSC_W\", \"BV421-A\": \"BV421_A\", \"FITC-A\": \"FITC_A\",\n",
        "                                  \"PerCP-A\": \"PerCP_A\", \"PE-Texas Red-A\": \"PE_Texas_Red_A\", \"BV421-A\": \"BV421_A\",\n",
        "                                   \"Alexa Fluor 700-A\": \"Alexa_Fluor_700_A\"}, inplace = True)\n",
        "\n",
        "# renaming test pop 2\n",
        "Total_pop_2_test.drop(['Unnamed: 0','Time'], axis = 1, inplace = True)\n",
        "Total_pop_2_test.rename(columns={\"FSC-A\": \"FSC_A\", \"FSC-H\": \"FSC_H\", \"FSC-W\": \"FSC_W\", \"SSC-A\": \"SSC_A\",\n",
        "                                  \"SSC-H\": \"SSC_H\", \"SSC-W\": \"SSC_W\", \"BV421-A\": \"BV421_A\", \"FITC-A\": \"FITC_A\",\n",
        "                                  \"PerCP-A\": \"PerCP_A\", \"PE-Texas Red-A\": \"PE_Texas_Red_A\", \"BV421-A\": \"BV421_A\",\n",
        "                                   \"Alexa Fluor 700-A\": \"Alexa_Fluor_700_A\"}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIma2V7s61BG"
      },
      "outputs": [],
      "source": [
        "# since we merged the files we need to set a new index\n",
        "# not needed if there is only one sample of each population\n",
        "\n",
        "# setting a list of indexes for pop 1\n",
        "index_list_pop_1 = get_index(Total_pop_1['FSC_A'])\n",
        "index_list_pop_1_test = get_index(Total_pop_1_test['FSC_A'])\n",
        "\n",
        "# setting a list of indexes index for pop 2\n",
        "index_list_pop_2 = get_index(Total_pop_2['FSC_A'])\n",
        "index_list_pop_2_test = get_index(Total_pop_2_test['FSC_A'])\n",
        "\n",
        "# assigning index pop 1\n",
        "Total_pop_1['row_index'] = index_list_pop_1\n",
        "Total_pop_1_test['row_index'] = index_list_pop_1_test\n",
        "\n",
        "# assigning index pop 2\n",
        "Total_pop_2['row_index'] = index_list_pop_2\n",
        "Total_pop_2_test['row_index'] = index_list_pop_2_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvDxkyNw61BH"
      },
      "outputs": [],
      "source": [
        "# assigning the data sets to groups of cells (500/200/100/50)\n",
        "\n",
        "#spliting to sub-populations pop 1\n",
        "Total_pop_1[['population_number']] = Total_pop_1.apply(lambda x: split_df(x.row_index), axis=1)\n",
        "Total_pop_1_test[['population_number']] = Total_pop_1_test.apply(lambda x: split_df(x.row_index), axis=1)\n",
        "\n",
        "#spliting to sub-populations pop 2\n",
        "Total_pop_2[['population_number']] = Total_pop_2.apply(lambda x: split_df(x.row_index), axis=1)\n",
        "Total_pop_2_test[['population_number']] = Total_pop_2_test.apply(lambda x: split_df(x.row_index), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the data:\n",
        "\n",
        "this step is not always necessary and depands on the data.\n",
        "the numbers here are good for the H460 cells with particles.\n",
        "\n",
        "* H460 cells without particles:\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-W']< 130000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-W']> 70000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-A']> 10000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-W']> 75000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-A']> 25000]\n",
        "\n",
        "\n",
        "for the A375 cells we used:\n",
        "* with particles:\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-A']> 30000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['FSC']> 25000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-A']> 30000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['FSC_A']> 25000]\n",
        "\n",
        "* without particles:\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['FSC']> 20000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-A']> 15000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['FSC-A']> 23000]\n",
        "\n",
        "\n",
        "for the PC3M cells we used:\n",
        "* with particles:\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-W']< 130000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-W']> 70000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-A']> 20000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-W']> 75000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-W']< 160000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-A']> 15000]\n",
        "\n",
        "* without particles:\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-W']< 130000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-W']> 75000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['SSC-A']> 15000]\n",
        "\n",
        " Total_pop_1 = Total_pop_1[Total_pop_1['FSC-A']> 32500]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-W']> 75000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-W']< 160000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['SSC-A']> 20000]\n",
        "\n",
        " Total_pop_2 = Total_pop_2[Total_pop_2['FSC-A']> 32500]"
      ],
      "metadata": {
        "id": "nCVECFvFyuBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7eme2kWWM1K"
      },
      "outputs": [],
      "source": [
        "# Cleaning the data of the first population based on a preliminary analysis\n",
        "\n",
        "# filtering out doublets and debris pop 1\n",
        "Total_pop_1 = Total_pop_1[Total_pop_1['SSC_W']< 125000]\n",
        "Total_pop_1 = Total_pop_1[Total_pop_1['SSC_W']> 70000]\n",
        "Total_pop_1 = Total_pop_1[Total_pop_1['SSC_A']> 10000]\n",
        "\n",
        "Total_pop_1_test = Total_pop_1_test[Total_pop_1_test['SSC_W']< 125000]\n",
        "Total_pop_1_test = Total_pop_1_test[Total_pop_1_test['SSC_W']> 70000]\n",
        "Total_pop_1_test = Total_pop_1_test[Total_pop_1_test['SSC_A']> 10000]\n",
        "\n",
        "# Cleaning the data of the second population based on a preliminary analysis (the numbers in this example are good for H460 cis-res cells)\n",
        "Total_pop_2 = Total_pop_2[Total_pop_2['SSC_W']> 70000]\n",
        "Total_pop_2 = Total_pop_2[Total_pop_2['SSC_A']> 25000]\n",
        "\n",
        "Total_pop_2_test = Total_pop_2_test[Total_pop_2_test['SSC_W']> 70000]\n",
        "Total_pop_2_test = Total_pop_2_test[Total_pop_2_test['SSC_A']> 25000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAlk2HY261BG"
      },
      "outputs": [],
      "source": [
        "# log transformation of the data\n",
        "\n",
        "# train pop 1\n",
        "Total_pop_1_log = Total_pop_1.transform(log, channels=['FSC_A','FSC_H', 'FSC_W', 'SSC_A', 'SSC_H', 'SSC_W', 'BV421_A', 'FITC_A','PerCP_A', 'PE_Texas_Red_A', 'Alexa_Fluor_700_A'], b=500.0, inplace = True)\n",
        "Total_pop_1_log.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "Total_pop_1_log = Total_pop_1_log.fillna(0)\n",
        "Total_pop_1_log['Cell_line'] = 0\n",
        "Total_pop_1_log.reset_index(inplace = True)\n",
        "group_list_1 = Total_pop_1.group.tolist()\n",
        "Total_pop_1_log['group'] = group_list_1\n",
        "prefeatured_pop_1 = Total_pop_1_log\n",
        "\n",
        "# test pop 1\n",
        "Total_pop_1_test_log = Total_pop_1_test.transform(log, channels=['FSC_A','FSC_H', 'FSC_W', 'SSC_A', 'SSC_H', 'SSC_W', 'BV421_A', 'FITC_A','PerCP_A', 'PE_Texas_Red_A', 'Alexa_Fluor_700_A'], b=500.0, inplace = True)\n",
        "Total_pop_1_test_log.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "Total_pop_1_test_log = Total_pop_1_test_log.fillna(0)\n",
        "Total_pop_1_test_log['Cell_line'] = 0\n",
        "Total_pop_1_test_log.reset_index(inplace = True)\n",
        "prefeatured_pop_1_test = Total_pop_1_test_log\n",
        "\n",
        "# train pop 2\n",
        "Total_pop_2_log = Total_pop_2.transform(log, channels=['FSC_A','FSC_H', 'FSC_W', 'SSC_A', 'SSC_H', 'SSC_W', 'BV421_A', 'FITC_A','PerCP_A', 'PE_Texas_Red_A', 'Alexa_Fluor_700_A'], b=500.0)\n",
        "Total_pop_2_log.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "Total_pop_2_log = Total_pop_2_log.fillna(0)\n",
        "Total_pop_2_log['Cell_line'] = 1\n",
        "Total_pop_2_log.reset_index(inplace = True)\n",
        "group_list_2 = Total_pop_2.group.tolist()\n",
        "Total_pop_2_log['group'] = group_list_2\n",
        "prefeatured_pop_2 = Total_pop_2_log\n",
        "\n",
        "# test pop 2\n",
        "Total_pop_2_test_log = Total_pop_2_test.transform(log, channels=['FSC_A','FSC_H', 'FSC_W', 'SSC_A', 'SSC_H', 'SSC_W', 'BV421_A', 'FITC_A','PerCP_A', 'PE_Texas_Red_A', 'Alexa_Fluor_700_A'], b=500.0)\n",
        "Total_pop_2_test_log.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "Total_pop_2_test_log = Total_pop_2_test_log.fillna(0)\n",
        "Total_pop_2_test_log['Cell_line'] = 1\n",
        "Total_pop_2_test_log.reset_index(inplace = True)\n",
        "prefeatured_pop_2_test = Total_pop_2_test_log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the preprocessed datasets - named the data as needed\n",
        "prefeatured_pop_1.to_csv(data_path + 'Total_train_H460_filtered_log.csv')\n",
        "prefeatured_pop_2.to_csv(data_path + 'Total_train_H460_cis_res_filtered_log.csv')"
      ],
      "metadata": {
        "id": "e_aV1cxAkboE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This part includes the different approachs for clasify the data:\n",
        "*   Only uptake data\n",
        "*   Only physiological data\n",
        "*   Only FSC data\n",
        "*   Only SSC data\n",
        "*   All the data\n",
        "*   Only uptake with bins\n",
        "*   Only physiological with bins\n",
        "\n",
        "Each one start with preparation of the features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xwOuEacAxlYc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4HLoStl61BH"
      },
      "source": [
        "## Uptake Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHjilTOR61BI"
      },
      "source": [
        "Pop 1 - bins mean and median"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 1 train data"
      ],
      "metadata": {
        "id": "op7dSdXlN6BK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNYbliO161BI"
      },
      "outputs": [],
      "source": [
        "# apply bins function on the different particle features\n",
        "Total_pop_1_log[['BV421_A_1', 'BV421_A_2', 'BV421_A_3', 'BV421_A_4', 'BV421_A_5', 'BV421_A_6', 'BV421_A_7', 'BV421_A_8', 'BV421_A_9', 'BV421_A_10', 'BV421_A_11']] = Total_pop_1_log['BV421_A'].apply(bins)\n",
        "Total_pop_1_log[['FITC_A_1', 'FITC_A_2', 'FITC_A_3', 'FITC_A_4', 'FITC_A_5', 'FITC_A_6', 'FITC_A_7', 'FITC_A_8', 'FITC_A_9', 'FITC_A_10', 'FITC_A_11']] = Total_pop_1_log['FITC_A'].apply(bins)\n",
        "Total_pop_1_log[['PE_Texas_Red_A_1', 'PE_Texas_Red_A_2', 'PE_Texas_Red_A_3', 'PE_Texas_Red_A_4', 'PE_Texas_Red_A_5', 'PE_Texas_Red_A_6', 'PE_Texas_Red_A_7', 'PE_Texas_Red_A_8', 'PE_Texas_Red_A_9', 'PE_Texas_Red_A_10', 'PE_Texas_Red_A_11']] = Total_pop_1_log['PE_Texas_Red_A'].apply(bins)\n",
        "Total_pop_1_log[['Alexa_Fluor_700_A_1', 'Alexa_Fluor_700_A_2', 'Alexa_Fluor_700_A_3', 'Alexa_Fluor_700_A_4', 'Alexa_Fluor_700_A_5', 'Alexa_Fluor_700_A_6', 'Alexa_Fluor_700_A_7', 'Alexa_Fluor_700_A_8', 'Alexa_Fluor_700_A_9', 'Alexa_Fluor_700_A_10', 'Alexa_Fluor_700_A_11']] = Total_pop_1_log['Alexa_Fluor_700_A'].apply(bins)\n",
        "Total_pop_1_log[['PerCP_A_1', 'PerCP_A_2', 'PerCP_A_3', 'PerCP_A_4', 'PerCP_A_5', 'PerCP_A_6', 'PerCP_A_7', 'PerCP_A_8', 'PerCP_A_9', 'PerCP_A_10', 'PerCP_A_11']] = Total_pop_1_log['PerCP_A'].apply(bins)\n",
        "Total_pop_1_log.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWu9LPXb61BI"
      },
      "outputs": [],
      "source": [
        "# applying bins intensity function on the particle features\n",
        "Total_pop_1_log[['0.5_1_intensity', '0.5_2_intensity', '0.5_3_intensity', '0.5_4_intensity', '0.5_5_intensity', '0.5_6_intensity', '0.5_7_intensity', '0.5_8_intensity', '0.5_9_intensity', '0.5_10_intensity', '0.5_11_intensity']] = Total_pop_1_log.apply(lambda x: bins_intensity(x.BV421_A, x.BV421_A_1, x.BV421_A_2, x.BV421_A_3, x.BV421_A_4, x.BV421_A_5, x.BV421_A_6, x.BV421_A_7, x.BV421_A_8, x.BV421_A_9, x.BV421_A_10, x.BV421_A_11), axis=1)\n",
        "Total_pop_1_log[['0.8_1_intensity', '0.8_2_intensity', '0.8_3_intensity', '0.8_4_intensity', '0.8_5_intensity', '0.8_6_intensity', '0.8_7_intensity', '0.8_8_intensity', '0.8_9_intensity', '0.8_10_intensity', '0.8_11_intensity']] = Total_pop_1_log.apply(lambda x: bins_intensity(x.FITC_A, x.FITC_A_1, x.FITC_A_2, x.FITC_A_3, x.FITC_A_4, x.FITC_A_5, x.FITC_A_6, x.FITC_A_7, x.FITC_A_8, x.FITC_A_9, x.FITC_A_10, x.FITC_A_11), axis=1)\n",
        "Total_pop_1_log[['2.4_1_intensity', '2.4_2_intensity', '2.4_3_intensity', '2.4_4_intensity', '2.4_5_intensity', '2.4_6_intensity', '2.4_7_intensity', '2.4_8_intensity', '2.4_9_intensity', '2.4_10_intensity', '2.4_11_intensity']] = Total_pop_1_log.apply(lambda x: bins_intensity(x.PE_Texas_Red_A, x.PE_Texas_Red_A_1, x.PE_Texas_Red_A_2, x.PE_Texas_Red_A_3, x.PE_Texas_Red_A_4, x.PE_Texas_Red_A_5, x.PE_Texas_Red_A_6, x.PE_Texas_Red_A_7, x.PE_Texas_Red_A_8, x.PE_Texas_Red_A_9, x.PE_Texas_Red_A_10, x.PE_Texas_Red_A_11), axis=1)\n",
        "Total_pop_1_log[['3.36_1_intensity', '3.36_2_intensity', '3.36_3_intensity', '3.36_4_intensity', '3.36_5_intensity', '3.36_6_intensity', '3.36_7_intensity', '3.36_8_intensity', '3.36_9_intensity', '3.36_10_intensity', '3.36_11_intensity']] = Total_pop_1_log.apply(lambda x: bins_intensity(x.Alexa_Fluor_700_A, x.Alexa_Fluor_700_A_1, x.Alexa_Fluor_700_A_2, x.Alexa_Fluor_700_A_3, x.Alexa_Fluor_700_A_4, x.Alexa_Fluor_700_A_5, x.Alexa_Fluor_700_A_6, x.Alexa_Fluor_700_A_7, x.Alexa_Fluor_700_A_8, x.Alexa_Fluor_700_A_9, x.Alexa_Fluor_700_A_10, x.Alexa_Fluor_700_A_11), axis=1)\n",
        "Total_pop_1_log[['0.04_1_intensity', '0.04_2_intensity', '0.04_3_intensity', '0.04_4_intensity', '0.04_5_intensity', '0.04_6_intensity', '0.04_7_intensity', '0.04_8_intensity', '0.04_9_intensity', '0.04_10_intensity', '0.04_11_intensity']] = Total_pop_1_log.apply(lambda x: bins_intensity(x.PerCP_A, x.PerCP_A_1, x.PerCP_A_2, x.PerCP_A_3, x.PerCP_A_4, x.PerCP_A_5, x.PerCP_A_6, x.PerCP_A_7, x.PerCP_A_8, x.PerCP_A_9, x.PerCP_A_10, x.PerCP_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eMZ-jZs61BI"
      },
      "outputs": [],
      "source": [
        "  # grouping all the relevant data per population and building new df's that include only the relevant data for analysis\n",
        "grouper = Total_pop_1_log.groupby(pd.Grouper(key='population_number'))\n",
        "BV421_A_1_intensity_sum = grouper['0.5_1_intensity'].sum().to_frame(name='intensity_sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_intensity_sum = grouper['0.5_2_intensity'].sum().to_frame(name='intensity_sum_BV421_A_2').reset_index()\n",
        "BV421_A_3_intensity_sum = grouper['0.5_3_intensity'].sum().to_frame(name='intensity_sum_BV421_A_3').reset_index()\n",
        "BV421_A_4_intensity_sum = grouper['0.5_4_intensity'].sum().to_frame(name='intensity_sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_intensity_sum = grouper['0.5_5_intensity'].sum().to_frame(name='intensity_sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_intensity_sum = grouper['0.5_6_intensity'].sum().to_frame(name='intensity_sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_intensity_sum = grouper['0.5_7_intensity'].sum().to_frame(name='intensity_sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_intensity_sum = grouper['0.5_8_intensity'].sum().to_frame(name='intensity_sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_intensity_sum = grouper['0.5_9_intensity'].sum().to_frame(name='intensity_sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_intensity_sum = grouper['0.5_10_intensity'].sum().to_frame(name='intensity_sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_intensity_sum = grouper['0.5_11_intensity'].sum().to_frame(name='intensity_sum_BV421_A_11').reset_index()\n",
        "BV421_A_1_sum = grouper['BV421_A_1'].sum().to_frame(name='sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_sum = grouper['BV421_A_2'].sum().to_frame(name='sum_BV421_A_2').reset_index()\n",
        "BV421_A_3_sum = grouper['BV421_A_3'].sum().to_frame(name='sum_BV421_A_3').reset_index()\n",
        "BV421_A_4_sum = grouper['BV421_A_4'].sum().to_frame(name='sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_sum = grouper['BV421_A_5'].sum().to_frame(name='sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_sum = grouper['BV421_A_6'].sum().to_frame(name='sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_sum = grouper['BV421_A_7'].sum().to_frame(name='sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_sum = grouper['BV421_A_8'].sum().to_frame(name='sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_sum = grouper['BV421_A_9'].sum().to_frame(name='sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_sum = grouper['BV421_A_10'].sum().to_frame(name='sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_sum = grouper['BV421_A_11'].sum().to_frame(name='sum_BV421_A_11').reset_index()\n",
        "\n",
        "FITC_A_1_intensity_sum = grouper['0.8_1_intensity'].sum().to_frame(name='intensity_sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_intensity_sum = grouper['0.8_2_intensity'].sum().to_frame(name='intensity_sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_intensity_sum = grouper['0.8_3_intensity'].sum().to_frame(name='intensity_sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_intensity_sum = grouper['0.8_4_intensity'].sum().to_frame(name='intensity_sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_intensity_sum = grouper['0.8_5_intensity'].sum().to_frame(name='intensity_sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_intensity_sum = grouper['0.8_6_intensity'].sum().to_frame(name='intensity_sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_intensity_sum = grouper['0.8_7_intensity'].sum().to_frame(name='intensity_sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_intensity_sum = grouper['0.8_8_intensity'].sum().to_frame(name='intensity_sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_intensity_sum = grouper['0.8_9_intensity'].sum().to_frame(name='intensity_sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_intensity_sum = grouper['0.8_10_intensity'].sum().to_frame(name='intensity_sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_intensity_sum = grouper['0.8_11_intensity'].sum().to_frame(name='intensity_sum_FITC_A_11').reset_index()\n",
        "FITC_A_1_sum = grouper['FITC_A_1'].sum().to_frame(name='sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_sum = grouper['FITC_A_2'].sum().to_frame(name='sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_sum = grouper['FITC_A_3'].sum().to_frame(name='sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_sum = grouper['FITC_A_4'].sum().to_frame(name='sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_sum = grouper['FITC_A_5'].sum().to_frame(name='sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_sum = grouper['FITC_A_6'].sum().to_frame(name='sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_sum = grouper['FITC_A_7'].sum().to_frame(name='sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_sum = grouper['FITC_A_8'].sum().to_frame(name='sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_sum = grouper['FITC_A_9'].sum().to_frame(name='sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_sum = grouper['FITC_A_10'].sum().to_frame(name='sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_sum = grouper['FITC_A_11'].sum().to_frame(name='sum_FITC_A_11').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_1_intensity_sum = grouper['2.4_1_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_intensity_sum = grouper['2.4_2_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_intensity_sum = grouper['2.4_3_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_intensity_sum = grouper['2.4_4_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_intensity_sum = grouper['2.4_5_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_intensity_sum = grouper['2.4_6_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_intensity_sum = grouper['2.4_7_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_intensity_sum = grouper['2.4_8_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_intensity_sum = grouper['2.4_9_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_intensity_sum = grouper['2.4_10_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_intensity_sum = grouper['2.4_11_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_11').reset_index()\n",
        "PE_Texas_Red_A_1_sum = grouper['PE_Texas_Red_A_1'].sum().to_frame(name='sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_sum = grouper['PE_Texas_Red_A_2'].sum().to_frame(name='sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_sum = grouper['PE_Texas_Red_A_3'].sum().to_frame(name='sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_sum = grouper['PE_Texas_Red_A_4'].sum().to_frame(name='sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_sum = grouper['PE_Texas_Red_A_5'].sum().to_frame(name='sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_sum = grouper['PE_Texas_Red_A_6'].sum().to_frame(name='sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_sum = grouper['PE_Texas_Red_A_7'].sum().to_frame(name='sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_sum = grouper['PE_Texas_Red_A_8'].sum().to_frame(name='sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_sum = grouper['PE_Texas_Red_A_9'].sum().to_frame(name='sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_sum = grouper['PE_Texas_Red_A_10'].sum().to_frame(name='sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_sum = grouper['PE_Texas_Red_A_11'].sum().to_frame(name='sum_PE_Texas_Red_A_11').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_1_intensity_sum = grouper['3.36_1_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_intensity_sum = grouper['3.36_2_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_intensity_sum = grouper['3.36_3_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_intensity_sum = grouper['3.36_4_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_intensity_sum = grouper['3.36_5_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_intensity_sum = grouper['3.36_6_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_intensity_sum = grouper['3.36_7_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_intensity_sum = grouper['3.36_8_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_intensity_sum = grouper['3.36_9_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_intensity_sum = grouper['3.36_10_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_intensity_sum = grouper['3.36_11_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "Alexa_Fluor_700_A_1_sum = grouper['Alexa_Fluor_700_A_1'].sum().to_frame(name='sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_sum = grouper['Alexa_Fluor_700_A_2'].sum().to_frame(name='sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_sum = grouper['Alexa_Fluor_700_A_3'].sum().to_frame(name='sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_sum = grouper['Alexa_Fluor_700_A_4'].sum().to_frame(name='sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_sum = grouper['Alexa_Fluor_700_A_5'].sum().to_frame(name='sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_sum = grouper['Alexa_Fluor_700_A_6'].sum().to_frame(name='sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_sum = grouper['Alexa_Fluor_700_A_7'].sum().to_frame(name='sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_sum = grouper['Alexa_Fluor_700_A_8'].sum().to_frame(name='sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_sum = grouper['Alexa_Fluor_700_A_9'].sum().to_frame(name='sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_sum = grouper['Alexa_Fluor_700_A_10'].sum().to_frame(name='sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_sum = grouper['Alexa_Fluor_700_A_11'].sum().to_frame(name='sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "\n",
        "PerCP_A_1_intensity_sum = grouper['0.04_1_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_intensity_sum = grouper['0.04_2_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_intensity_sum = grouper['0.04_3_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_intensity_sum = grouper['0.04_4_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_intensity_sum = grouper['0.04_5_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_intensity_sum = grouper['0.04_6_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_intensity_sum = grouper['0.04_7_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_intensity_sum = grouper['0.04_8_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_intensity_sum = grouper['0.04_9_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_intensity_sum = grouper['0.04_10_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_intensity_sum = grouper['0.04_11_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_11').reset_index()\n",
        "PerCP_A_1_sum = grouper['PerCP_A_1'].sum().to_frame(name='sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_sum = grouper['PerCP_A_2'].sum().to_frame(name='sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_sum = grouper['PerCP_A_3'].sum().to_frame(name='sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_sum = grouper['PerCP_A_4'].sum().to_frame(name='sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_sum = grouper['PerCP_A_5'].sum().to_frame(name='sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_sum = grouper['PerCP_A_6'].sum().to_frame(name='sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_sum = grouper['PerCP_A_7'].sum().to_frame(name='sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_sum = grouper['PerCP_A_8'].sum().to_frame(name='sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_sum = grouper['PerCP_A_9'].sum().to_frame(name='sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_sum = grouper['PerCP_A_10'].sum().to_frame(name='sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_sum = grouper['PerCP_A_11'].sum().to_frame(name='sum_PerCP_A_11').reset_index()\n",
        "Cell_line = grouper['Cell_line'].mean().to_frame(name='Cell_line').reset_index()\n",
        "group = grouper['group'].mean().to_frame(name='group').reset_index()\n",
        "\n",
        "mean_FSC_A = grouper['FSC_A'].mean().to_frame(name='FSC_A_mean').reset_index()\n",
        "mean_FSC_H = grouper['FSC_H'].mean().to_frame(name='FSC_H_mean').reset_index()\n",
        "mean_FSC_W = grouper['FSC_W'].mean().to_frame(name='FSC_W_mean').reset_index()\n",
        "mean_SSC_A = grouper['SSC_A'].mean().to_frame(name='SSC_A_mean').reset_index()\n",
        "mean_SSC_H = grouper['SSC_H'].mean().to_frame(name='SSC_H_mean').reset_index()\n",
        "mean_SSC_W = grouper['SSC_W'].mean().to_frame(name='SSC_W_mean').reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgPw9dZq61BJ"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_1_log_grouper = [BV421_A_1_intensity_sum, BV421_A_2_intensity_sum, BV421_A_3_intensity_sum, BV421_A_4_intensity_sum, BV421_A_5_intensity_sum,\n",
        "         BV421_A_6_intensity_sum, BV421_A_7_intensity_sum, BV421_A_8_intensity_sum, BV421_A_9_intensity_sum, BV421_A_10_intensity_sum, BV421_A_11_intensity_sum,\n",
        "         BV421_A_1_sum, BV421_A_2_sum, BV421_A_3_sum, BV421_A_4_sum, BV421_A_5_sum, BV421_A_6_sum, BV421_A_7_sum, BV421_A_8_sum, BV421_A_9_sum, BV421_A_10_sum, BV421_A_11_sum,\n",
        "         FITC_A_1_intensity_sum, FITC_A_2_intensity_sum, FITC_A_3_intensity_sum, FITC_A_4_intensity_sum, FITC_A_5_intensity_sum, FITC_A_6_intensity_sum, FITC_A_7_intensity_sum,\n",
        "         FITC_A_8_intensity_sum, FITC_A_9_intensity_sum, FITC_A_10_intensity_sum, FITC_A_11_intensity_sum, FITC_A_1_sum, FITC_A_2_sum, FITC_A_3_sum, FITC_A_4_sum, FITC_A_5_sum,\n",
        "         FITC_A_6_sum, FITC_A_7_sum, FITC_A_8_sum, FITC_A_9_sum, FITC_A_10_sum, FITC_A_11_sum,PE_Texas_Red_A_1_intensity_sum, PE_Texas_Red_A_2_intensity_sum, PE_Texas_Red_A_3_intensity_sum,\n",
        "         PE_Texas_Red_A_4_intensity_sum, PE_Texas_Red_A_5_intensity_sum, PE_Texas_Red_A_6_intensity_sum, PE_Texas_Red_A_7_intensity_sum, PE_Texas_Red_A_8_intensity_sum, PE_Texas_Red_A_9_intensity_sum,\n",
        "         PE_Texas_Red_A_10_intensity_sum, PE_Texas_Red_A_11_intensity_sum, PE_Texas_Red_A_1_sum, PE_Texas_Red_A_2_sum, PE_Texas_Red_A_3_sum, PE_Texas_Red_A_4_sum, PE_Texas_Red_A_5_sum,\n",
        "         PE_Texas_Red_A_6_sum, PE_Texas_Red_A_7_sum, PE_Texas_Red_A_8_sum, PE_Texas_Red_A_9_sum, PE_Texas_Red_A_10_sum, PE_Texas_Red_A_11_sum,\n",
        "         Alexa_Fluor_700_A_1_intensity_sum, Alexa_Fluor_700_A_2_intensity_sum, Alexa_Fluor_700_A_3_intensity_sum, Alexa_Fluor_700_A_4_intensity_sum, Alexa_Fluor_700_A_5_intensity_sum,\n",
        "         Alexa_Fluor_700_A_6_intensity_sum, Alexa_Fluor_700_A_7_intensity_sum, Alexa_Fluor_700_A_8_intensity_sum, Alexa_Fluor_700_A_9_intensity_sum, Alexa_Fluor_700_A_10_intensity_sum, Alexa_Fluor_700_A_11_intensity_sum,\n",
        "         Alexa_Fluor_700_A_1_sum, Alexa_Fluor_700_A_2_sum, Alexa_Fluor_700_A_3_sum, Alexa_Fluor_700_A_4_sum, Alexa_Fluor_700_A_5_sum, Alexa_Fluor_700_A_6_sum, Alexa_Fluor_700_A_7_sum, Alexa_Fluor_700_A_8_sum,\n",
        "         Alexa_Fluor_700_A_9_sum, Alexa_Fluor_700_A_10_sum, Alexa_Fluor_700_A_11_sum, PerCP_A_1_intensity_sum, PerCP_A_2_intensity_sum, PerCP_A_3_intensity_sum, PerCP_A_4_intensity_sum, PerCP_A_5_intensity_sum,\n",
        "         PerCP_A_6_intensity_sum, PerCP_A_7_intensity_sum, PerCP_A_8_intensity_sum, PerCP_A_9_intensity_sum, PerCP_A_10_intensity_sum, PerCP_A_11_intensity_sum,\n",
        "         PerCP_A_1_sum, PerCP_A_2_sum, PerCP_A_3_sum, PerCP_A_4_sum, PerCP_A_5_sum, PerCP_A_6_sum, PerCP_A_7_sum, PerCP_A_8_sum, PerCP_A_9_sum, PerCP_A_10_sum, PerCP_A_11_sum,\n",
        "         mean_FSC_A, mean_FSC_H, mean_FSC_W, mean_SSC_A, mean_SSC_H, mean_SSC_W, group, Cell_line]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDCT83Yh61BJ"
      },
      "outputs": [],
      "source": [
        "# building a new dfs of the grouped data\n",
        "pop_1_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_1_log_grouper)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalizing the data"
      ],
      "metadata": {
        "id": "z_i8aZ-K1gYj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru2HHznsAU2m"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['normalized_intensity_BV421_A_1', 'normalized_intensity_BV421_A_2', 'normalized_intensity_BV421_A_3', 'normalized_intensity_BV421_A_4'\n",
        "                  , 'normalized_intensity_BV421_A_5', 'normalized_intensity_BV421_A_6', 'normalized_intensity_BV421_A_7', 'normalized_intensity_BV421_A_8'\n",
        "                  , 'normalized_intensity_BV421_A_9', 'normalized_intensity_BV421_A_10', 'normalized_intensity_BV421_A_11']] = pop_1_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_BV421_A_1, x.intensity_sum_BV421_A_2, x.intensity_sum_BV421_A_3, x.intensity_sum_BV421_A_4, x.intensity_sum_BV421_A_5,\n",
        "                  x.intensity_sum_BV421_A_6, x.intensity_sum_BV421_A_7, x.intensity_sum_BV421_A_8, x.intensity_sum_BV421_A_9, x.intensity_sum_BV421_A_10, x.intensity_sum_BV421_A_11,\n",
        "                  x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZpNqVQbAoNk"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['normalized_intensity_FITC_A_1', 'normalized_intensity_FITC_A_2', 'normalized_intensity_FITC_A_3', 'normalized_intensity_FITC_A_4',\n",
        "                          'normalized_intensity_FITC_A_5', 'normalized_intensity_FITC_A_6', 'normalized_intensity_FITC_A_7', 'normalized_intensity_FITC_A_8',\n",
        "                          'normalized_intensity_FITC_A_9', 'normalized_intensity_FITC_A_10', 'normalized_intensity_FITC_A_11']] = pop_1_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_FITC_A_1, x.intensity_sum_FITC_A_2, x.intensity_sum_FITC_A_3, x.intensity_sum_FITC_A_4, x.intensity_sum_FITC_A_5,\n",
        "                          x.intensity_sum_FITC_A_6, x.intensity_sum_FITC_A_7, x.intensity_sum_FITC_A_8, x.intensity_sum_FITC_A_9, x.intensity_sum_FITC_A_10, x.intensity_sum_FITC_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "914QYoQCA1Lc"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'normalized_intensity_PE_Texas_Red_A_2', 'normalized_intensity_PE_Texas_Red_A_3', 'normalized_intensity_PE_Texas_Red_A_4',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_5', 'normalized_intensity_PE_Texas_Red_A_6', 'normalized_intensity_PE_Texas_Red_A_7', 'normalized_intensity_PE_Texas_Red_A_8',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_9','normalized_intensity_PE_Texas_Red_A_10','normalized_intensity_PE_Texas_Red_A_11']] = pop_1_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PE_Texas_Red_A_1, x.intensity_sum_PE_Texas_Red_A_2, x.intensity_sum_PE_Texas_Red_A_3, x.intensity_sum_PE_Texas_Red_A_4, x.intensity_sum_PE_Texas_Red_A_5,\n",
        "                          x.intensity_sum_PE_Texas_Red_A_6, x.intensity_sum_PE_Texas_Red_A_7, x.intensity_sum_PE_Texas_Red_A_8, x.intensity_sum_PE_Texas_Red_A_9,\n",
        "                          x.intensity_sum_PE_Texas_Red_A_10, x.intensity_sum_PE_Texas_Red_A_11, x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAQ2G0ciBCl6"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'normalized_intensity_Alexa_Fluor_700_A_2', 'normalized_intensity_Alexa_Fluor_700_A_3', 'normalized_intensity_Alexa_Fluor_700_A_4',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_5', 'normalized_intensity_Alexa_Fluor_700_A_6', 'normalized_intensity_Alexa_Fluor_700_A_7', 'normalized_intensity_Alexa_Fluor_700_A_8',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_9', 'normalized_intensity_Alexa_Fluor_700_A_10', 'normalized_intensity_Alexa_Fluor_700_A_11']] = pop_1_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_Alexa_Fluor_700_A_1, x.intensity_sum_Alexa_Fluor_700_A_2, x.intensity_sum_Alexa_Fluor_700_A_3, x.intensity_sum_Alexa_Fluor_700_A_4, x.intensity_sum_Alexa_Fluor_700_A_5,\n",
        "                          x.intensity_sum_Alexa_Fluor_700_A_6, x.intensity_sum_Alexa_Fluor_700_A_7, x.intensity_sum_Alexa_Fluor_700_A_8, x.intensity_sum_Alexa_Fluor_700_A_9, x.intensity_sum_Alexa_Fluor_700_A_10, x.intensity_sum_Alexa_Fluor_700_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rQAeDmdBQOO"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['normalized_intensity_PerCP_A_1', 'normalized_intensity_PerCP_A_2', 'normalized_intensity_PerCP_A_3', 'normalized_intensity_PerCP_A_4',\n",
        "                          'normalized_intensity_PerCP_A_5', 'normalized_intensity_PerCP_A_6', 'normalized_intensity_PerCP_A_7', 'normalized_intensity_PerCP_A_8',\n",
        "                          'normalized_intensity_PerCP_A_9', 'normalized_intensity_PerCP_A_10', 'normalized_intensity_PerCP_A_11']] = pop_1_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PerCP_A_1, x.intensity_sum_PerCP_A_2, x.intensity_sum_PerCP_A_3, x.intensity_sum_PerCP_A_4, x.intensity_sum_PerCP_A_5,\n",
        "                          x.intensity_sum_PerCP_A_6, x.intensity_sum_PerCP_A_7, x.intensity_sum_PerCP_A_8, x.intensity_sum_PerCP_A_9, x.intensity_sum_PerCP_A_10, x.intensity_sum_PerCP_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating the mean uptake intensities per bin"
      ],
      "metadata": {
        "id": "yv2NE-Pu15rT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HePnHWrBgpU"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['mean_intensity_0.5_1', 'mean_intensity_0.5_2', 'mean_intensity_0.5_3', 'mean_intensity_0.5_4',\n",
        "                          'mean_intensity_0.5_5', 'mean_intensity_0.5_6', 'mean_intensity_0.5_7', 'mean_intensity_0.5_8',\n",
        "                          'mean_intensity_0.5_9', 'mean_intensity_0.5_10', 'mean_intensity_0.5_11']] = pop_1_log_grouped.apply(lambda x: average(x.normalized_intensity_BV421_A_1, x.normalized_intensity_BV421_A_2, x.normalized_intensity_BV421_A_3, x.normalized_intensity_BV421_A_4, x.normalized_intensity_BV421_A_5,\n",
        "                          x.normalized_intensity_BV421_A_6, x.normalized_intensity_BV421_A_7, x.normalized_intensity_BV421_A_8, x.normalized_intensity_BV421_A_9, x.normalized_intensity_BV421_A_10, x.normalized_intensity_BV421_A_11,\n",
        "                          x.sum_BV421_A_1, x.sum_BV421_A_2, x.sum_BV421_A_3, x.sum_BV421_A_4, x.sum_BV421_A_5, x.sum_BV421_A_6, x.sum_BV421_A_7, x.sum_BV421_A_8,x.sum_BV421_A_9, x.sum_BV421_A_10, x.sum_BV421_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naxY-lCrBtBr"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['mean_intensity_0.8_1', 'mean_intensity_0.8_2', 'mean_intensity_0.8_3', 'mean_intensity_0.8_4',\n",
        "                          'mean_intensity_0.8_5', 'mean_intensity_0.8_6', 'mean_intensity_0.8_7', 'mean_intensity_0.8_8',\n",
        "                          'mean_intensity_0.8_9', 'mean_intensity_0.8_10', 'mean_intensity_0.8_11']] = pop_1_log_grouped.apply(lambda x: average(x.normalized_intensity_FITC_A_1, x.normalized_intensity_FITC_A_2, x.normalized_intensity_FITC_A_3, x.normalized_intensity_FITC_A_4, x.normalized_intensity_FITC_A_5,\n",
        "                          x.normalized_intensity_FITC_A_6, x.normalized_intensity_FITC_A_7, x.normalized_intensity_FITC_A_8, x.normalized_intensity_FITC_A_9, x.normalized_intensity_FITC_A_10, x.normalized_intensity_FITC_A_11,\n",
        "                          x.sum_FITC_A_1, x.sum_FITC_A_2, x.sum_FITC_A_3, x.sum_FITC_A_4, x.sum_FITC_A_5, x.sum_FITC_A_6, x.sum_FITC_A_7, x.sum_FITC_A_8, x.sum_FITC_A_9, x.sum_FITC_A_10, x.sum_FITC_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slvRc_Q-B7Zx"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['mean_intensity_2.4_1', 'mean_intensity_2.4_2', 'mean_intensity_2.4_3', 'mean_intensity_2.4_4',\n",
        "                          'mean_intensity_2.4_5', 'mean_intensity_2.4_6', 'mean_intensity_2.4_7', 'mean_intensity_2.4_8',\n",
        "                          'mean_intensity_2.4_9', 'mean_intensity_2.4_10', 'mean_intensity_2.4_11']] = pop_1_log_grouped.apply(lambda x: average(x.normalized_intensity_PE_Texas_Red_A_1, x.normalized_intensity_PE_Texas_Red_A_2, x.normalized_intensity_PE_Texas_Red_A_3, x.normalized_intensity_PE_Texas_Red_A_4, x.normalized_intensity_PE_Texas_Red_A_5,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_6, x.normalized_intensity_PE_Texas_Red_A_7, x.normalized_intensity_PE_Texas_Red_A_8,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_9, x.normalized_intensity_PE_Texas_Red_A_10, x.normalized_intensity_PE_Texas_Red_A_11,\n",
        "                          x.sum_PE_Texas_Red_A_1, x.sum_PE_Texas_Red_A_2, x.sum_PE_Texas_Red_A_3, x.sum_PE_Texas_Red_A_4, x.sum_PE_Texas_Red_A_5,\n",
        "                          x.sum_PE_Texas_Red_A_6, x.sum_PE_Texas_Red_A_7, x.sum_PE_Texas_Red_A_8, x.sum_PE_Texas_Red_A_9, x.sum_PE_Texas_Red_A_10, x.sum_PE_Texas_Red_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wftouFSCP1j"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['mean_intensity_3.36_1', 'mean_intensity_3.36_2', 'mean_intensity_3.36_3', 'mean_intensity_3.36_4',\n",
        "                          'mean_intensity_3.36_5', 'mean_intensity_3.36_6', 'mean_intensity_3.36_7', 'mean_intensity_3.36_8',\n",
        "                          'mean_intensity_3.36_9', 'mean_intensity_3.36_10', 'mean_intensity_3.36_11']] = pop_1_log_grouped.apply(lambda x: average(x.normalized_intensity_Alexa_Fluor_700_A_1, x.normalized_intensity_Alexa_Fluor_700_A_2, x.normalized_intensity_Alexa_Fluor_700_A_3, x.normalized_intensity_Alexa_Fluor_700_A_4, x.normalized_intensity_Alexa_Fluor_700_A_5,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_6, x.normalized_intensity_Alexa_Fluor_700_A_7, x.normalized_intensity_Alexa_Fluor_700_A_8,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_9, x.normalized_intensity_Alexa_Fluor_700_A_10, x.normalized_intensity_Alexa_Fluor_700_A_11,\n",
        "                          x.sum_Alexa_Fluor_700_A_1, x.sum_Alexa_Fluor_700_A_2, x.sum_Alexa_Fluor_700_A_3, x.sum_Alexa_Fluor_700_A_4, x.sum_Alexa_Fluor_700_A_5,\n",
        "                          x.sum_Alexa_Fluor_700_A_6, x.sum_Alexa_Fluor_700_A_7, x.sum_Alexa_Fluor_700_A_8, x.sum_Alexa_Fluor_700_A_9, x.sum_Alexa_Fluor_700_A_10, x.sum_Alexa_Fluor_700_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKni5ajICcCO"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped[['mean_intensity_0.04_1', 'mean_intensity_0.04_2', 'mean_intensity_0.04_3', 'mean_intensity_0.04_4',\n",
        "                          'mean_intensity_0.04_5', 'mean_intensity_0.04_6', 'mean_intensity_0.04_7', 'mean_intensity_0.04_8',\n",
        "                          'mean_intensity_0.04_9', 'mean_intensity_0.04_10', 'mean_intensity_0.04_11']] = pop_1_log_grouped.apply(lambda x: average(x.normalized_intensity_PerCP_A_1, x.normalized_intensity_PerCP_A_2, x.normalized_intensity_PerCP_A_3, x.normalized_intensity_PerCP_A_4, x.normalized_intensity_PerCP_A_5,\n",
        "                          x.normalized_intensity_PerCP_A_6, x.normalized_intensity_PerCP_A_7, x.normalized_intensity_PerCP_A_8,\n",
        "                          x.normalized_intensity_PerCP_A_9, x.normalized_intensity_PerCP_A_10, x.normalized_intensity_PerCP_A_11,\n",
        "                          x.sum_PerCP_A_1, x.sum_PerCP_A_2, x.sum_PerCP_A_3, x.sum_PerCP_A_4, x.sum_PerCP_A_5,\n",
        "                          x.sum_PerCP_A_6, x.sum_PerCP_A_7, x.sum_PerCP_A_8,x.sum_PerCP_A_9, x.sum_PerCP_A_10, x.sum_PerCP_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYY7K4p361BM"
      },
      "outputs": [],
      "source": [
        "pop_1_log_grouped.to_csv('H460_log_filtered_grouped.csv') # name the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating median intesities per bin"
      ],
      "metadata": {
        "id": "ETCR8TPv3j71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Prfx4-Z61BM"
      },
      "outputs": [],
      "source": [
        "# buliding dfs for median calculations\n",
        "df_median_1_1 = pop_1_log_grouped[['normalized_intensity_BV421_A_1', 'population_number']]\n",
        "df_median_1_2 = pop_1_log_grouped[['normalized_intensity_BV421_A_2', 'population_number']]\n",
        "df_median_1_3 = pop_1_log_grouped[['normalized_intensity_BV421_A_3', 'population_number']]\n",
        "df_median_1_4 = pop_1_log_grouped[['normalized_intensity_BV421_A_4', 'population_number']]\n",
        "df_median_1_5 = pop_1_log_grouped[['normalized_intensity_BV421_A_5', 'population_number']]\n",
        "df_median_1_6 = pop_1_log_grouped[['normalized_intensity_BV421_A_6', 'population_number']]\n",
        "df_median_1_7 = pop_1_log_grouped[['normalized_intensity_BV421_A_7', 'population_number']]\n",
        "df_median_1_8 = pop_1_log_grouped[['normalized_intensity_BV421_A_8', 'population_number']]\n",
        "df_median_1_9 = pop_1_log_grouped[['normalized_intensity_BV421_A_9', 'population_number']]\n",
        "df_median_1_10 = pop_1_log_grouped[['normalized_intensity_BV421_A_10', 'population_number']]\n",
        "df_median_1_11 = pop_1_log_grouped[['normalized_intensity_BV421_A_11', 'population_number']]\n",
        "\n",
        "df_median_2_1 = pop_1_log_grouped[['normalized_intensity_FITC_A_1', 'population_number']]\n",
        "df_median_2_2 = pop_1_log_grouped[['normalized_intensity_FITC_A_2', 'population_number']]\n",
        "df_median_2_3 = pop_1_log_grouped[['normalized_intensity_FITC_A_3', 'population_number']]\n",
        "df_median_2_4 = pop_1_log_grouped[['normalized_intensity_FITC_A_4', 'population_number']]\n",
        "df_median_2_5 = pop_1_log_grouped[['normalized_intensity_FITC_A_5', 'population_number']]\n",
        "df_median_2_6 = pop_1_log_grouped[['normalized_intensity_FITC_A_6', 'population_number']]\n",
        "df_median_2_7 = pop_1_log_grouped[['normalized_intensity_FITC_A_7', 'population_number']]\n",
        "df_median_2_8 = pop_1_log_grouped[['normalized_intensity_FITC_A_8', 'population_number']]\n",
        "df_median_2_9 = pop_1_log_grouped[['normalized_intensity_FITC_A_9', 'population_number']]\n",
        "df_median_2_10 = pop_1_log_grouped[['normalized_intensity_FITC_A_10', 'population_number']]\n",
        "df_median_2_11 = pop_1_log_grouped[['normalized_intensity_FITC_A_11', 'population_number']]\n",
        "\n",
        "df_median_3_1 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'population_number']]\n",
        "df_median_3_2 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_2', 'population_number']]\n",
        "df_median_3_3 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_3', 'population_number']]\n",
        "df_median_3_4 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_4', 'population_number']]\n",
        "df_median_3_5 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_5', 'population_number']]\n",
        "df_median_3_6 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_6', 'population_number']]\n",
        "df_median_3_7 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_7', 'population_number']]\n",
        "df_median_3_8 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_8', 'population_number']]\n",
        "df_median_3_9 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_9', 'population_number']]\n",
        "df_median_3_10 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_10', 'population_number']]\n",
        "df_median_3_11 = pop_1_log_grouped[['normalized_intensity_PE_Texas_Red_A_11', 'population_number']]\n",
        "\n",
        "df_median_4_1 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'population_number']]\n",
        "df_median_4_2 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_2', 'population_number']]\n",
        "df_median_4_3 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_3', 'population_number']]\n",
        "df_median_4_4 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_4', 'population_number']]\n",
        "df_median_4_5 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_5', 'population_number']]\n",
        "df_median_4_6 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_6', 'population_number']]\n",
        "df_median_4_7 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_7', 'population_number']]\n",
        "df_median_4_8 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_8', 'population_number']]\n",
        "df_median_4_9 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_9', 'population_number']]\n",
        "df_median_4_10 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_10', 'population_number']]\n",
        "df_median_4_11 = pop_1_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_11', 'population_number']]\n",
        "\n",
        "df_median_5_1 = pop_1_log_grouped[['normalized_intensity_PerCP_A_1', 'population_number']]\n",
        "df_median_5_2 = pop_1_log_grouped[['normalized_intensity_PerCP_A_2', 'population_number']]\n",
        "df_median_5_3 = pop_1_log_grouped[['normalized_intensity_PerCP_A_3', 'population_number']]\n",
        "df_median_5_4 = pop_1_log_grouped[['normalized_intensity_PerCP_A_4', 'population_number']]\n",
        "df_median_5_5 = pop_1_log_grouped[['normalized_intensity_PerCP_A_5', 'population_number']]\n",
        "df_median_5_6 = pop_1_log_grouped[['normalized_intensity_PerCP_A_6', 'population_number']]\n",
        "df_median_5_7 = pop_1_log_grouped[['normalized_intensity_PerCP_A_7', 'population_number']]\n",
        "df_median_5_8 = pop_1_log_grouped[['normalized_intensity_PerCP_A_8', 'population_number']]\n",
        "df_median_5_9 = pop_1_log_grouped[['normalized_intensity_PerCP_A_9', 'population_number']]\n",
        "df_median_5_10 = pop_1_log_grouped[['normalized_intensity_PerCP_A_10', 'population_number']]\n",
        "df_median_5_11 = pop_1_log_grouped[['normalized_intensity_PerCP_A_11', 'population_number']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yie9Mir661BN"
      },
      "outputs": [],
      "source": [
        "# filtering out zeroes\n",
        "df_median_1_1 = df_median_1_1[df_median_1_1['normalized_intensity_BV421_A_1'] > 0]\n",
        "df_median_1_2 = df_median_1_2[df_median_1_2['normalized_intensity_BV421_A_2'] > 0]\n",
        "df_median_1_3 = df_median_1_3[df_median_1_3['normalized_intensity_BV421_A_3'] > 0]\n",
        "df_median_1_4 = df_median_1_4[df_median_1_4['normalized_intensity_BV421_A_4'] > 0]\n",
        "df_median_1_5 = df_median_1_5[df_median_1_5['normalized_intensity_BV421_A_5'] > 0]\n",
        "df_median_1_6 = df_median_1_6[df_median_1_6['normalized_intensity_BV421_A_6'] > 0]\n",
        "df_median_1_7 = df_median_1_7[df_median_1_7['normalized_intensity_BV421_A_7'] > 0]\n",
        "df_median_1_8 = df_median_1_8[df_median_1_8['normalized_intensity_BV421_A_8'] > 0]\n",
        "df_median_1_9 = df_median_1_9[df_median_1_9['normalized_intensity_BV421_A_9'] > 0]\n",
        "df_median_1_10 = df_median_1_10[df_median_1_10['normalized_intensity_BV421_A_10'] > 0]\n",
        "df_median_1_11 = df_median_1_11[df_median_1_11['normalized_intensity_BV421_A_11'] > 0]\n",
        "\n",
        "df_median_2_1 = df_median_2_1[df_median_2_1['normalized_intensity_FITC_A_1'] > 0]\n",
        "df_median_2_2 = df_median_2_2[df_median_2_2['normalized_intensity_FITC_A_2'] > 0]\n",
        "df_median_2_3 = df_median_2_3[df_median_2_3['normalized_intensity_FITC_A_3'] > 0]\n",
        "df_median_2_4 = df_median_2_4[df_median_2_4['normalized_intensity_FITC_A_4'] > 0]\n",
        "df_median_2_5 = df_median_2_5[df_median_2_5['normalized_intensity_FITC_A_5'] > 0]\n",
        "df_median_2_6 = df_median_2_6[df_median_2_6['normalized_intensity_FITC_A_6'] > 0]\n",
        "df_median_2_7 = df_median_2_7[df_median_2_7['normalized_intensity_FITC_A_7'] > 0]\n",
        "df_median_2_8 = df_median_2_8[df_median_2_8['normalized_intensity_FITC_A_8'] > 0]\n",
        "df_median_2_9 = df_median_2_9[df_median_2_9['normalized_intensity_FITC_A_9'] > 0]\n",
        "df_median_2_10 = df_median_2_10[df_median_2_10['normalized_intensity_FITC_A_10'] > 0]\n",
        "df_median_2_11 = df_median_2_11[df_median_2_11['normalized_intensity_FITC_A_11'] > 0]\n",
        "\n",
        "df_median_3_1 = df_median_3_1[df_median_3_1['normalized_intensity_PE_Texas_Red_A_1'] > 0]\n",
        "df_median_3_2 = df_median_3_2[df_median_3_2['normalized_intensity_PE_Texas_Red_A_2'] > 0]\n",
        "df_median_3_3 = df_median_3_3[df_median_3_3['normalized_intensity_PE_Texas_Red_A_3'] > 0]\n",
        "df_median_3_4 = df_median_3_4[df_median_3_4['normalized_intensity_PE_Texas_Red_A_4'] > 0]\n",
        "df_median_3_5 = df_median_3_5[df_median_3_5['normalized_intensity_PE_Texas_Red_A_5'] > 0]\n",
        "df_median_3_6 = df_median_3_6[df_median_3_6['normalized_intensity_PE_Texas_Red_A_6'] > 0]\n",
        "df_median_3_7 = df_median_3_7[df_median_3_7['normalized_intensity_PE_Texas_Red_A_7'] > 0]\n",
        "df_median_3_8 = df_median_3_8[df_median_3_8['normalized_intensity_PE_Texas_Red_A_8'] > 0]\n",
        "df_median_3_9 = df_median_3_9[df_median_3_9['normalized_intensity_PE_Texas_Red_A_9'] > 0]\n",
        "df_median_3_10 = df_median_3_10[df_median_3_10['normalized_intensity_PE_Texas_Red_A_10'] > 0]\n",
        "df_median_3_11 = df_median_3_11[df_median_3_11['normalized_intensity_PE_Texas_Red_A_11'] > 0]\n",
        "\n",
        "df_median_4_1 = df_median_4_1[df_median_4_1['normalized_intensity_Alexa_Fluor_700_A_1'] > 0]\n",
        "df_median_4_2 = df_median_4_2[df_median_4_2['normalized_intensity_Alexa_Fluor_700_A_2'] > 0]\n",
        "df_median_4_3 = df_median_4_3[df_median_4_3['normalized_intensity_Alexa_Fluor_700_A_3'] > 0]\n",
        "df_median_4_4 = df_median_4_4[df_median_4_4['normalized_intensity_Alexa_Fluor_700_A_4'] > 0]\n",
        "df_median_4_5 = df_median_4_5[df_median_4_5['normalized_intensity_Alexa_Fluor_700_A_5'] > 0]\n",
        "df_median_4_6 = df_median_4_6[df_median_4_6['normalized_intensity_Alexa_Fluor_700_A_6'] > 0]\n",
        "df_median_4_7 = df_median_4_7[df_median_4_7['normalized_intensity_Alexa_Fluor_700_A_7'] > 0]\n",
        "df_median_4_8 = df_median_4_8[df_median_4_8['normalized_intensity_Alexa_Fluor_700_A_8'] > 0]\n",
        "df_median_4_9 = df_median_4_9[df_median_4_9['normalized_intensity_Alexa_Fluor_700_A_9'] > 0]\n",
        "df_median_4_10 = df_median_4_10[df_median_4_10['normalized_intensity_Alexa_Fluor_700_A_10'] > 0]\n",
        "df_median_4_11 = df_median_4_11[df_median_4_11['normalized_intensity_Alexa_Fluor_700_A_11'] > 0]\n",
        "\n",
        "df_median_5_1 = df_median_5_1[df_median_5_1['normalized_intensity_PerCP_A_1'] > 0]\n",
        "df_median_5_2 = df_median_5_2[df_median_5_2['normalized_intensity_PerCP_A_2'] > 0]\n",
        "df_median_5_3 = df_median_5_3[df_median_5_3['normalized_intensity_PerCP_A_3'] > 0]\n",
        "df_median_5_4 = df_median_5_4[df_median_5_4['normalized_intensity_PerCP_A_4'] > 0]\n",
        "df_median_5_5 = df_median_5_5[df_median_5_5['normalized_intensity_PerCP_A_5'] > 0]\n",
        "df_median_5_6 = df_median_5_6[df_median_5_6['normalized_intensity_PerCP_A_6'] > 0]\n",
        "df_median_5_7 = df_median_5_7[df_median_5_7['normalized_intensity_PerCP_A_7'] > 0]\n",
        "df_median_5_8 = df_median_5_8[df_median_5_8['normalized_intensity_PerCP_A_8'] > 0]\n",
        "df_median_5_9 = df_median_5_9[df_median_5_9['normalized_intensity_PerCP_A_9'] > 0]\n",
        "df_median_5_10 = df_median_5_10[df_median_5_10['normalized_intensity_PerCP_A_10'] > 0]\n",
        "df_median_5_11 = df_median_5_11[df_median_5_11['normalized_intensity_PerCP_A_11'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAQO5XhT61BN"
      },
      "outputs": [],
      "source": [
        "# grouping and calculating the median per positive (>0) values per bin\n",
        "df_subset_1_1 = df_median_1_1.loc[:, ['population_number', 'normalized_intensity_BV421_A_1'] ]\n",
        "df_median_1_1_grouped = df_median_1_1.groupby('population_number').median()\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.rename(columns={\"normalized_intensity_BV421_A_1\": \"0.5_1_median_intensity\"})\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.reset_index()\n",
        "df_subset_1_2 = df_median_1_2.loc[:, ['population_number', 'normalized_intensity_BV421_A_2'] ]\n",
        "df_median_1_2_grouped = df_median_1_2.groupby('population_number').median()\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.rename(columns={\"normalized_intensity_BV421_A_2\": \"0.5_2_median_intensity\"})\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.reset_index()\n",
        "df_subset_1_3 = df_median_1_3.loc[:, ['population_number', 'normalized_intensity_BV421_A_3'] ]\n",
        "df_median_1_3_grouped = df_median_1_3.groupby('population_number').median()\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.rename(columns={\"normalized_intensity_BV421_A_3\": \"0.5_3_median_intensity\"})\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.reset_index()\n",
        "df_subset_1_4 = df_median_1_4.loc[:, ['population_number', 'normalized_intensity_BV421_A_4'] ]\n",
        "df_median_1_4_grouped = df_median_1_4.groupby('population_number').median()\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.rename(columns={\"normalized_intensity_BV421_A_4\": \"0.5_4_median_intensity\"})\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.reset_index()\n",
        "df_subset_1_5 = df_median_1_5.loc[:, ['population_number', 'normalized_intensity_BV421_A_5'] ]\n",
        "df_median_1_5_grouped = df_median_1_5.groupby('population_number').median()\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.rename(columns={\"normalized_intensity_BV421_A_5\": \"0.5_5_median_intensity\"})\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.reset_index()\n",
        "df_subset_1_6 = df_median_1_6.loc[:, ['population_number', 'normalized_intensity_BV421_A_6'] ]\n",
        "df_median_1_6_grouped = df_median_1_6.groupby('population_number').median()\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.rename(columns={\"normalized_intensity_BV421_A_6\": \"0.5_6_median_intensity\"})\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.reset_index()\n",
        "df_subset_1_7 = df_median_1_7.loc[:, ['population_number', 'normalized_intensity_BV421_A_7'] ]\n",
        "df_median_1_7_grouped = df_median_1_7.groupby('population_number').median()\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.rename(columns={\"normalized_intensity_BV421_A_7\": \"0.5_7_median_intensity\"})\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.reset_index()\n",
        "df_subset_1_8 = df_median_1_8.loc[:, ['population_number', 'normalized_intensity_BV421_A_8'] ]\n",
        "df_median_1_8_grouped = df_median_1_8.groupby('population_number').median()\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.rename(columns={\"normalized_intensity_BV421_A_8\": \"0.5_8_median_intensity\"})\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.reset_index()\n",
        "df_subset_1_9 = df_median_1_9.loc[:, ['population_number', 'normalized_intensity_BV421_A_9'] ]\n",
        "df_median_1_9_grouped = df_median_1_9.groupby('population_number').median()\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.rename(columns={\"normalized_intensity_BV421_A_9\": \"0.5_9_median_intensity\"})\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.reset_index()\n",
        "df_subset_1_10 = df_median_1_10.loc[:, ['population_number', 'normalized_intensity_BV421_A_10'] ]\n",
        "df_median_1_10_grouped = df_median_1_10.groupby('population_number').median()\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.rename(columns={\"normalized_intensity_BV421_A_10\": \"0.5_10_median_intensity\"})\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.reset_index()\n",
        "df_subset_1_11 = df_median_1_11.loc[:, ['population_number', 'normalized_intensity_BV421_A_11'] ]\n",
        "df_median_1_11_grouped = df_median_1_11.groupby('population_number').median()\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.rename(columns={\"normalized_intensity_BV421_A_11\": \"0.5_11_median_intensity\"})\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.reset_index()\n",
        "\n",
        "df_subset_2_1 = df_median_2_1.loc[:, ['population_number', 'normalized_intensity_FITC_A_1'] ]\n",
        "df_median_2_1_grouped = df_median_2_1.groupby('population_number').median()\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.rename(columns={\"normalized_intensity_FITC_A_1\": \"0.8_1_median_intensity\"})\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.reset_index()\n",
        "df_subset_2_2 = df_median_2_2.loc[:, ['population_number', 'normalized_intensity_FITC_A_2'] ]\n",
        "df_median_2_2_grouped = df_median_2_2.groupby('population_number').median()\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.rename(columns={\"normalized_intensity_FITC_A_2\": \"0.8_2_median_intensity\"})\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.reset_index()\n",
        "df_subset_2_3 = df_median_2_3.loc[:, ['population_number', 'normalized_intensity_FITC_A_3'] ]\n",
        "df_median_2_3_grouped = df_median_2_3.groupby('population_number').median()\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.rename(columns= {\"normalized_intensity_FITC_A_3\": \"0.8_3_median_intensity\"})\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.reset_index()\n",
        "df_subset_2_4 = df_median_2_4.loc[:, ['population_number', 'normalized_intensity_FITC_A_4'] ]\n",
        "df_median_2_4_grouped = df_median_2_4.groupby('population_number').median()\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.rename(columns={\"normalized_intensity_FITC_A_4\": \"0.8_4_median_intensity\"})\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.reset_index()\n",
        "df_subset_2_5 = df_median_2_5.loc[:, ['population_number', 'normalized_intensity_FITC_A_5'] ]\n",
        "df_median_2_5_grouped = df_median_2_5.groupby('population_number').median()\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.rename(columns={\"normalized_intensity_FITC_A_5\": \"0.8_5_median_intensity\"})\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.reset_index()\n",
        "df_subset_2_6 = df_median_2_6.loc[:, ['population_number', 'normalized_intensity_FITC_A_6'] ]\n",
        "df_median_2_6_grouped = df_median_2_6.groupby('population_number').median()\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.rename(columns={\"normalized_intensity_FITC_A_6\": \"0.8_6_median_intensity\"})\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.reset_index()\n",
        "df_subset_2_7 = df_median_2_7.loc[:, ['population_number', 'normalized_intensity_FITC_A_7'] ]\n",
        "df_median_2_7_grouped = df_median_2_7.groupby('population_number').median()\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.rename(columns={\"normalized_intensity_FITC_A_7\": \"0.8_7_median_intensity\"})\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.reset_index()\n",
        "df_subset_2_8 = df_median_2_8.loc[:, ['population_number', 'normalized_intensity_FITC_A_8'] ]\n",
        "df_median_2_8_grouped = df_median_2_8.groupby('population_number').median()\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.rename(columns={\"normalized_intensity_FITC_A_8\": \"0.8_8_median_intensity\"})\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.reset_index()\n",
        "df_subset_2_9 = df_median_2_9.loc[:, ['population_number', 'normalized_intensity_FITC_A_9'] ]\n",
        "df_median_2_9_grouped = df_median_2_9.groupby('population_number').median()\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.rename(columns={\"normalized_intensity_FITC_A_9\": \"0.8_9_median_intensity\"})\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.reset_index()\n",
        "df_subset_2_10 = df_median_2_10.loc[:, ['population_number', 'normalized_intensity_FITC_A_10'] ]\n",
        "df_median_2_10_grouped = df_median_2_10.groupby('population_number').median()\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.rename(columns={\"normalized_intensity_FITC_A_10\": \"0.8_10_median_intensity\"})\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.reset_index()\n",
        "df_subset_2_11 = df_median_2_11.loc[:, ['population_number', 'normalized_intensity_FITC_A_11'] ]\n",
        "df_median_2_11_grouped = df_median_2_11.groupby('population_number').median()\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.rename(columns={\"normalized_intensity_FITC_A_11\": \"0.8_11_median_intensity\"})\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.reset_index()\n",
        "\n",
        "df_subset_3_1 = df_median_3_1.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_1'] ]\n",
        "df_median_3_1_grouped = df_median_3_1.groupby('population_number').median()\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_1\": \"2.4_1_median_intensity\"})\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.reset_index()\n",
        "df_subset_3_2 = df_median_3_2.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_2'] ]\n",
        "df_median_3_2_grouped = df_median_3_2.groupby('population_number').median()\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_2\": \"2.4_2_median_intensity\"})\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.reset_index()\n",
        "df_subset_3_3 = df_median_3_3.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_3'] ]\n",
        "df_median_3_3_grouped = df_median_3_3.groupby('population_number').median()\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.rename(columns= {\"normalized_intensity_PE_Texas_Red_A_3\": \"2.4_3_median_intensity\"})\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.reset_index()\n",
        "df_subset_3_4 = df_median_3_4.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_4'] ]\n",
        "df_median_3_4_grouped = df_median_3_4.groupby('population_number').median()\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_4\": \"2.4_4_median_intensity\"})\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.reset_index()\n",
        "df_subset_3_5 = df_median_3_5.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_5'] ]\n",
        "df_median_3_5_grouped = df_median_3_5.groupby('population_number').median()\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_5\": \"2.4_5_median_intensity\"})\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.reset_index()\n",
        "df_subset_3_6 = df_median_3_6.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_6'] ]\n",
        "df_median_3_6_grouped = df_median_3_6.groupby('population_number').median()\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_6\": \"2.4_6_median_intensity\"})\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.reset_index()\n",
        "df_subset_3_7 = df_median_3_7.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_7'] ]\n",
        "df_median_3_7_grouped = df_median_3_7.groupby('population_number').median()\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_7\": \"2.4_7_median_intensity\"})\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.reset_index()\n",
        "df_subset_3_8 = df_median_3_8.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_8'] ]\n",
        "df_median_3_8_grouped = df_median_3_8.groupby('population_number').median()\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_8\": \"2.4_8_median_intensity\"})\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.reset_index()\n",
        "df_subset_3_9 = df_median_3_9.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_9'] ]\n",
        "df_median_3_9_grouped = df_median_3_9.groupby('population_number').median()\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_9\": \"2.4_9_median_intensity\"})\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.reset_index()\n",
        "df_subset_3_10 = df_median_3_10.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_10'] ]\n",
        "df_median_3_10_grouped = df_median_3_10.groupby('population_number').median()\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_10\": \"2.4_10_median_intensity\"})\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.reset_index()\n",
        "df_subset_3_11 = df_median_3_11.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_11'] ]\n",
        "df_median_3_11_grouped = df_median_3_11.groupby('population_number').median()\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_11\": \"2.4_11_median_intensity\"})\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.reset_index()\n",
        "\n",
        "df_subset_4_1 = df_median_4_1.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_1'] ]\n",
        "df_median_4_1_grouped = df_median_4_1.groupby('population_number').median()\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_1\": \"3.36_1_median_intensity\"})\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.reset_index()\n",
        "df_subset_4_2 = df_median_4_2.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_2'] ]\n",
        "df_median_4_2_grouped = df_median_4_2.groupby('population_number').median()\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_2\": \"3.36_2_median_intensity\"})\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.reset_index()\n",
        "df_subset_4_3 = df_median_4_3.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_3'] ]\n",
        "df_median_4_3_grouped = df_median_4_3.groupby('population_number').median()\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.rename(columns= {\"normalized_intensity_Alexa_Fluor_700_A_3\": \"3.36_3_median_intensity\"})\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.reset_index()\n",
        "df_subset_4_4 = df_median_4_4.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_4'] ]\n",
        "df_median_4_4_grouped = df_median_4_4.groupby('population_number').median()\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_4\": \"3.36_4_median_intensity\"})\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.reset_index()\n",
        "df_subset_4_5 = df_median_4_5.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_5'] ]\n",
        "df_median_4_5_grouped = df_median_4_5.groupby('population_number').median()\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_5\": \"3.36_5_median_intensity\"})\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.reset_index()\n",
        "df_subset_4_6 = df_median_4_6.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_6'] ]\n",
        "df_median_4_6_grouped = df_median_4_6.groupby('population_number').median()\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_6\": \"3.36_6_median_intensity\"})\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.reset_index()\n",
        "df_subset_4_7 = df_median_4_7.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_7'] ]\n",
        "df_median_4_7_grouped = df_median_4_7.groupby('population_number').median()\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_7\": \"3.36_7_median_intensity\"})\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.reset_index()\n",
        "df_subset_4_8 = df_median_4_8.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_8'] ]\n",
        "df_median_4_8_grouped = df_median_4_8.groupby('population_number').median()\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_8\": \"3.36_8_median_intensity\"})\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.reset_index()\n",
        "df_subset_4_9 = df_median_4_9.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_9'] ]\n",
        "df_median_4_9_grouped = df_median_4_9.groupby('population_number').median()\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_9\": \"3.36_9_median_intensity\"})\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.reset_index()\n",
        "df_subset_4_10 = df_median_4_10.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_10'] ]\n",
        "df_median_4_10_grouped = df_median_4_10.groupby('population_number').median()\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_10\": \"3.36_10_median_intensity\"})\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.reset_index()\n",
        "df_subset_4_11 = df_median_4_11.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_11'] ]\n",
        "df_median_4_11_grouped = df_median_4_11.groupby('population_number').median()\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_11\": \"3.36_11_median_intensity\"})\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.reset_index()\n",
        "\n",
        "df_subset_5_1 = df_median_5_1.loc[:, ['population_number', 'normalized_intensity_PerCP_A_1']]\n",
        "df_median_5_1_grouped = df_median_5_1.groupby('population_number').median()\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.rename(columns={\"normalized_intensity_PerCP_A_1\": \"0.04_1_median_intensity\"})\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.reset_index()\n",
        "df_subset_5_2 = df_median_5_2.loc[:, ['population_number', 'normalized_intensity_PerCP_A_2'] ]\n",
        "df_median_5_2_grouped = df_median_5_2.groupby('population_number').median()\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.rename(columns={\"normalized_intensity_PerCP_A_2\": \"0.04_2_median_intensity\"})\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.reset_index()\n",
        "df_subset_5_3 = df_median_5_3.loc[:, ['population_number', 'normalized_intensity_PerCP_A_3'] ]\n",
        "df_median_5_3_grouped = df_median_5_3.groupby('population_number').median()\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.rename(columns= {\"normalized_intensity_PerCP_A_3\": \"0.04_3_median_intensity\"})\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.reset_index()\n",
        "df_subset_5_4 = df_median_5_4.loc[:, ['population_number', 'normalized_intensity_PerCP_A_4'] ]\n",
        "df_median_5_4_grouped = df_median_5_4.groupby('population_number').median()\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.rename(columns={\"normalized_intensity_PerCP_A_4\": \"0.04_4_median_intensity\"})\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.reset_index()\n",
        "df_subset_5_5 = df_median_5_5.loc[:, ['population_number', 'normalized_intensity_PerCP_A_5'] ]\n",
        "df_median_5_5_grouped = df_median_5_5.groupby('population_number').median()\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.rename(columns={\"normalized_intensity_PerCP_A_5\": \"0.04_5_median_intensity\"})\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.reset_index()\n",
        "df_subset_5_6 = df_median_5_6.loc[:, ['population_number', 'normalized_intensity_PerCP_A_6'] ]\n",
        "df_median_5_6_grouped = df_median_5_6.groupby('population_number').median()\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.rename(columns={\"normalized_intensity_PerCP_A_6\": \"0.04_6_median_intensity\"})\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.reset_index()\n",
        "df_subset_5_7 = df_median_5_7.loc[:, ['population_number', 'normalized_intensity_PerCP_A_7'] ]\n",
        "df_median_5_7_grouped = df_median_5_7.groupby('population_number').median()\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.rename(columns={\"normalized_intensity_PerCP_A_7\": \"0.04_7_median_intensity\"})\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.reset_index()\n",
        "df_subset_5_8 = df_median_5_8.loc[:, ['population_number', 'normalized_intensity_PerCP_A_8'] ]\n",
        "df_median_5_8_grouped = df_median_5_8.groupby('population_number').median()\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.rename(columns={\"normalized_intensity_PerCP_A_8\": \"0.04_8_median_intensity\"})\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.reset_index()\n",
        "df_subset_5_9 = df_median_5_9.loc[:, ['population_number', 'normalized_intensity_PerCP_A_9'] ]\n",
        "df_median_5_9_grouped = df_median_5_9.groupby('population_number').median()\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.rename(columns={\"normalized_intensity_PerCP_A_9\": \"0.04_9_median_intensity\"})\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.reset_index()\n",
        "df_subset_5_10 = df_median_5_10.loc[:, ['population_number', 'normalized_intensity_PerCP_A_10'] ]\n",
        "df_median_5_10_grouped = df_median_5_10.groupby('population_number').median()\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.rename(columns={\"normalized_intensity_PerCP_A_10\": \"0.04_10_median_intensity\"})\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.reset_index()\n",
        "df_subset_5_11 = df_median_5_11.loc[:, ['population_number', 'normalized_intensity_PerCP_A_11'] ]\n",
        "df_median_5_11_grouped = df_median_5_11.groupby('population_number').median()\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.rename(columns={\"normalized_intensity_PerCP_A_11\": \"0.04_11_median_intensity\"})\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnmeA4Cc61BO"
      },
      "outputs": [],
      "source": [
        "# returning the zeros that were filtered out to the dfs\n",
        "pop_1_log_grouped[['0.5_1_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_1_grouped.population_number, df_median_1_1_grouped['0.5_1_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_2_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_2_grouped.population_number, df_median_1_2_grouped['0.5_2_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_3_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_3_grouped.population_number, df_median_1_3_grouped['0.5_3_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_4_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_4_grouped.population_number, df_median_1_4_grouped['0.5_4_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_5_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_5_grouped.population_number, df_median_1_5_grouped['0.5_5_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_6_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_6_grouped.population_number, df_median_1_6_grouped['0.5_6_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_7_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_7_grouped.population_number, df_median_1_7_grouped['0.5_7_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_8_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_8_grouped.population_number, df_median_1_8_grouped['0.5_8_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_9_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_9_grouped.population_number, df_median_1_9_grouped['0.5_9_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_10_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_10_grouped.population_number, df_median_1_10_grouped['0.5_10_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.5_11_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_11_grouped.population_number, df_median_1_11_grouped['0.5_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_log_grouped[['0.8_1_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_1_grouped.population_number, df_median_2_1_grouped['0.8_1_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_2_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_2_grouped.population_number, df_median_2_2_grouped['0.8_2_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_3_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_3_grouped.population_number, df_median_2_3_grouped['0.8_3_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_4_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_4_grouped.population_number, df_median_2_4_grouped['0.8_4_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_5_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_5_grouped.population_number, df_median_2_5_grouped['0.8_5_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_6_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_6_grouped.population_number, df_median_2_6_grouped['0.8_6_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_7_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_7_grouped.population_number, df_median_2_7_grouped['0.8_7_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_8_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_8_grouped.population_number, df_median_2_8_grouped['0.8_8_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_9_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_9_grouped.population_number, df_median_2_9_grouped['0.8_9_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_10_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_10_grouped.population_number, df_median_2_10_grouped['0.8_10_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.8_11_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_11_grouped.population_number, df_median_2_11_grouped['0.8_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_log_grouped[['2.4_1_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_1_grouped.population_number, df_median_3_1_grouped['2.4_1_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_2_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_2_grouped.population_number, df_median_3_2_grouped['2.4_2_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_3_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_3_grouped.population_number, df_median_3_3_grouped['2.4_3_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_4_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_4_grouped.population_number, df_median_3_4_grouped['2.4_4_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_5_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_5_grouped.population_number, df_median_3_5_grouped['2.4_5_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_6_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_6_grouped.population_number, df_median_3_6_grouped['2.4_6_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_7_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_7_grouped.population_number, df_median_3_7_grouped['2.4_7_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_8_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_8_grouped.population_number, df_median_3_8_grouped['2.4_8_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_9_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_9_grouped.population_number, df_median_3_9_grouped['2.4_9_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_10_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_10_grouped.population_number, df_median_3_10_grouped['2.4_10_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['2.4_11_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_11_grouped.population_number, df_median_3_11_grouped['2.4_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_log_grouped[['3.36_1_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_1_grouped.population_number, df_median_4_1_grouped['3.36_1_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_2_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_2_grouped.population_number, df_median_4_2_grouped['3.36_2_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_3_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_3_grouped.population_number, df_median_4_3_grouped['3.36_3_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_4_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_4_grouped.population_number, df_median_4_4_grouped['3.36_4_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_5_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_5_grouped.population_number, df_median_4_5_grouped['3.36_5_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_6_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_6_grouped.population_number, df_median_4_6_grouped['3.36_6_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_7_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_7_grouped.population_number, df_median_4_7_grouped['3.36_7_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_8_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_8_grouped.population_number, df_median_4_8_grouped['3.36_8_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_9_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_9_grouped.population_number, df_median_4_9_grouped['3.36_9_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_10_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_10_grouped.population_number, df_median_4_10_grouped['3.36_10_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['3.36_11_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_11_grouped.population_number, df_median_4_11_grouped['3.36_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_log_grouped[['0.04_1_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_1_grouped.population_number, df_median_5_1_grouped['0.04_1_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_2_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_2_grouped.population_number, df_median_5_2_grouped['0.04_2_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_3_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_3_grouped.population_number, df_median_5_3_grouped['0.04_3_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_4_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_4_grouped.population_number, df_median_5_4_grouped['0.04_4_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_5_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_5_grouped.population_number, df_median_5_5_grouped['0.04_5_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_6_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_6_grouped.population_number, df_median_5_6_grouped['0.04_6_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_7_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_7_grouped.population_number, df_median_5_7_grouped['0.04_7_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_8_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_8_grouped.population_number, df_median_5_8_grouped['0.04_8_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_9_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_9_grouped.population_number, df_median_5_9_grouped['0.04_9_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_10_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_10_grouped.population_number, df_median_5_10_grouped['0.04_10_median_intensity']), axis = 1)\n",
        "pop_1_log_grouped[['0.04_11_median_intensity']] = pop_1_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_11_grouped.population_number, df_median_5_11_grouped['0.04_11_median_intensity']), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQd3zYJX61BO"
      },
      "outputs": [],
      "source": [
        "# removing float groups values\n",
        "pop_1_log_grouped.drop(pop_1_log_grouped[pop_1_log_grouped['group'] > pop_1_log_grouped['group'].apply(np.floor)].index, inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS2UfLTV61BO"
      },
      "outputs": [],
      "source": [
        "# saving the pop 1 uptake dataset\n",
        "pop_1_log_grouped.to_csv('H460_log_grouped_filtered_final.csv') # name the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 1 - testing dataframe"
      ],
      "metadata": {
        "id": "DIm80U7r4-Sb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QPYoDCgblN9"
      },
      "outputs": [],
      "source": [
        "# apply bins function on the different particle features\n",
        "Total_pop_1_test_log[['BV421_A_1', 'BV421_A_2', 'BV421_A_3', 'BV421_A_4', 'BV421_A_5', 'BV421_A_6', 'BV421_A_7', 'BV421_A_8', 'BV421_A_9', 'BV421_A_10', 'BV421_A_11']] = Total_pop_1_test_log['BV421_A'].apply(bins)\n",
        "Total_pop_1_test_log[['FITC_A_1', 'FITC_A_2', 'FITC_A_3', 'FITC_A_4', 'FITC_A_5', 'FITC_A_6', 'FITC_A_7', 'FITC_A_8', 'FITC_A_9', 'FITC_A_10', 'FITC_A_11']] = Total_pop_1_test_log['FITC_A'].apply(bins)\n",
        "Total_pop_1_test_log[['PE_Texas_Red_A_1', 'PE_Texas_Red_A_2', 'PE_Texas_Red_A_3', 'PE_Texas_Red_A_4', 'PE_Texas_Red_A_5', 'PE_Texas_Red_A_6', 'PE_Texas_Red_A_7', 'PE_Texas_Red_A_8', 'PE_Texas_Red_A_9', 'PE_Texas_Red_A_10', 'PE_Texas_Red_A_11']] = Total_pop_1_test_log['PE_Texas_Red_A'].apply(bins)\n",
        "Total_pop_1_test_log[['Alexa_Fluor_700_A_1', 'Alexa_Fluor_700_A_2', 'Alexa_Fluor_700_A_3', 'Alexa_Fluor_700_A_4', 'Alexa_Fluor_700_A_5', 'Alexa_Fluor_700_A_6', 'Alexa_Fluor_700_A_7', 'Alexa_Fluor_700_A_8', 'Alexa_Fluor_700_A_9', 'Alexa_Fluor_700_A_10', 'Alexa_Fluor_700_A_11']] = Total_pop_1_test_log['Alexa_Fluor_700_A'].apply(bins)\n",
        "Total_pop_1_test_log[['PerCP_A_1', 'PerCP_A_2', 'PerCP_A_3', 'PerCP_A_4', 'PerCP_A_5', 'PerCP_A_6', 'PerCP_A_7', 'PerCP_A_8', 'PerCP_A_9', 'PerCP_A_10', 'PerCP_A_11']] = Total_pop_1_test_log['PerCP_A'].apply(bins)\n",
        "Total_pop_1_test_log.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK9ZAZ2Zbmfy"
      },
      "outputs": [],
      "source": [
        "# applying bins intensity function on the particle features\n",
        "Total_pop_1_test_log[['0.5_1_intensity', '0.5_2_intensity', '0.5_3_intensity', '0.5_4_intensity', '0.5_5_intensity', '0.5_6_intensity', '0.5_7_intensity', '0.5_8_intensity', '0.5_9_intensity', '0.5_10_intensity', '0.5_11_intensity']] = Total_pop_1_test_log.apply(lambda x: bins_intensity(x.BV421_A, x.BV421_A_1, x.BV421_A_2, x.BV421_A_3, x.BV421_A_4, x.BV421_A_5, x.BV421_A_6, x.BV421_A_7, x.BV421_A_8, x.BV421_A_9, x.BV421_A_10, x.BV421_A_11), axis=1)\n",
        "Total_pop_1_test_log[['0.8_1_intensity', '0.8_2_intensity', '0.8_3_intensity', '0.8_4_intensity', '0.8_5_intensity', '0.8_6_intensity', '0.8_7_intensity', '0.8_8_intensity', '0.8_9_intensity', '0.8_10_intensity', '0.8_11_intensity']] = Total_pop_1_test_log.apply(lambda x: bins_intensity(x.FITC_A, x.FITC_A_1, x.FITC_A_2, x.FITC_A_3, x.FITC_A_4, x.FITC_A_5, x.FITC_A_6, x.FITC_A_7, x.FITC_A_8, x.FITC_A_9, x.FITC_A_10, x.FITC_A_11), axis=1)\n",
        "Total_pop_1_test_log[['2.4_1_intensity', '2.4_2_intensity', '2.4_3_intensity', '2.4_4_intensity', '2.4_5_intensity', '2.4_6_intensity', '2.4_7_intensity', '2.4_8_intensity', '2.4_9_intensity', '2.4_10_intensity', '2.4_11_intensity']] = Total_pop_1_test_log.apply(lambda x: bins_intensity(x.PE_Texas_Red_A, x.PE_Texas_Red_A_1, x.PE_Texas_Red_A_2, x.PE_Texas_Red_A_3, x.PE_Texas_Red_A_4, x.PE_Texas_Red_A_5, x.PE_Texas_Red_A_6, x.PE_Texas_Red_A_7, x.PE_Texas_Red_A_8, x.PE_Texas_Red_A_9, x.PE_Texas_Red_A_10, x.PE_Texas_Red_A_11), axis=1)\n",
        "Total_pop_1_test_log[['3.36_1_intensity', '3.36_2_intensity', '3.36_3_intensity', '3.36_4_intensity', '3.36_5_intensity', '3.36_6_intensity', '3.36_7_intensity', '3.36_8_intensity', '3.36_9_intensity', '3.36_10_intensity', '3.36_11_intensity']] = Total_pop_1_test_log.apply(lambda x: bins_intensity(x.Alexa_Fluor_700_A, x.Alexa_Fluor_700_A_1, x.Alexa_Fluor_700_A_2, x.Alexa_Fluor_700_A_3, x.Alexa_Fluor_700_A_4, x.Alexa_Fluor_700_A_5, x.Alexa_Fluor_700_A_6, x.Alexa_Fluor_700_A_7, x.Alexa_Fluor_700_A_8, x.Alexa_Fluor_700_A_9, x.Alexa_Fluor_700_A_10, x.Alexa_Fluor_700_A_11), axis=1)\n",
        "Total_pop_1_test_log[['0.04_1_intensity', '0.04_2_intensity', '0.04_3_intensity', '0.04_4_intensity', '0.04_5_intensity', '0.04_6_intensity', '0.04_7_intensity', '0.04_8_intensity', '0.04_9_intensity', '0.04_10_intensity', '0.04_11_intensity']] = Total_pop_1_test_log.apply(lambda x: bins_intensity(x.PerCP_A, x.PerCP_A_1, x.PerCP_A_2, x.PerCP_A_3, x.PerCP_A_4, x.PerCP_A_5, x.PerCP_A_6, x.PerCP_A_7, x.PerCP_A_8, x.PerCP_A_9, x.PerCP_A_10, x.PerCP_A_11), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J28lQcwb9Pb"
      },
      "outputs": [],
      "source": [
        "# grouping all the relevant data per population and building new df's that include only the relevant data for analysis\n",
        "grouper = Total_pop_1_test_log.groupby(pd.Grouper(key='population_number'))\n",
        "BV421_A_1_intensity_sum = grouper['0.5_1_intensity'].sum().to_frame(name='intensity_sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_intensity_sum = grouper['0.5_2_intensity'].sum().to_frame(name='intensity_sum_BV421_A_2').reset_index()\n",
        "BV421_A_3_intensity_sum = grouper['0.5_3_intensity'].sum().to_frame(name='intensity_sum_BV421_A_3').reset_index()\n",
        "BV421_A_4_intensity_sum = grouper['0.5_4_intensity'].sum().to_frame(name='intensity_sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_intensity_sum = grouper['0.5_5_intensity'].sum().to_frame(name='intensity_sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_intensity_sum = grouper['0.5_6_intensity'].sum().to_frame(name='intensity_sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_intensity_sum = grouper['0.5_7_intensity'].sum().to_frame(name='intensity_sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_intensity_sum = grouper['0.5_8_intensity'].sum().to_frame(name='intensity_sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_intensity_sum = grouper['0.5_9_intensity'].sum().to_frame(name='intensity_sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_intensity_sum = grouper['0.5_10_intensity'].sum().to_frame(name='intensity_sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_intensity_sum = grouper['0.5_11_intensity'].sum().to_frame(name='intensity_sum_BV421_A_11').reset_index()\n",
        "BV421_A_1_sum = grouper['BV421_A_1'].sum().to_frame(name='sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_sum = grouper['BV421_A_2'].sum().to_frame(name='sum_BV421_A_2').reset_index()\n",
        "BV421_A_4_sum = grouper['BV421_A_4'].sum().to_frame(name='sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_sum = grouper['BV421_A_5'].sum().to_frame(name='sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_sum = grouper['BV421_A_6'].sum().to_frame(name='sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_sum = grouper['BV421_A_7'].sum().to_frame(name='sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_sum = grouper['BV421_A_8'].sum().to_frame(name='sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_sum = grouper['BV421_A_9'].sum().to_frame(name='sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_sum = grouper['BV421_A_10'].sum().to_frame(name='sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_sum = grouper['BV421_A_11'].sum().to_frame(name='sum_BV421_A_11').reset_index()\n",
        "\n",
        "FITC_A_1_intensity_sum = grouper['0.8_1_intensity'].sum().to_frame(name='intensity_sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_intensity_sum = grouper['0.8_2_intensity'].sum().to_frame(name='intensity_sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_intensity_sum = grouper['0.8_3_intensity'].sum().to_frame(name='intensity_sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_intensity_sum = grouper['0.8_4_intensity'].sum().to_frame(name='intensity_sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_intensity_sum = grouper['0.8_5_intensity'].sum().to_frame(name='intensity_sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_intensity_sum = grouper['0.8_6_intensity'].sum().to_frame(name='intensity_sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_intensity_sum = grouper['0.8_7_intensity'].sum().to_frame(name='intensity_sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_intensity_sum = grouper['0.8_8_intensity'].sum().to_frame(name='intensity_sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_intensity_sum = grouper['0.8_9_intensity'].sum().to_frame(name='intensity_sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_intensity_sum = grouper['0.8_10_intensity'].sum().to_frame(name='intensity_sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_intensity_sum = grouper['0.8_11_intensity'].sum().to_frame(name='intensity_sum_FITC_A_11').reset_index()\n",
        "FITC_A_1_sum = grouper['FITC_A_1'].sum().to_frame(name='sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_sum = grouper['FITC_A_2'].sum().to_frame(name='sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_sum = grouper['FITC_A_3'].sum().to_frame(name='sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_sum = grouper['FITC_A_4'].sum().to_frame(name='sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_sum = grouper['FITC_A_5'].sum().to_frame(name='sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_sum = grouper['FITC_A_6'].sum().to_frame(name='sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_sum = grouper['FITC_A_7'].sum().to_frame(name='sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_sum = grouper['FITC_A_8'].sum().to_frame(name='sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_sum = grouper['FITC_A_9'].sum().to_frame(name='sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_sum = grouper['FITC_A_10'].sum().to_frame(name='sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_sum = grouper['FITC_A_11'].sum().to_frame(name='sum_FITC_A_11').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_1_intensity_sum = grouper['2.4_1_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_intensity_sum = grouper['2.4_2_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_intensity_sum = grouper['2.4_3_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_intensity_sum = grouper['2.4_4_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_intensity_sum = grouper['2.4_5_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_intensity_sum = grouper['2.4_6_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_intensity_sum = grouper['2.4_7_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_intensity_sum = grouper['2.4_8_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_intensity_sum = grouper['2.4_9_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_intensity_sum = grouper['2.4_10_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_intensity_sum = grouper['2.4_11_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_11').reset_index()\n",
        "PE_Texas_Red_A_1_sum = grouper['PE_Texas_Red_A_1'].sum().to_frame(name='sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_sum = grouper['PE_Texas_Red_A_2'].sum().to_frame(name='sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_sum = grouper['PE_Texas_Red_A_3'].sum().to_frame(name='sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_sum = grouper['PE_Texas_Red_A_4'].sum().to_frame(name='sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_sum = grouper['PE_Texas_Red_A_5'].sum().to_frame(name='sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_sum = grouper['PE_Texas_Red_A_6'].sum().to_frame(name='sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_sum = grouper['PE_Texas_Red_A_7'].sum().to_frame(name='sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_sum = grouper['PE_Texas_Red_A_8'].sum().to_frame(name='sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_sum = grouper['PE_Texas_Red_A_9'].sum().to_frame(name='sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_sum = grouper['PE_Texas_Red_A_10'].sum().to_frame(name='sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_sum = grouper['PE_Texas_Red_A_11'].sum().to_frame(name='sum_PE_Texas_Red_A_11').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_1_intensity_sum = grouper['3.36_1_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_intensity_sum = grouper['3.36_2_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_intensity_sum = grouper['3.36_3_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_intensity_sum = grouper['3.36_4_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_intensity_sum = grouper['3.36_5_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_intensity_sum = grouper['3.36_6_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_intensity_sum = grouper['3.36_7_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_intensity_sum = grouper['3.36_8_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_intensity_sum = grouper['3.36_9_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_intensity_sum = grouper['3.36_10_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_intensity_sum = grouper['3.36_11_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "Alexa_Fluor_700_A_1_sum = grouper['Alexa_Fluor_700_A_1'].sum().to_frame(name='sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_sum = grouper['Alexa_Fluor_700_A_2'].sum().to_frame(name='sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_sum = grouper['Alexa_Fluor_700_A_3'].sum().to_frame(name='sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_sum = grouper['Alexa_Fluor_700_A_4'].sum().to_frame(name='sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_sum = grouper['Alexa_Fluor_700_A_5'].sum().to_frame(name='sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_sum = grouper['Alexa_Fluor_700_A_6'].sum().to_frame(name='sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_sum = grouper['Alexa_Fluor_700_A_7'].sum().to_frame(name='sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_sum = grouper['Alexa_Fluor_700_A_8'].sum().to_frame(name='sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_sum = grouper['Alexa_Fluor_700_A_9'].sum().to_frame(name='sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_sum = grouper['Alexa_Fluor_700_A_10'].sum().to_frame(name='sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_sum = grouper['Alexa_Fluor_700_A_11'].sum().to_frame(name='sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "\n",
        "PerCP_A_1_intensity_sum = grouper['0.04_1_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_intensity_sum = grouper['0.04_2_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_intensity_sum = grouper['0.04_3_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_intensity_sum = grouper['0.04_4_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_intensity_sum = grouper['0.04_5_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_intensity_sum = grouper['0.04_6_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_intensity_sum = grouper['0.04_7_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_intensity_sum = grouper['0.04_8_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_intensity_sum = grouper['0.04_9_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_intensity_sum = grouper['0.04_10_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_intensity_sum = grouper['0.04_11_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_11').reset_index()\n",
        "PerCP_A_1_sum = grouper['PerCP_A_1'].sum().to_frame(name='sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_sum = grouper['PerCP_A_2'].sum().to_frame(name='sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_sum = grouper['PerCP_A_3'].sum().to_frame(name='sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_sum = grouper['PerCP_A_4'].sum().to_frame(name='sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_sum = grouper['PerCP_A_5'].sum().to_frame(name='sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_sum = grouper['PerCP_A_6'].sum().to_frame(name='sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_sum = grouper['PerCP_A_7'].sum().to_frame(name='sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_sum = grouper['PerCP_A_8'].sum().to_frame(name='sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_sum = grouper['PerCP_A_9'].sum().to_frame(name='sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_sum = grouper['PerCP_A_10'].sum().to_frame(name='sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_sum = grouper['PerCP_A_11'].sum().to_frame(name='sum_PerCP_A_11').reset_index()\n",
        "Cell_line = grouper['Cell_line'].mean().to_frame(name='Cell_line').reset_index()\n",
        "\n",
        "mean_FSC_A = grouper['FSC_A'].mean().to_frame(name='FSC_A_mean').reset_index()\n",
        "mean_FSC_H = grouper['FSC_H'].mean().to_frame(name='FSC_H_mean').reset_index()\n",
        "mean_FSC_W = grouper['FSC_W'].mean().to_frame(name='FSC_W_mean').reset_index()\n",
        "mean_SSC_A = grouper['SSC_A'].mean().to_frame(name='SSC_A_mean').reset_index()\n",
        "mean_SSC_H = grouper['SSC_H'].mean().to_frame(name='SSC_H_mean').reset_index()\n",
        "mean_SSC_W = grouper['SSC_W'].mean().to_frame(name='SSC_W_mean').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_NSrVUKcGM8"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_1_test_log_grouper = [BV421_A_1_intensity_sum, BV421_A_2_intensity_sum, BV421_A_3_intensity_sum, BV421_A_4_intensity_sum, BV421_A_5_intensity_sum,\n",
        "         BV421_A_6_intensity_sum, BV421_A_7_intensity_sum, BV421_A_8_intensity_sum, BV421_A_9_intensity_sum, BV421_A_10_intensity_sum, BV421_A_11_intensity_sum,\n",
        "         BV421_A_1_sum, BV421_A_2_sum, BV421_A_3_sum, BV421_A_4_sum, BV421_A_5_sum, BV421_A_6_sum, BV421_A_7_sum, BV421_A_8_sum, BV421_A_9_sum, BV421_A_10_sum, BV421_A_11_sum,\n",
        "         FITC_A_1_intensity_sum, FITC_A_2_intensity_sum, FITC_A_3_intensity_sum, FITC_A_4_intensity_sum, FITC_A_5_intensity_sum, FITC_A_6_intensity_sum, FITC_A_7_intensity_sum,\n",
        "         FITC_A_8_intensity_sum, FITC_A_9_intensity_sum, FITC_A_10_intensity_sum, FITC_A_11_intensity_sum, FITC_A_1_sum, FITC_A_2_sum, FITC_A_3_sum, FITC_A_4_sum, FITC_A_5_sum,\n",
        "         FITC_A_6_sum, FITC_A_7_sum, FITC_A_8_sum, FITC_A_9_sum, FITC_A_10_sum, FITC_A_11_sum,PE_Texas_Red_A_1_intensity_sum, PE_Texas_Red_A_2_intensity_sum, PE_Texas_Red_A_3_intensity_sum,\n",
        "         PE_Texas_Red_A_4_intensity_sum, PE_Texas_Red_A_5_intensity_sum, PE_Texas_Red_A_6_intensity_sum, PE_Texas_Red_A_7_intensity_sum, PE_Texas_Red_A_8_intensity_sum, PE_Texas_Red_A_9_intensity_sum,\n",
        "         PE_Texas_Red_A_10_intensity_sum, PE_Texas_Red_A_11_intensity_sum, PE_Texas_Red_A_1_sum, PE_Texas_Red_A_2_sum, PE_Texas_Red_A_3_sum, PE_Texas_Red_A_4_sum, PE_Texas_Red_A_5_sum,\n",
        "         PE_Texas_Red_A_6_sum, PE_Texas_Red_A_7_sum, PE_Texas_Red_A_8_sum, PE_Texas_Red_A_9_sum, PE_Texas_Red_A_10_sum, PE_Texas_Red_A_11_sum,\n",
        "         Alexa_Fluor_700_A_1_intensity_sum, Alexa_Fluor_700_A_2_intensity_sum, Alexa_Fluor_700_A_3_intensity_sum, Alexa_Fluor_700_A_4_intensity_sum, Alexa_Fluor_700_A_5_intensity_sum,\n",
        "         Alexa_Fluor_700_A_6_intensity_sum, Alexa_Fluor_700_A_7_intensity_sum, Alexa_Fluor_700_A_8_intensity_sum, Alexa_Fluor_700_A_9_intensity_sum, Alexa_Fluor_700_A_10_intensity_sum, Alexa_Fluor_700_A_11_intensity_sum,\n",
        "         Alexa_Fluor_700_A_1_sum, Alexa_Fluor_700_A_2_sum, Alexa_Fluor_700_A_3_sum, Alexa_Fluor_700_A_4_sum, Alexa_Fluor_700_A_5_sum, Alexa_Fluor_700_A_6_sum, Alexa_Fluor_700_A_7_sum, Alexa_Fluor_700_A_8_sum,\n",
        "         Alexa_Fluor_700_A_9_sum, Alexa_Fluor_700_A_10_sum, Alexa_Fluor_700_A_11_sum, PerCP_A_1_intensity_sum, PerCP_A_2_intensity_sum, PerCP_A_3_intensity_sum, PerCP_A_4_intensity_sum, PerCP_A_5_intensity_sum,\n",
        "         PerCP_A_6_intensity_sum, PerCP_A_7_intensity_sum, PerCP_A_8_intensity_sum, PerCP_A_9_intensity_sum, PerCP_A_10_intensity_sum, PerCP_A_11_intensity_sum,\n",
        "         PerCP_A_1_sum, PerCP_A_2_sum, PerCP_A_3_sum, PerCP_A_4_sum, PerCP_A_5_sum, PerCP_A_6_sum, PerCP_A_7_sum, PerCP_A_8_sum, PerCP_A_9_sum, PerCP_A_10_sum, PerCP_A_11_sum,\n",
        "         mean_FSC_A, mean_FSC_H, mean_FSC_W, mean_SSC_A, mean_SSC_H, mean_SSC_W, Cell_line]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk8KTglFcGM-"
      },
      "outputs": [],
      "source": [
        "# building a new dfs of the grouped data\n",
        "pop_1_test_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_1_test_log_grouper)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalizing uptake intensities per bin\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ud39MKiTPKCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxBeW0uEcGM-"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['normalized_intensity_BV421_A_1', 'normalized_intensity_BV421_A_2', 'normalized_intensity_BV421_A_3', 'normalized_intensity_BV421_A_4'\n",
        "                  , 'normalized_intensity_BV421_A_5', 'normalized_intensity_BV421_A_6', 'normalized_intensity_BV421_A_7', 'normalized_intensity_BV421_A_8'\n",
        "                  , 'normalized_intensity_BV421_A_9', 'normalized_intensity_BV421_A_10', 'normalized_intensity_BV421_A_11']] = pop_1_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_BV421_A_1, x.intensity_sum_BV421_A_2, x.intensity_sum_BV421_A_3, x.intensity_sum_BV421_A_4, x.intensity_sum_BV421_A_5,\n",
        "                  x.intensity_sum_BV421_A_6, x.intensity_sum_BV421_A_7, x.intensity_sum_BV421_A_8, x.intensity_sum_BV421_A_9, x.intensity_sum_BV421_A_10, x.intensity_sum_BV421_A_11,\n",
        "                  x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NqgOsdMcGM_"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['normalized_intensity_FITC_A_1', 'normalized_intensity_FITC_A_2', 'normalized_intensity_FITC_A_3', 'normalized_intensity_FITC_A_4',\n",
        "                          'normalized_intensity_FITC_A_5', 'normalized_intensity_FITC_A_6', 'normalized_intensity_FITC_A_7', 'normalized_intensity_FITC_A_8',\n",
        "                          'normalized_intensity_FITC_A_9', 'normalized_intensity_FITC_A_10', 'normalized_intensity_FITC_A_11']] = pop_1_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_FITC_A_1, x.intensity_sum_FITC_A_2, x.intensity_sum_FITC_A_3, x.intensity_sum_FITC_A_4, x.intensity_sum_FITC_A_5,\n",
        "                          x.intensity_sum_FITC_A_6, x.intensity_sum_FITC_A_7, x.intensity_sum_FITC_A_8, x.intensity_sum_FITC_A_9, x.intensity_sum_FITC_A_10, x.intensity_sum_FITC_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1mWzK68cGM_"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'normalized_intensity_PE_Texas_Red_A_2', 'normalized_intensity_PE_Texas_Red_A_3', 'normalized_intensity_PE_Texas_Red_A_4',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_5', 'normalized_intensity_PE_Texas_Red_A_6', 'normalized_intensity_PE_Texas_Red_A_7', 'normalized_intensity_PE_Texas_Red_A_8',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_9','normalized_intensity_PE_Texas_Red_A_10','normalized_intensity_PE_Texas_Red_A_11']] = pop_1_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PE_Texas_Red_A_1, x.intensity_sum_PE_Texas_Red_A_2, x.intensity_sum_PE_Texas_Red_A_3, x.intensity_sum_PE_Texas_Red_A_4, x.intensity_sum_PE_Texas_Red_A_5,\n",
        "                          x.intensity_sum_PE_Texas_Red_A_6, x.intensity_sum_PE_Texas_Red_A_7, x.intensity_sum_PE_Texas_Red_A_8, x.intensity_sum_PE_Texas_Red_A_9,\n",
        "                          x.intensity_sum_PE_Texas_Red_A_10, x.intensity_sum_PE_Texas_Red_A_11, x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCf0fmnRcGM_"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'normalized_intensity_Alexa_Fluor_700_A_2', 'normalized_intensity_Alexa_Fluor_700_A_3', 'normalized_intensity_Alexa_Fluor_700_A_4',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_5', 'normalized_intensity_Alexa_Fluor_700_A_6', 'normalized_intensity_Alexa_Fluor_700_A_7', 'normalized_intensity_Alexa_Fluor_700_A_8',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_9', 'normalized_intensity_Alexa_Fluor_700_A_10', 'normalized_intensity_Alexa_Fluor_700_A_11']] = pop_1_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_Alexa_Fluor_700_A_1, x.intensity_sum_Alexa_Fluor_700_A_2, x.intensity_sum_Alexa_Fluor_700_A_3, x.intensity_sum_Alexa_Fluor_700_A_4, x.intensity_sum_Alexa_Fluor_700_A_5,\n",
        "                          x.intensity_sum_Alexa_Fluor_700_A_6, x.intensity_sum_Alexa_Fluor_700_A_7, x.intensity_sum_Alexa_Fluor_700_A_8, x.intensity_sum_Alexa_Fluor_700_A_9, x.intensity_sum_Alexa_Fluor_700_A_10, x.intensity_sum_Alexa_Fluor_700_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Z1kyZkcGNA"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['normalized_intensity_PerCP_A_1', 'normalized_intensity_PerCP_A_2', 'normalized_intensity_PerCP_A_3', 'normalized_intensity_PerCP_A_4',\n",
        "                          'normalized_intensity_PerCP_A_5', 'normalized_intensity_PerCP_A_6', 'normalized_intensity_PerCP_A_7', 'normalized_intensity_PerCP_A_8',\n",
        "                          'normalized_intensity_PerCP_A_9', 'normalized_intensity_PerCP_A_10', 'normalized_intensity_PerCP_A_11']] = pop_1_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PerCP_A_1, x.intensity_sum_PerCP_A_2, x.intensity_sum_PerCP_A_3, x.intensity_sum_PerCP_A_4, x.intensity_sum_PerCP_A_5,\n",
        "                          x.intensity_sum_PerCP_A_6, x.intensity_sum_PerCP_A_7, x.intensity_sum_PerCP_A_8, x.intensity_sum_PerCP_A_9, x.intensity_sum_PerCP_A_10, x.intensity_sum_PerCP_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating mean uptake intesities per bin"
      ],
      "metadata": {
        "id": "u6nH6QNYPVf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yObnBLMfcGNA"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['mean_intensity_0.5_1', 'mean_intensity_0.5_2', 'mean_intensity_0.5_3', 'mean_intensity_0.5_4',\n",
        "                          'mean_intensity_0.5_5', 'mean_intensity_0.5_6', 'mean_intensity_0.5_7', 'mean_intensity_0.5_8',\n",
        "                          'mean_intensity_0.5_9', 'mean_intensity_0.5_10', 'mean_intensity_0.5_11']] = pop_1_test_log_grouped.apply(lambda x: average(x.normalized_intensity_BV421_A_1, x.normalized_intensity_BV421_A_2, x.normalized_intensity_BV421_A_3, x.normalized_intensity_BV421_A_4, x.normalized_intensity_BV421_A_5,\n",
        "                          x.normalized_intensity_BV421_A_6, x.normalized_intensity_BV421_A_7, x.normalized_intensity_BV421_A_8, x.normalized_intensity_BV421_A_9, x.normalized_intensity_BV421_A_10, x.normalized_intensity_BV421_A_11,\n",
        "                          x.sum_BV421_A_1, x.sum_BV421_A_2, x.sum_BV421_A_3, x.sum_BV421_A_4, x.sum_BV421_A_5, x.sum_BV421_A_6, x.sum_BV421_A_7, x.sum_BV421_A_8,x.sum_BV421_A_9, x.sum_BV421_A_10, x.sum_BV421_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuMlW10ucGNB"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['mean_intensity_0.8_1', 'mean_intensity_0.8_2', 'mean_intensity_0.8_3', 'mean_intensity_0.8_4',\n",
        "                          'mean_intensity_0.8_5', 'mean_intensity_0.8_6', 'mean_intensity_0.8_7', 'mean_intensity_0.8_8',\n",
        "                          'mean_intensity_0.8_9', 'mean_intensity_0.8_10', 'mean_intensity_0.8_11']] = pop_1_test_log_grouped.apply(lambda x: average(x.normalized_intensity_FITC_A_1, x.normalized_intensity_FITC_A_2, x.normalized_intensity_FITC_A_3, x.normalized_intensity_FITC_A_4, x.normalized_intensity_FITC_A_5,\n",
        "                          x.normalized_intensity_FITC_A_6, x.normalized_intensity_FITC_A_7, x.normalized_intensity_FITC_A_8, x.normalized_intensity_FITC_A_9, x.normalized_intensity_FITC_A_10, x.normalized_intensity_FITC_A_11,\n",
        "                          x.sum_FITC_A_1, x.sum_FITC_A_2, x.sum_FITC_A_3, x.sum_FITC_A_4, x.sum_FITC_A_5, x.sum_FITC_A_6, x.sum_FITC_A_7, x.sum_FITC_A_8, x.sum_FITC_A_9, x.sum_FITC_A_10, x.sum_FITC_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToKypkXucGNB"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['mean_intensity_2.4_1', 'mean_intensity_2.4_2', 'mean_intensity_2.4_3', 'mean_intensity_2.4_4',\n",
        "                          'mean_intensity_2.4_5', 'mean_intensity_2.4_6', 'mean_intensity_2.4_7', 'mean_intensity_2.4_8',\n",
        "                          'mean_intensity_2.4_9', 'mean_intensity_2.4_10', 'mean_intensity_2.4_11']] = pop_1_test_log_grouped.apply(lambda x: average(x.normalized_intensity_PE_Texas_Red_A_1, x.normalized_intensity_PE_Texas_Red_A_2, x.normalized_intensity_PE_Texas_Red_A_3, x.normalized_intensity_PE_Texas_Red_A_4, x.normalized_intensity_PE_Texas_Red_A_5,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_6, x.normalized_intensity_PE_Texas_Red_A_7, x.normalized_intensity_PE_Texas_Red_A_8,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_9, x.normalized_intensity_PE_Texas_Red_A_10, x.normalized_intensity_PE_Texas_Red_A_11,\n",
        "                          x.sum_PE_Texas_Red_A_1, x.sum_PE_Texas_Red_A_2, x.sum_PE_Texas_Red_A_3, x.sum_PE_Texas_Red_A_4, x.sum_PE_Texas_Red_A_5,\n",
        "                          x.sum_PE_Texas_Red_A_6, x.sum_PE_Texas_Red_A_7, x.sum_PE_Texas_Red_A_8, x.sum_PE_Texas_Red_A_9, x.sum_PE_Texas_Red_A_10, x.sum_PE_Texas_Red_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAt6_hoXcGNC"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['mean_intensity_3.36_1', 'mean_intensity_3.36_2', 'mean_intensity_3.36_3', 'mean_intensity_3.36_4',\n",
        "                          'mean_intensity_3.36_5', 'mean_intensity_3.36_6', 'mean_intensity_3.36_7', 'mean_intensity_3.36_8',\n",
        "                          'mean_intensity_3.36_9', 'mean_intensity_3.36_10', 'mean_intensity_3.36_11']] = pop_1_test_log_grouped.apply(lambda x: average(x.normalized_intensity_Alexa_Fluor_700_A_1, x.normalized_intensity_Alexa_Fluor_700_A_2, x.normalized_intensity_Alexa_Fluor_700_A_3, x.normalized_intensity_Alexa_Fluor_700_A_4, x.normalized_intensity_Alexa_Fluor_700_A_5,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_6, x.normalized_intensity_Alexa_Fluor_700_A_7, x.normalized_intensity_Alexa_Fluor_700_A_8,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_9, x.normalized_intensity_Alexa_Fluor_700_A_10, x.normalized_intensity_Alexa_Fluor_700_A_11,\n",
        "                          x.sum_Alexa_Fluor_700_A_1, x.sum_Alexa_Fluor_700_A_2, x.sum_Alexa_Fluor_700_A_3, x.sum_Alexa_Fluor_700_A_4, x.sum_Alexa_Fluor_700_A_5,\n",
        "                          x.sum_Alexa_Fluor_700_A_6, x.sum_Alexa_Fluor_700_A_7, x.sum_Alexa_Fluor_700_A_8, x.sum_Alexa_Fluor_700_A_9, x.sum_Alexa_Fluor_700_A_10, x.sum_Alexa_Fluor_700_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjXurxi-cGNC"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped[['mean_intensity_0.04_1', 'mean_intensity_0.04_2', 'mean_intensity_0.04_3', 'mean_intensity_0.04_4',\n",
        "                          'mean_intensity_0.04_5', 'mean_intensity_0.04_6', 'mean_intensity_0.04_7', 'mean_intensity_0.04_8',\n",
        "                          'mean_intensity_0.04_9', 'mean_intensity_0.04_10', 'mean_intensity_0.04_11']] = pop_1_test_log_grouped.apply(lambda x: average(x.normalized_intensity_PerCP_A_1, x.normalized_intensity_PerCP_A_2, x.normalized_intensity_PerCP_A_3, x.normalized_intensity_PerCP_A_4, x.normalized_intensity_PerCP_A_5,\n",
        "                          x.normalized_intensity_PerCP_A_6, x.normalized_intensity_PerCP_A_7, x.normalized_intensity_PerCP_A_8,\n",
        "                          x.normalized_intensity_PerCP_A_9, x.normalized_intensity_PerCP_A_10, x.normalized_intensity_PerCP_A_11,\n",
        "                          x.sum_PerCP_A_1, x.sum_PerCP_A_2, x.sum_PerCP_A_3, x.sum_PerCP_A_4, x.sum_PerCP_A_5,\n",
        "                          x.sum_PerCP_A_6, x.sum_PerCP_A_7, x.sum_PerCP_A_8,x.sum_PerCP_A_9, x.sum_PerCP_A_10, x.sum_PerCP_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSyBOG-qcGND"
      },
      "outputs": [],
      "source": [
        "# buliding dfs for median calculations\n",
        "df_median_1_1 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_1', 'population_number']]\n",
        "df_median_1_2 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_2', 'population_number']]\n",
        "df_median_1_3 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_3', 'population_number']]\n",
        "df_median_1_4 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_4', 'population_number']]\n",
        "df_median_1_5 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_5', 'population_number']]\n",
        "df_median_1_6 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_6', 'population_number']]\n",
        "df_median_1_7 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_7', 'population_number']]\n",
        "df_median_1_8 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_8', 'population_number']]\n",
        "df_median_1_9 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_9', 'population_number']]\n",
        "df_median_1_10 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_10', 'population_number']]\n",
        "df_median_1_11 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_11', 'population_number']]\n",
        "\n",
        "df_median_2_1 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_1', 'population_number']]\n",
        "df_median_2_2 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_2', 'population_number']]\n",
        "df_median_2_3 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_3', 'population_number']]\n",
        "df_median_2_4 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_4', 'population_number']]\n",
        "df_median_2_5 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_5', 'population_number']]\n",
        "df_median_2_6 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_6', 'population_number']]\n",
        "df_median_2_7 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_7', 'population_number']]\n",
        "df_median_2_8 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_8', 'population_number']]\n",
        "df_median_2_9 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_9', 'population_number']]\n",
        "df_median_2_10 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_10', 'population_number']]\n",
        "df_median_2_11 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_11', 'population_number']]\n",
        "\n",
        "df_median_3_1 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'population_number']]\n",
        "df_median_3_2 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_2', 'population_number']]\n",
        "df_median_3_3 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_3', 'population_number']]\n",
        "df_median_3_4 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_4', 'population_number']]\n",
        "df_median_3_5 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_5', 'population_number']]\n",
        "df_median_3_6 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_6', 'population_number']]\n",
        "df_median_3_7 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_7', 'population_number']]\n",
        "df_median_3_8 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_8', 'population_number']]\n",
        "df_median_3_9 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_9', 'population_number']]\n",
        "df_median_3_10 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_10', 'population_number']]\n",
        "df_median_3_11 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_11', 'population_number']]\n",
        "\n",
        "df_median_4_1 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'population_number']]\n",
        "df_median_4_2 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_2', 'population_number']]\n",
        "df_median_4_3 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_3', 'population_number']]\n",
        "df_median_4_4 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_4', 'population_number']]\n",
        "df_median_4_5 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_5', 'population_number']]\n",
        "df_median_4_6 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_6', 'population_number']]\n",
        "df_median_4_7 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_7', 'population_number']]\n",
        "df_median_4_8 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_8', 'population_number']]\n",
        "df_median_4_9 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_9', 'population_number']]\n",
        "df_median_4_10 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_10', 'population_number']]\n",
        "df_median_4_11 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_11', 'population_number']]\n",
        "\n",
        "df_median_5_1 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_1', 'population_number']]\n",
        "df_median_5_2 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_2', 'population_number']]\n",
        "df_median_5_3 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_3', 'population_number']]\n",
        "df_median_5_4 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_4', 'population_number']]\n",
        "df_median_5_5 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_5', 'population_number']]\n",
        "df_median_5_6 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_6', 'population_number']]\n",
        "df_median_5_7 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_7', 'population_number']]\n",
        "df_median_5_8 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_8', 'population_number']]\n",
        "df_median_5_9 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_9', 'population_number']]\n",
        "df_median_5_10 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_10', 'population_number']]\n",
        "df_median_5_11 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_11', 'population_number']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u85vd514cGND"
      },
      "outputs": [],
      "source": [
        "# filtering out zeroes\n",
        "df_median_1_1 = df_median_1_1[df_median_1_1['normalized_intensity_BV421_A_1'] > 0]\n",
        "df_median_1_2 = df_median_1_2[df_median_1_2['normalized_intensity_BV421_A_2'] > 0]\n",
        "df_median_1_3 = df_median_1_3[df_median_1_3['normalized_intensity_BV421_A_3'] > 0]\n",
        "df_median_1_4 = df_median_1_4[df_median_1_4['normalized_intensity_BV421_A_4'] > 0]\n",
        "df_median_1_5 = df_median_1_5[df_median_1_5['normalized_intensity_BV421_A_5'] > 0]\n",
        "df_median_1_6 = df_median_1_6[df_median_1_6['normalized_intensity_BV421_A_6'] > 0]\n",
        "df_median_1_7 = df_median_1_7[df_median_1_7['normalized_intensity_BV421_A_7'] > 0]\n",
        "df_median_1_8 = df_median_1_8[df_median_1_8['normalized_intensity_BV421_A_8'] > 0]\n",
        "df_median_1_9 = df_median_1_9[df_median_1_9['normalized_intensity_BV421_A_9'] > 0]\n",
        "df_median_1_10 = df_median_1_10[df_median_1_10['normalized_intensity_BV421_A_10'] > 0]\n",
        "df_median_1_11 = df_median_1_11[df_median_1_11['normalized_intensity_BV421_A_11'] > 0]\n",
        "\n",
        "df_median_2_1 = df_median_2_1[df_median_2_1['normalized_intensity_FITC_A_1'] > 0]\n",
        "df_median_2_2 = df_median_2_2[df_median_2_2['normalized_intensity_FITC_A_2'] > 0]\n",
        "df_median_2_3 = df_median_2_3[df_median_2_3['normalized_intensity_FITC_A_3'] > 0]\n",
        "df_median_2_4 = df_median_2_4[df_median_2_4['normalized_intensity_FITC_A_4'] > 0]\n",
        "df_median_2_5 = df_median_2_5[df_median_2_5['normalized_intensity_FITC_A_5'] > 0]\n",
        "df_median_2_6 = df_median_2_6[df_median_2_6['normalized_intensity_FITC_A_6'] > 0]\n",
        "df_median_2_7 = df_median_2_7[df_median_2_7['normalized_intensity_FITC_A_7'] > 0]\n",
        "df_median_2_8 = df_median_2_8[df_median_2_8['normalized_intensity_FITC_A_8'] > 0]\n",
        "df_median_2_9 = df_median_2_9[df_median_2_9['normalized_intensity_FITC_A_9'] > 0]\n",
        "df_median_2_10 = df_median_2_10[df_median_2_10['normalized_intensity_FITC_A_10'] > 0]\n",
        "df_median_2_11 = df_median_2_11[df_median_2_11['normalized_intensity_FITC_A_11'] > 0]\n",
        "\n",
        "df_median_3_1 = df_median_3_1[df_median_3_1['normalized_intensity_PE_Texas_Red_A_1'] > 0]\n",
        "df_median_3_2 = df_median_3_2[df_median_3_2['normalized_intensity_PE_Texas_Red_A_2'] > 0]\n",
        "df_median_3_3 = df_median_3_3[df_median_3_3['normalized_intensity_PE_Texas_Red_A_3'] > 0]\n",
        "df_median_3_4 = df_median_3_4[df_median_3_4['normalized_intensity_PE_Texas_Red_A_4'] > 0]\n",
        "df_median_3_5 = df_median_3_5[df_median_3_5['normalized_intensity_PE_Texas_Red_A_5'] > 0]\n",
        "df_median_3_6 = df_median_3_6[df_median_3_6['normalized_intensity_PE_Texas_Red_A_6'] > 0]\n",
        "df_median_3_7 = df_median_3_7[df_median_3_7['normalized_intensity_PE_Texas_Red_A_7'] > 0]\n",
        "df_median_3_8 = df_median_3_8[df_median_3_8['normalized_intensity_PE_Texas_Red_A_8'] > 0]\n",
        "df_median_3_9 = df_median_3_9[df_median_3_9['normalized_intensity_PE_Texas_Red_A_9'] > 0]\n",
        "df_median_3_10 = df_median_3_10[df_median_3_10['normalized_intensity_PE_Texas_Red_A_10'] > 0]\n",
        "df_median_3_11 = df_median_3_11[df_median_3_11['normalized_intensity_PE_Texas_Red_A_11'] > 0]\n",
        "\n",
        "df_median_4_1 = df_median_4_1[df_median_4_1['normalized_intensity_Alexa_Fluor_700_A_1'] > 0]\n",
        "df_median_4_2 = df_median_4_2[df_median_4_2['normalized_intensity_Alexa_Fluor_700_A_2'] > 0]\n",
        "df_median_4_3 = df_median_4_3[df_median_4_3['normalized_intensity_Alexa_Fluor_700_A_3'] > 0]\n",
        "df_median_4_4 = df_median_4_4[df_median_4_4['normalized_intensity_Alexa_Fluor_700_A_4'] > 0]\n",
        "df_median_4_5 = df_median_4_5[df_median_4_5['normalized_intensity_Alexa_Fluor_700_A_5'] > 0]\n",
        "df_median_4_6 = df_median_4_6[df_median_4_6['normalized_intensity_Alexa_Fluor_700_A_6'] > 0]\n",
        "df_median_4_7 = df_median_4_7[df_median_4_7['normalized_intensity_Alexa_Fluor_700_A_7'] > 0]\n",
        "df_median_4_8 = df_median_4_8[df_median_4_8['normalized_intensity_Alexa_Fluor_700_A_8'] > 0]\n",
        "df_median_4_9 = df_median_4_9[df_median_4_9['normalized_intensity_Alexa_Fluor_700_A_9'] > 0]\n",
        "df_median_4_10 = df_median_4_10[df_median_4_10['normalized_intensity_Alexa_Fluor_700_A_10'] > 0]\n",
        "df_median_4_11 = df_median_4_11[df_median_4_11['normalized_intensity_Alexa_Fluor_700_A_11'] > 0]\n",
        "\n",
        "df_median_5_1 = df_median_5_1[df_median_5_1['normalized_intensity_PerCP_A_1'] > 0]\n",
        "df_median_5_2 = df_median_5_2[df_median_5_2['normalized_intensity_PerCP_A_2'] > 0]\n",
        "df_median_5_3 = df_median_5_3[df_median_5_3['normalized_intensity_PerCP_A_3'] > 0]\n",
        "df_median_5_4 = df_median_5_4[df_median_5_4['normalized_intensity_PerCP_A_4'] > 0]\n",
        "df_median_5_5 = df_median_5_5[df_median_5_5['normalized_intensity_PerCP_A_5'] > 0]\n",
        "df_median_5_6 = df_median_5_6[df_median_5_6['normalized_intensity_PerCP_A_6'] > 0]\n",
        "df_median_5_7 = df_median_5_7[df_median_5_7['normalized_intensity_PerCP_A_7'] > 0]\n",
        "df_median_5_8 = df_median_5_8[df_median_5_8['normalized_intensity_PerCP_A_8'] > 0]\n",
        "df_median_5_9 = df_median_5_9[df_median_5_9['normalized_intensity_PerCP_A_9'] > 0]\n",
        "df_median_5_10 = df_median_5_10[df_median_5_10['normalized_intensity_PerCP_A_10'] > 0]\n",
        "df_median_5_11 = df_median_5_11[df_median_5_11['normalized_intensity_PerCP_A_11'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td1-HbgTcGNE"
      },
      "outputs": [],
      "source": [
        "# grouping and calculating the median per positive (>0) values per bin\n",
        "df_subset_1_1 = df_median_1_1.loc[:, ['population_number', 'normalized_intensity_BV421_A_1'] ]\n",
        "df_median_1_1_grouped = df_median_1_1.groupby('population_number').median()\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.rename(columns={\"normalized_intensity_BV421_A_1\": \"0.5_1_median_intensity\"})\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.reset_index()\n",
        "df_subset_1_2 = df_median_1_2.loc[:, ['population_number', 'normalized_intensity_BV421_A_2'] ]\n",
        "df_median_1_2_grouped = df_median_1_2.groupby('population_number').median()\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.rename(columns={\"normalized_intensity_BV421_A_2\": \"0.5_2_median_intensity\"})\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.reset_index()\n",
        "df_subset_1_3 = df_median_1_3.loc[:, ['population_number', 'normalized_intensity_BV421_A_3'] ]\n",
        "df_median_1_3_grouped = df_median_1_3.groupby('population_number').median()\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.rename(columns={\"normalized_intensity_BV421_A_3\": \"0.5_3_median_intensity\"})\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.reset_index()\n",
        "df_subset_1_4 = df_median_1_4.loc[:, ['population_number', 'normalized_intensity_BV421_A_4'] ]\n",
        "df_median_1_4_grouped = df_median_1_4.groupby('population_number').median()\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.rename(columns={\"normalized_intensity_BV421_A_4\": \"0.5_4_median_intensity\"})\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.reset_index()\n",
        "df_subset_1_5 = df_median_1_5.loc[:, ['population_number', 'normalized_intensity_BV421_A_5'] ]\n",
        "df_median_1_5_grouped = df_median_1_5.groupby('population_number').median()\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.rename(columns={\"normalized_intensity_BV421_A_5\": \"0.5_5_median_intensity\"})\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.reset_index()\n",
        "df_subset_1_6 = df_median_1_6.loc[:, ['population_number', 'normalized_intensity_BV421_A_6'] ]\n",
        "df_median_1_6_grouped = df_median_1_6.groupby('population_number').median()\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.rename(columns={\"normalized_intensity_BV421_A_6\": \"0.5_6_median_intensity\"})\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.reset_index()\n",
        "df_subset_1_7 = df_median_1_7.loc[:, ['population_number', 'normalized_intensity_BV421_A_7'] ]\n",
        "df_median_1_7_grouped = df_median_1_7.groupby('population_number').median()\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.rename(columns={\"normalized_intensity_BV421_A_7\": \"0.5_7_median_intensity\"})\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.reset_index()\n",
        "df_subset_1_8 = df_median_1_8.loc[:, ['population_number', 'normalized_intensity_BV421_A_8'] ]\n",
        "df_median_1_8_grouped = df_median_1_8.groupby('population_number').median()\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.rename(columns={\"normalized_intensity_BV421_A_8\": \"0.5_8_median_intensity\"})\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.reset_index()\n",
        "df_subset_1_9 = df_median_1_9.loc[:, ['population_number', 'normalized_intensity_BV421_A_9'] ]\n",
        "df_median_1_9_grouped = df_median_1_9.groupby('population_number').median()\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.rename(columns={\"normalized_intensity_BV421_A_9\": \"0.5_9_median_intensity\"})\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.reset_index()\n",
        "df_subset_1_10 = df_median_1_10.loc[:, ['population_number', 'normalized_intensity_BV421_A_10'] ]\n",
        "df_median_1_10_grouped = df_median_1_10.groupby('population_number').median()\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.rename(columns={\"normalized_intensity_BV421_A_10\": \"0.5_10_median_intensity\"})\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.reset_index()\n",
        "df_subset_1_11 = df_median_1_11.loc[:, ['population_number', 'normalized_intensity_BV421_A_11'] ]\n",
        "df_median_1_11_grouped = df_median_1_11.groupby('population_number').median()\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.rename(columns={\"normalized_intensity_BV421_A_11\": \"0.5_11_median_intensity\"})\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.reset_index()\n",
        "\n",
        "df_subset_2_1 = df_median_2_1.loc[:, ['population_number', 'normalized_intensity_FITC_A_1'] ]\n",
        "df_median_2_1_grouped = df_median_2_1.groupby('population_number').median()\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.rename(columns={\"normalized_intensity_FITC_A_1\": \"0.8_1_median_intensity\"})\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.reset_index()\n",
        "df_subset_2_2 = df_median_2_2.loc[:, ['population_number', 'normalized_intensity_FITC_A_2'] ]\n",
        "df_median_2_2_grouped = df_median_2_2.groupby('population_number').median()\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.rename(columns={\"normalized_intensity_FITC_A_2\": \"0.8_2_median_intensity\"})\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.reset_index()\n",
        "df_subset_2_3 = df_median_2_3.loc[:, ['population_number', 'normalized_intensity_FITC_A_3'] ]\n",
        "df_median_2_3_grouped = df_median_2_3.groupby('population_number').median()\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.rename(columns= {\"normalized_intensity_FITC_A_3\": \"0.8_3_median_intensity\"})\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.reset_index()\n",
        "df_subset_2_4 = df_median_2_4.loc[:, ['population_number', 'normalized_intensity_FITC_A_4'] ]\n",
        "df_median_2_4_grouped = df_median_2_4.groupby('population_number').median()\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.rename(columns={\"normalized_intensity_FITC_A_4\": \"0.8_4_median_intensity\"})\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.reset_index()\n",
        "df_subset_2_5 = df_median_2_5.loc[:, ['population_number', 'normalized_intensity_FITC_A_5'] ]\n",
        "df_median_2_5_grouped = df_median_2_5.groupby('population_number').median()\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.rename(columns={\"normalized_intensity_FITC_A_5\": \"0.8_5_median_intensity\"})\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.reset_index()\n",
        "df_subset_2_6 = df_median_2_6.loc[:, ['population_number', 'normalized_intensity_FITC_A_6'] ]\n",
        "df_median_2_6_grouped = df_median_2_6.groupby('population_number').median()\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.rename(columns={\"normalized_intensity_FITC_A_6\": \"0.8_6_median_intensity\"})\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.reset_index()\n",
        "df_subset_2_7 = df_median_2_7.loc[:, ['population_number', 'normalized_intensity_FITC_A_7'] ]\n",
        "df_median_2_7_grouped = df_median_2_7.groupby('population_number').median()\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.rename(columns={\"normalized_intensity_FITC_A_7\": \"0.8_7_median_intensity\"})\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.reset_index()\n",
        "df_subset_2_8 = df_median_2_8.loc[:, ['population_number', 'normalized_intensity_FITC_A_8'] ]\n",
        "df_median_2_8_grouped = df_median_2_8.groupby('population_number').median()\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.rename(columns={\"normalized_intensity_FITC_A_8\": \"0.8_8_median_intensity\"})\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.reset_index()\n",
        "df_subset_2_9 = df_median_2_9.loc[:, ['population_number', 'normalized_intensity_FITC_A_9'] ]\n",
        "df_median_2_9_grouped = df_median_2_9.groupby('population_number').median()\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.rename(columns={\"normalized_intensity_FITC_A_9\": \"0.8_9_median_intensity\"})\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.reset_index()\n",
        "df_subset_2_10 = df_median_2_10.loc[:, ['population_number', 'normalized_intensity_FITC_A_10'] ]\n",
        "df_median_2_10_grouped = df_median_2_10.groupby('population_number').median()\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.rename(columns={\"normalized_intensity_FITC_A_10\": \"0.8_10_median_intensity\"})\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.reset_index()\n",
        "df_subset_2_11 = df_median_2_11.loc[:, ['population_number', 'normalized_intensity_FITC_A_11'] ]\n",
        "df_median_2_11_grouped = df_median_2_11.groupby('population_number').median()\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.rename(columns={\"normalized_intensity_FITC_A_11\": \"0.8_11_median_intensity\"})\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.reset_index()\n",
        "\n",
        "df_subset_3_1 = df_median_3_1.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_1'] ]\n",
        "df_median_3_1_grouped = df_median_3_1.groupby('population_number').median()\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_1\": \"2.4_1_median_intensity\"})\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.reset_index()\n",
        "df_subset_3_2 = df_median_3_2.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_2'] ]\n",
        "df_median_3_2_grouped = df_median_3_2.groupby('population_number').median()\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_2\": \"2.4_2_median_intensity\"})\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.reset_index()\n",
        "df_subset_3_3 = df_median_3_3.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_3'] ]\n",
        "df_median_3_3_grouped = df_median_3_3.groupby('population_number').median()\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.rename(columns= {\"normalized_intensity_PE_Texas_Red_A_3\": \"2.4_3_median_intensity\"})\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.reset_index()\n",
        "df_subset_3_4 = df_median_3_4.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_4'] ]\n",
        "df_median_3_4_grouped = df_median_3_4.groupby('population_number').median()\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_4\": \"2.4_4_median_intensity\"})\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.reset_index()\n",
        "df_subset_3_5 = df_median_3_5.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_5'] ]\n",
        "df_median_3_5_grouped = df_median_3_5.groupby('population_number').median()\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_5\": \"2.4_5_median_intensity\"})\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.reset_index()\n",
        "df_subset_3_6 = df_median_3_6.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_6'] ]\n",
        "df_median_3_6_grouped = df_median_3_6.groupby('population_number').median()\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_6\": \"2.4_6_median_intensity\"})\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.reset_index()\n",
        "df_subset_3_7 = df_median_3_7.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_7'] ]\n",
        "df_median_3_7_grouped = df_median_3_7.groupby('population_number').median()\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_7\": \"2.4_7_median_intensity\"})\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.reset_index()\n",
        "df_subset_3_8 = df_median_3_8.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_8'] ]\n",
        "df_median_3_8_grouped = df_median_3_8.groupby('population_number').median()\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_8\": \"2.4_8_median_intensity\"})\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.reset_index()\n",
        "df_subset_3_9 = df_median_3_9.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_9'] ]\n",
        "df_median_3_9_grouped = df_median_3_9.groupby('population_number').median()\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_9\": \"2.4_9_median_intensity\"})\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.reset_index()\n",
        "df_subset_3_10 = df_median_3_10.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_10'] ]\n",
        "df_median_3_10_grouped = df_median_3_10.groupby('population_number').median()\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_10\": \"2.4_10_median_intensity\"})\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.reset_index()\n",
        "df_subset_3_11 = df_median_3_11.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_11'] ]\n",
        "df_median_3_11_grouped = df_median_3_11.groupby('population_number').median()\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_11\": \"2.4_11_median_intensity\"})\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.reset_index()\n",
        "\n",
        "df_subset_4_1 = df_median_4_1.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_1'] ]\n",
        "df_median_4_1_grouped = df_median_4_1.groupby('population_number').median()\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_1\": \"3.36_1_median_intensity\"})\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.reset_index()\n",
        "df_subset_4_2 = df_median_4_2.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_2'] ]\n",
        "df_median_4_2_grouped = df_median_4_2.groupby('population_number').median()\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_2\": \"3.36_2_median_intensity\"})\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.reset_index()\n",
        "df_subset_4_3 = df_median_4_3.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_3'] ]\n",
        "df_median_4_3_grouped = df_median_4_3.groupby('population_number').median()\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.rename(columns= {\"normalized_intensity_Alexa_Fluor_700_A_3\": \"3.36_3_median_intensity\"})\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.reset_index()\n",
        "df_subset_4_4 = df_median_4_4.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_4'] ]\n",
        "df_median_4_4_grouped = df_median_4_4.groupby('population_number').median()\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_4\": \"3.36_4_median_intensity\"})\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.reset_index()\n",
        "df_subset_4_5 = df_median_4_5.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_5'] ]\n",
        "df_median_4_5_grouped = df_median_4_5.groupby('population_number').median()\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_5\": \"3.36_5_median_intensity\"})\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.reset_index()\n",
        "df_subset_4_6 = df_median_4_6.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_6'] ]\n",
        "df_median_4_6_grouped = df_median_4_6.groupby('population_number').median()\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_6\": \"3.36_6_median_intensity\"})\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.reset_index()\n",
        "df_subset_4_7 = df_median_4_7.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_7'] ]\n",
        "df_median_4_7_grouped = df_median_4_7.groupby('population_number').median()\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_7\": \"3.36_7_median_intensity\"})\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.reset_index()\n",
        "df_subset_4_8 = df_median_4_8.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_8'] ]\n",
        "df_median_4_8_grouped = df_median_4_8.groupby('population_number').median()\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_8\": \"3.36_8_median_intensity\"})\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.reset_index()\n",
        "df_subset_4_9 = df_median_4_9.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_9'] ]\n",
        "df_median_4_9_grouped = df_median_4_9.groupby('population_number').median()\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_9\": \"3.36_9_median_intensity\"})\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.reset_index()\n",
        "df_subset_4_10 = df_median_4_10.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_10'] ]\n",
        "df_median_4_10_grouped = df_median_4_10.groupby('population_number').median()\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_10\": \"3.36_10_median_intensity\"})\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.reset_index()\n",
        "df_subset_4_11 = df_median_4_11.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_11'] ]\n",
        "df_median_4_11_grouped = df_median_4_11.groupby('population_number').median()\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_11\": \"3.36_11_median_intensity\"})\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.reset_index()\n",
        "\n",
        "df_subset_5_1 = df_median_5_1.loc[:, ['population_number', 'normalized_intensity_PerCP_A_1']]\n",
        "df_median_5_1_grouped = df_median_5_1.groupby('population_number').median()\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.rename(columns={\"normalized_intensity_PerCP_A_1\": \"0.04_1_median_intensity\"})\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.reset_index()\n",
        "df_subset_5_2 = df_median_5_2.loc[:, ['population_number', 'normalized_intensity_PerCP_A_2'] ]\n",
        "df_median_5_2_grouped = df_median_5_2.groupby('population_number').median()\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.rename(columns={\"normalized_intensity_PerCP_A_2\": \"0.04_2_median_intensity\"})\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.reset_index()\n",
        "df_subset_5_3 = df_median_5_3.loc[:, ['population_number', 'normalized_intensity_PerCP_A_3'] ]\n",
        "df_median_5_3_grouped = df_median_5_3.groupby('population_number').median()\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.rename(columns= {\"normalized_intensity_PerCP_A_3\": \"0.04_3_median_intensity\"})\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.reset_index()\n",
        "df_subset_5_4 = df_median_5_4.loc[:, ['population_number', 'normalized_intensity_PerCP_A_4'] ]\n",
        "df_median_5_4_grouped = df_median_5_4.groupby('population_number').median()\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.rename(columns={\"normalized_intensity_PerCP_A_4\": \"0.04_4_median_intensity\"})\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.reset_index()\n",
        "df_subset_5_5 = df_median_5_5.loc[:, ['population_number', 'normalized_intensity_PerCP_A_5'] ]\n",
        "df_median_5_5_grouped = df_median_5_5.groupby('population_number').median()\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.rename(columns={\"normalized_intensity_PerCP_A_5\": \"0.04_5_median_intensity\"})\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.reset_index()\n",
        "df_subset_5_6 = df_median_5_6.loc[:, ['population_number', 'normalized_intensity_PerCP_A_6'] ]\n",
        "df_median_5_6_grouped = df_median_5_6.groupby('population_number').median()\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.rename(columns={\"normalized_intensity_PerCP_A_6\": \"0.04_6_median_intensity\"})\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.reset_index()\n",
        "df_subset_5_7 = df_median_5_7.loc[:, ['population_number', 'normalized_intensity_PerCP_A_7'] ]\n",
        "df_median_5_7_grouped = df_median_5_7.groupby('population_number').median()\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.rename(columns={\"normalized_intensity_PerCP_A_7\": \"0.04_7_median_intensity\"})\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.reset_index()\n",
        "df_subset_5_8 = df_median_5_8.loc[:, ['population_number', 'normalized_intensity_PerCP_A_8'] ]\n",
        "df_median_5_8_grouped = df_median_5_8.groupby('population_number').median()\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.rename(columns={\"normalized_intensity_PerCP_A_8\": \"0.04_8_median_intensity\"})\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.reset_index()\n",
        "df_subset_5_9 = df_median_5_9.loc[:, ['population_number', 'normalized_intensity_PerCP_A_9'] ]\n",
        "df_median_5_9_grouped = df_median_5_9.groupby('population_number').median()\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.rename(columns={\"normalized_intensity_PerCP_A_9\": \"0.04_9_median_intensity\"})\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.reset_index()\n",
        "df_subset_5_10 = df_median_5_10.loc[:, ['population_number', 'normalized_intensity_PerCP_A_10'] ]\n",
        "df_median_5_10_grouped = df_median_5_10.groupby('population_number').median()\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.rename(columns={\"normalized_intensity_PerCP_A_10\": \"0.04_10_median_intensity\"})\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.reset_index()\n",
        "df_subset_5_11 = df_median_5_11.loc[:, ['population_number', 'normalized_intensity_PerCP_A_11'] ]\n",
        "df_median_5_11_grouped = df_median_5_11.groupby('population_number').median()\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.rename(columns={\"normalized_intensity_PerCP_A_11\": \"0.04_11_median_intensity\"})\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQD7-fnjcGNF"
      },
      "outputs": [],
      "source": [
        "# returning the zeros that were filtered out to the dfs\n",
        "pop_1_test_log_grouped[['0.5_1_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_1_grouped.population_number, df_median_1_1_grouped['0.5_1_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_2_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_2_grouped.population_number, df_median_1_2_grouped['0.5_2_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_3_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_3_grouped.population_number, df_median_1_3_grouped['0.5_3_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_4_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_4_grouped.population_number, df_median_1_4_grouped['0.5_4_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_5_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_5_grouped.population_number, df_median_1_5_grouped['0.5_5_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_6_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_6_grouped.population_number, df_median_1_6_grouped['0.5_6_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_7_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_7_grouped.population_number, df_median_1_7_grouped['0.5_7_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_8_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_8_grouped.population_number, df_median_1_8_grouped['0.5_8_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_9_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_9_grouped.population_number, df_median_1_9_grouped['0.5_9_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_10_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_10_grouped.population_number, df_median_1_10_grouped['0.5_10_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.5_11_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_11_grouped.population_number, df_median_1_11_grouped['0.5_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_test_log_grouped[['0.8_1_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_1_grouped.population_number, df_median_2_1_grouped['0.8_1_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_2_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_2_grouped.population_number, df_median_2_2_grouped['0.8_2_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_3_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_3_grouped.population_number, df_median_2_3_grouped['0.8_3_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_4_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_4_grouped.population_number, df_median_2_4_grouped['0.8_4_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_5_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_5_grouped.population_number, df_median_2_5_grouped['0.8_5_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_6_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_6_grouped.population_number, df_median_2_6_grouped['0.8_6_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_7_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_7_grouped.population_number, df_median_2_7_grouped['0.8_7_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_8_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_8_grouped.population_number, df_median_2_8_grouped['0.8_8_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_9_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_9_grouped.population_number, df_median_2_9_grouped['0.8_9_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_10_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_10_grouped.population_number, df_median_2_10_grouped['0.8_10_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.8_11_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_11_grouped.population_number, df_median_2_11_grouped['0.8_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_test_log_grouped[['2.4_1_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_1_grouped.population_number, df_median_3_1_grouped['2.4_1_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_2_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_2_grouped.population_number, df_median_3_2_grouped['2.4_2_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_3_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_3_grouped.population_number, df_median_3_3_grouped['2.4_3_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_4_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_4_grouped.population_number, df_median_3_4_grouped['2.4_4_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_5_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_5_grouped.population_number, df_median_3_5_grouped['2.4_5_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_6_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_6_grouped.population_number, df_median_3_6_grouped['2.4_6_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_7_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_7_grouped.population_number, df_median_3_7_grouped['2.4_7_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_8_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_8_grouped.population_number, df_median_3_8_grouped['2.4_8_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_9_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_9_grouped.population_number, df_median_3_9_grouped['2.4_9_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_10_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_10_grouped.population_number, df_median_3_10_grouped['2.4_10_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['2.4_11_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_11_grouped.population_number, df_median_3_11_grouped['2.4_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_test_log_grouped[['3.36_1_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_1_grouped.population_number, df_median_4_1_grouped['3.36_1_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_2_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_2_grouped.population_number, df_median_4_2_grouped['3.36_2_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_3_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_3_grouped.population_number, df_median_4_3_grouped['3.36_3_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_4_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_4_grouped.population_number, df_median_4_4_grouped['3.36_4_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_5_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_5_grouped.population_number, df_median_4_5_grouped['3.36_5_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_6_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_6_grouped.population_number, df_median_4_6_grouped['3.36_6_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_7_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_7_grouped.population_number, df_median_4_7_grouped['3.36_7_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_8_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_8_grouped.population_number, df_median_4_8_grouped['3.36_8_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_9_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_9_grouped.population_number, df_median_4_9_grouped['3.36_9_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_10_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_10_grouped.population_number, df_median_4_10_grouped['3.36_10_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['3.36_11_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_11_grouped.population_number, df_median_4_11_grouped['3.36_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_1_test_log_grouped[['0.04_1_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_1_grouped.population_number, df_median_5_1_grouped['0.04_1_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_2_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_2_grouped.population_number, df_median_5_2_grouped['0.04_2_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_3_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_3_grouped.population_number, df_median_5_3_grouped['0.04_3_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_4_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_4_grouped.population_number, df_median_5_4_grouped['0.04_4_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_5_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_5_grouped.population_number, df_median_5_5_grouped['0.04_5_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_6_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_6_grouped.population_number, df_median_5_6_grouped['0.04_6_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_7_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_7_grouped.population_number, df_median_5_7_grouped['0.04_7_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_8_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_8_grouped.population_number, df_median_5_8_grouped['0.04_8_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_9_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_9_grouped.population_number, df_median_5_9_grouped['0.04_9_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_10_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_10_grouped.population_number, df_median_5_10_grouped['0.04_10_median_intensity']), axis = 1)\n",
        "pop_1_test_log_grouped[['0.04_11_median_intensity']] = pop_1_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_11_grouped.population_number, df_median_5_11_grouped['0.04_11_median_intensity']), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OngEQ1BycGNG"
      },
      "outputs": [],
      "source": [
        "pop_1_test_log_grouped.to_csv('H460_test_log_grouped_filtered_final.csv') # name the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPZzORfB61BO"
      },
      "source": [
        "Pop 2 - train data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wub4F5JM61BO"
      },
      "outputs": [],
      "source": [
        "# apply bins function on the different particle features\n",
        "Total_pop_2_log[['BV421_A_1', 'BV421_A_2', 'BV421_A_3', 'BV421_A_4', 'BV421_A_5', 'BV421_A_6', 'BV421_A_7', 'BV421_A_8', 'BV421_A_9', 'BV421_A_10', 'BV421_A_11']] = Total_pop_2_log['BV421_A'].apply(bins)\n",
        "Total_pop_2_log[['FITC_A_1', 'FITC_A_2', 'FITC_A_3', 'FITC_A_4', 'FITC_A_5', 'FITC_A_6', 'FITC_A_7', 'FITC_A_8', 'FITC_A_9', 'FITC_A_10', 'FITC_A_11']] = Total_pop_2_log['FITC_A'].apply(bins)\n",
        "Total_pop_2_log[['PE_Texas_Red_A_1', 'PE_Texas_Red_A_2', 'PE_Texas_Red_A_3', 'PE_Texas_Red_A_4', 'PE_Texas_Red_A_5', 'PE_Texas_Red_A_6', 'PE_Texas_Red_A_7', 'PE_Texas_Red_A_8', 'PE_Texas_Red_A_9', 'PE_Texas_Red_A_10', 'PE_Texas_Red_A_11']] = Total_pop_2_log['PE_Texas_Red_A'].apply(bins)\n",
        "Total_pop_2_log[['Alexa_Fluor_700_A_1', 'Alexa_Fluor_700_A_2', 'Alexa_Fluor_700_A_3', 'Alexa_Fluor_700_A_4', 'Alexa_Fluor_700_A_5', 'Alexa_Fluor_700_A_6', 'Alexa_Fluor_700_A_7', 'Alexa_Fluor_700_A_8', 'Alexa_Fluor_700_A_9', 'Alexa_Fluor_700_A_10', 'Alexa_Fluor_700_A_11']] = Total_pop_2_log['Alexa_Fluor_700_A'].apply(bins)\n",
        "Total_pop_2_log[['PerCP_A_1', 'PerCP_A_2', 'PerCP_A_3', 'PerCP_A_4', 'PerCP_A_5', 'PerCP_A_6', 'PerCP_A_7', 'PerCP_A_8', 'PerCP_A_9', 'PerCP_A_10', 'PerCP_A_11']] = Total_pop_2_log['PerCP_A'].apply(bins)\n",
        "Total_pop_2_log.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5OjdwWP61BP"
      },
      "outputs": [],
      "source": [
        "# applying bins intensity function on the particle features\n",
        "Total_pop_2_log[['0.5_1_intensity', '0.5_2_intensity', '0.5_3_intensity', '0.5_4_intensity', '0.5_5_intensity', '0.5_6_intensity', '0.5_7_intensity', '0.5_8_intensity', '0.5_9_intensity', '0.5_10_intensity', '0.5_11_intensity']] = Total_pop_2_log.apply(lambda x: bins_intensity(x.BV421_A, x.BV421_A_1, x.BV421_A_2, x.BV421_A_3, x.BV421_A_4, x.BV421_A_5, x.BV421_A_6, x.BV421_A_7, x.BV421_A_8, x.BV421_A_9, x.BV421_A_10, x.BV421_A_11), axis=1)\n",
        "Total_pop_2_log[['0.8_1_intensity', '0.8_2_intensity', '0.8_3_intensity', '0.8_4_intensity', '0.8_5_intensity', '0.8_6_intensity', '0.8_7_intensity', '0.8_8_intensity', '0.8_9_intensity', '0.8_10_intensity', '0.8_11_intensity']] = Total_pop_2_log.apply(lambda x: bins_intensity(x.FITC_A, x.FITC_A_1, x.FITC_A_2, x.FITC_A_3, x.FITC_A_4, x.FITC_A_5, x.FITC_A_6, x.FITC_A_7, x.FITC_A_8, x.FITC_A_9, x.FITC_A_10, x.FITC_A_11), axis=1)\n",
        "Total_pop_2_log[['2.4_1_intensity', '2.4_2_intensity', '2.4_3_intensity', '2.4_4_intensity', '2.4_5_intensity', '2.4_6_intensity', '2.4_7_intensity', '2.4_8_intensity', '2.4_9_intensity', '2.4_10_intensity', '2.4_11_intensity']] = Total_pop_2_log.apply(lambda x: bins_intensity(x.PE_Texas_Red_A, x.PE_Texas_Red_A_1, x.PE_Texas_Red_A_2, x.PE_Texas_Red_A_3, x.PE_Texas_Red_A_4, x.PE_Texas_Red_A_5, x.PE_Texas_Red_A_6, x.PE_Texas_Red_A_7, x.PE_Texas_Red_A_8, x.PE_Texas_Red_A_9, x.PE_Texas_Red_A_10, x.PE_Texas_Red_A_11), axis=1)\n",
        "Total_pop_2_log[['3.36_1_intensity', '3.36_2_intensity', '3.36_3_intensity', '3.36_4_intensity', '3.36_5_intensity', '3.36_6_intensity', '3.36_7_intensity', '3.36_8_intensity', '3.36_9_intensity', '3.36_10_intensity', '3.36_11_intensity']] = Total_pop_2_log.apply(lambda x: bins_intensity(x.Alexa_Fluor_700_A, x.Alexa_Fluor_700_A_1, x.Alexa_Fluor_700_A_2, x.Alexa_Fluor_700_A_3, x.Alexa_Fluor_700_A_4, x.Alexa_Fluor_700_A_5, x.Alexa_Fluor_700_A_6, x.Alexa_Fluor_700_A_7, x.Alexa_Fluor_700_A_8, x.Alexa_Fluor_700_A_9, x.Alexa_Fluor_700_A_10, x.Alexa_Fluor_700_A_11), axis=1)\n",
        "Total_pop_2_log[['0.04_1_intensity', '0.04_2_intensity', '0.04_3_intensity', '0.04_4_intensity', '0.04_5_intensity', '0.04_6_intensity', '0.04_7_intensity', '0.04_8_intensity', '0.04_9_intensity', '0.04_10_intensity', '0.04_11_intensity']] = Total_pop_2_log.apply(lambda x: bins_intensity(x.PerCP_A, x.PerCP_A_1, x.PerCP_A_2, x.PerCP_A_3, x.PerCP_A_4, x.PerCP_A_5, x.PerCP_A_6, x.PerCP_A_7, x.PerCP_A_8, x.PerCP_A_9, x.PerCP_A_10, x.PerCP_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIUaHeCb61BP"
      },
      "outputs": [],
      "source": [
        "# grouping all the relevant data per population and building new df's that include only the relevant data for analysis\n",
        "grouper = Total_pop_2_log.groupby(pd.Grouper(key='population_number'))\n",
        "BV421_A_1_intensity_sum = grouper['0.5_1_intensity'].sum().to_frame(name='intensity_sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_intensity_sum = grouper['0.5_2_intensity'].sum().to_frame(name='intensity_sum_BV421_A_2').reset_index()\n",
        "BV421_A_3_intensity_sum = grouper['0.5_3_intensity'].sum().to_frame(name='intensity_sum_BV421_A_3').reset_index()\n",
        "BV421_A_4_intensity_sum = grouper['0.5_4_intensity'].sum().to_frame(name='intensity_sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_intensity_sum = grouper['0.5_5_intensity'].sum().to_frame(name='intensity_sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_intensity_sum = grouper['0.5_6_intensity'].sum().to_frame(name='intensity_sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_intensity_sum = grouper['0.5_7_intensity'].sum().to_frame(name='intensity_sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_intensity_sum = grouper['0.5_8_intensity'].sum().to_frame(name='intensity_sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_intensity_sum = grouper['0.5_9_intensity'].sum().to_frame(name='intensity_sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_intensity_sum = grouper['0.5_10_intensity'].sum().to_frame(name='intensity_sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_intensity_sum = grouper['0.5_11_intensity'].sum().to_frame(name='intensity_sum_BV421_A_11').reset_index()\n",
        "BV421_A_1_sum = grouper['BV421_A_1'].sum().to_frame(name='sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_sum = grouper['BV421_A_2'].sum().to_frame(name='sum_BV421_A_2').reset_index()\n",
        "BV421_A_3_sum = grouper['BV421_A_3'].sum().to_frame(name='sum_BV421_A_3').reset_index()\n",
        "BV421_A_4_sum = grouper['BV421_A_4'].sum().to_frame(name='sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_sum = grouper['BV421_A_5'].sum().to_frame(name='sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_sum = grouper['BV421_A_6'].sum().to_frame(name='sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_sum = grouper['BV421_A_7'].sum().to_frame(name='sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_sum = grouper['BV421_A_8'].sum().to_frame(name='sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_sum = grouper['BV421_A_9'].sum().to_frame(name='sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_sum = grouper['BV421_A_10'].sum().to_frame(name='sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_sum = grouper['BV421_A_11'].sum().to_frame(name='sum_BV421_A_11').reset_index()\n",
        "\n",
        "FITC_A_1_intensity_sum = grouper['0.8_1_intensity'].sum().to_frame(name='intensity_sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_intensity_sum = grouper['0.8_2_intensity'].sum().to_frame(name='intensity_sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_intensity_sum = grouper['0.8_3_intensity'].sum().to_frame(name='intensity_sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_intensity_sum = grouper['0.8_4_intensity'].sum().to_frame(name='intensity_sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_intensity_sum = grouper['0.8_5_intensity'].sum().to_frame(name='intensity_sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_intensity_sum = grouper['0.8_6_intensity'].sum().to_frame(name='intensity_sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_intensity_sum = grouper['0.8_7_intensity'].sum().to_frame(name='intensity_sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_intensity_sum = grouper['0.8_8_intensity'].sum().to_frame(name='intensity_sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_intensity_sum = grouper['0.8_9_intensity'].sum().to_frame(name='intensity_sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_intensity_sum = grouper['0.8_10_intensity'].sum().to_frame(name='intensity_sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_intensity_sum = grouper['0.8_11_intensity'].sum().to_frame(name='intensity_sum_FITC_A_11').reset_index()\n",
        "FITC_A_1_sum = grouper['FITC_A_1'].sum().to_frame(name='sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_sum = grouper['FITC_A_2'].sum().to_frame(name='sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_sum = grouper['FITC_A_3'].sum().to_frame(name='sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_sum = grouper['FITC_A_4'].sum().to_frame(name='sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_sum = grouper['FITC_A_5'].sum().to_frame(name='sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_sum = grouper['FITC_A_6'].sum().to_frame(name='sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_sum = grouper['FITC_A_7'].sum().to_frame(name='sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_sum = grouper['FITC_A_8'].sum().to_frame(name='sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_sum = grouper['FITC_A_9'].sum().to_frame(name='sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_sum = grouper['FITC_A_10'].sum().to_frame(name='sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_sum = grouper['FITC_A_11'].sum().to_frame(name='sum_FITC_A_11').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_1_intensity_sum = grouper['2.4_1_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_intensity_sum = grouper['2.4_2_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_intensity_sum = grouper['2.4_3_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_intensity_sum = grouper['2.4_4_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_intensity_sum = grouper['2.4_5_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_intensity_sum = grouper['2.4_6_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_intensity_sum = grouper['2.4_7_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_intensity_sum = grouper['2.4_8_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_intensity_sum = grouper['2.4_9_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_intensity_sum = grouper['2.4_10_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_intensity_sum = grouper['2.4_11_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_11').reset_index()\n",
        "PE_Texas_Red_A_1_sum = grouper['PE_Texas_Red_A_1'].sum().to_frame(name='sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_sum = grouper['PE_Texas_Red_A_2'].sum().to_frame(name='sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_sum = grouper['PE_Texas_Red_A_3'].sum().to_frame(name='sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_sum = grouper['PE_Texas_Red_A_4'].sum().to_frame(name='sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_sum = grouper['PE_Texas_Red_A_5'].sum().to_frame(name='sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_sum = grouper['PE_Texas_Red_A_6'].sum().to_frame(name='sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_sum = grouper['PE_Texas_Red_A_7'].sum().to_frame(name='sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_sum = grouper['PE_Texas_Red_A_8'].sum().to_frame(name='sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_sum = grouper['PE_Texas_Red_A_9'].sum().to_frame(name='sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_sum = grouper['PE_Texas_Red_A_10'].sum().to_frame(name='sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_sum = grouper['PE_Texas_Red_A_11'].sum().to_frame(name='sum_PE_Texas_Red_A_11').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_1_intensity_sum = grouper['3.36_1_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_intensity_sum = grouper['3.36_2_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_intensity_sum = grouper['3.36_3_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_intensity_sum = grouper['3.36_4_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_intensity_sum = grouper['3.36_5_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_intensity_sum = grouper['3.36_6_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_intensity_sum = grouper['3.36_7_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_intensity_sum = grouper['3.36_8_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_intensity_sum = grouper['3.36_9_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_intensity_sum = grouper['3.36_10_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_intensity_sum = grouper['3.36_11_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "Alexa_Fluor_700_A_1_sum = grouper['Alexa_Fluor_700_A_1'].sum().to_frame(name='sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_sum = grouper['Alexa_Fluor_700_A_2'].sum().to_frame(name='sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_sum = grouper['Alexa_Fluor_700_A_3'].sum().to_frame(name='sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_sum = grouper['Alexa_Fluor_700_A_4'].sum().to_frame(name='sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_sum = grouper['Alexa_Fluor_700_A_5'].sum().to_frame(name='sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_sum = grouper['Alexa_Fluor_700_A_6'].sum().to_frame(name='sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_sum = grouper['Alexa_Fluor_700_A_7'].sum().to_frame(name='sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_sum = grouper['Alexa_Fluor_700_A_8'].sum().to_frame(name='sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_sum = grouper['Alexa_Fluor_700_A_9'].sum().to_frame(name='sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_sum = grouper['Alexa_Fluor_700_A_10'].sum().to_frame(name='sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_sum = grouper['Alexa_Fluor_700_A_11'].sum().to_frame(name='sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "\n",
        "PerCP_A_1_intensity_sum = grouper['0.04_1_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_intensity_sum = grouper['0.04_2_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_intensity_sum = grouper['0.04_3_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_intensity_sum = grouper['0.04_4_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_intensity_sum = grouper['0.04_5_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_intensity_sum = grouper['0.04_6_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_intensity_sum = grouper['0.04_7_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_intensity_sum = grouper['0.04_8_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_intensity_sum = grouper['0.04_9_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_intensity_sum = grouper['0.04_10_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_intensity_sum = grouper['0.04_11_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_11').reset_index()\n",
        "PerCP_A_1_sum = grouper['PerCP_A_1'].sum().to_frame(name='sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_sum = grouper['PerCP_A_2'].sum().to_frame(name='sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_sum = grouper['PerCP_A_3'].sum().to_frame(name='sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_sum = grouper['PerCP_A_4'].sum().to_frame(name='sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_sum = grouper['PerCP_A_5'].sum().to_frame(name='sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_sum = grouper['PerCP_A_6'].sum().to_frame(name='sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_sum = grouper['PerCP_A_7'].sum().to_frame(name='sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_sum = grouper['PerCP_A_8'].sum().to_frame(name='sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_sum = grouper['PerCP_A_9'].sum().to_frame(name='sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_sum = grouper['PerCP_A_10'].sum().to_frame(name='sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_sum = grouper['PerCP_A_11'].sum().to_frame(name='sum_PerCP_A_11').reset_index()\n",
        "Cell_line = grouper['Cell_line'].mean().to_frame(name='Cell_line').reset_index()\n",
        "group = grouper['group'].mean().to_frame(name='group').reset_index()\n",
        "\n",
        "mean_FSC_A = grouper['FSC_A'].mean().to_frame(name='FSC_A_mean').reset_index()\n",
        "mean_FSC_H = grouper['FSC_H'].mean().to_frame(name='FSC_H_mean').reset_index()\n",
        "mean_FSC_W = grouper['FSC_W'].mean().to_frame(name='FSC_W_mean').reset_index()\n",
        "mean_SSC_A = grouper['SSC_A'].mean().to_frame(name='SSC_A_mean').reset_index()\n",
        "mean_SSC_H = grouper['SSC_H'].mean().to_frame(name='SSC_H_mean').reset_index()\n",
        "mean_SSC_W = grouper['SSC_W'].mean().to_frame(name='SSC_W_mean').reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDi2nJiv61BP"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_2_log_grouper = [BV421_A_1_intensity_sum, BV421_A_2_intensity_sum, BV421_A_3_intensity_sum, BV421_A_4_intensity_sum, BV421_A_5_intensity_sum,\n",
        "         BV421_A_6_intensity_sum, BV421_A_7_intensity_sum, BV421_A_8_intensity_sum, BV421_A_9_intensity_sum, BV421_A_10_intensity_sum, BV421_A_11_intensity_sum,\n",
        "         BV421_A_1_sum, BV421_A_2_sum, BV421_A_3_sum, BV421_A_4_sum, BV421_A_5_sum, BV421_A_6_sum, BV421_A_7_sum, BV421_A_8_sum, BV421_A_9_sum, BV421_A_10_sum, BV421_A_11_sum,\n",
        "         FITC_A_1_intensity_sum, FITC_A_2_intensity_sum, FITC_A_3_intensity_sum, FITC_A_4_intensity_sum, FITC_A_5_intensity_sum, FITC_A_6_intensity_sum, FITC_A_7_intensity_sum,\n",
        "         FITC_A_8_intensity_sum, FITC_A_9_intensity_sum, FITC_A_10_intensity_sum, FITC_A_11_intensity_sum, FITC_A_1_sum, FITC_A_2_sum, FITC_A_3_sum, FITC_A_4_sum, FITC_A_5_sum,\n",
        "         FITC_A_6_sum, FITC_A_7_sum, FITC_A_8_sum, FITC_A_9_sum, FITC_A_10_sum, FITC_A_11_sum,PE_Texas_Red_A_1_intensity_sum, PE_Texas_Red_A_2_intensity_sum, PE_Texas_Red_A_3_intensity_sum,\n",
        "         PE_Texas_Red_A_4_intensity_sum, PE_Texas_Red_A_5_intensity_sum, PE_Texas_Red_A_6_intensity_sum, PE_Texas_Red_A_7_intensity_sum, PE_Texas_Red_A_8_intensity_sum, PE_Texas_Red_A_9_intensity_sum,\n",
        "         PE_Texas_Red_A_10_intensity_sum, PE_Texas_Red_A_11_intensity_sum, PE_Texas_Red_A_1_sum, PE_Texas_Red_A_2_sum, PE_Texas_Red_A_3_sum, PE_Texas_Red_A_4_sum, PE_Texas_Red_A_5_sum,\n",
        "         PE_Texas_Red_A_6_sum, PE_Texas_Red_A_7_sum, PE_Texas_Red_A_8_sum, PE_Texas_Red_A_9_sum, PE_Texas_Red_A_10_sum, PE_Texas_Red_A_11_sum,\n",
        "         Alexa_Fluor_700_A_1_intensity_sum, Alexa_Fluor_700_A_2_intensity_sum, Alexa_Fluor_700_A_3_intensity_sum, Alexa_Fluor_700_A_4_intensity_sum, Alexa_Fluor_700_A_5_intensity_sum,\n",
        "         Alexa_Fluor_700_A_6_intensity_sum, Alexa_Fluor_700_A_7_intensity_sum, Alexa_Fluor_700_A_8_intensity_sum, Alexa_Fluor_700_A_9_intensity_sum, Alexa_Fluor_700_A_10_intensity_sum, Alexa_Fluor_700_A_11_intensity_sum,\n",
        "         Alexa_Fluor_700_A_1_sum, Alexa_Fluor_700_A_2_sum, Alexa_Fluor_700_A_3_sum, Alexa_Fluor_700_A_4_sum, Alexa_Fluor_700_A_5_sum, Alexa_Fluor_700_A_6_sum, Alexa_Fluor_700_A_7_sum, Alexa_Fluor_700_A_8_sum,\n",
        "         Alexa_Fluor_700_A_9_sum, Alexa_Fluor_700_A_10_sum, Alexa_Fluor_700_A_11_sum, PerCP_A_1_intensity_sum, PerCP_A_2_intensity_sum, PerCP_A_3_intensity_sum, PerCP_A_4_intensity_sum, PerCP_A_5_intensity_sum,\n",
        "         PerCP_A_6_intensity_sum, PerCP_A_7_intensity_sum, PerCP_A_8_intensity_sum, PerCP_A_9_intensity_sum, PerCP_A_10_intensity_sum, PerCP_A_11_intensity_sum,\n",
        "         PerCP_A_1_sum, PerCP_A_2_sum, PerCP_A_3_sum, PerCP_A_4_sum, PerCP_A_5_sum, PerCP_A_6_sum, PerCP_A_7_sum, PerCP_A_8_sum, PerCP_A_9_sum, PerCP_A_10_sum, PerCP_A_11_sum,\n",
        "         mean_FSC_A, mean_FSC_H, mean_FSC_W, mean_SSC_A, mean_SSC_H, mean_SSC_W, group, Cell_line]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlt5oIF-61BP"
      },
      "outputs": [],
      "source": [
        "# building a new dfs of the grouped data\n",
        "pop_2_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_2_log_grouper)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalizing the uptake intensities per bin"
      ],
      "metadata": {
        "id": "17fd5axbfa5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkq-Ypl9HdB8"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['normalized_intensity_BV421_A_1', 'normalized_intensity_BV421_A_2', 'normalized_intensity_BV421_A_3', 'normalized_intensity_BV421_A_4'\n",
        "                  , 'normalized_intensity_BV421_A_5', 'normalized_intensity_BV421_A_6', 'normalized_intensity_BV421_A_7', 'normalized_intensity_BV421_A_8'\n",
        "                  , 'normalized_intensity_BV421_A_9', 'normalized_intensity_BV421_A_10', 'normalized_intensity_BV421_A_11']] = pop_2_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_BV421_A_1, x.intensity_sum_BV421_A_2, x.intensity_sum_BV421_A_3, x.intensity_sum_BV421_A_4, x.intensity_sum_BV421_A_5,\n",
        "                  x.intensity_sum_BV421_A_6, x.intensity_sum_BV421_A_7, x.intensity_sum_BV421_A_8, x.intensity_sum_BV421_A_9, x.intensity_sum_BV421_A_10, x.intensity_sum_BV421_A_11,\n",
        "                  x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_ey8WYOHdun"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['normalized_intensity_FITC_A_1', 'normalized_intensity_FITC_A_2', 'normalized_intensity_FITC_A_3', 'normalized_intensity_FITC_A_4',\n",
        "                          'normalized_intensity_FITC_A_5', 'normalized_intensity_FITC_A_6', 'normalized_intensity_FITC_A_7', 'normalized_intensity_FITC_A_8',\n",
        "                          'normalized_intensity_FITC_A_9', 'normalized_intensity_FITC_A_10', 'normalized_intensity_FITC_A_11']] = pop_2_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_FITC_A_1, x.intensity_sum_FITC_A_2, x.intensity_sum_FITC_A_3, x.intensity_sum_FITC_A_4, x.intensity_sum_FITC_A_5,\n",
        "                          x.intensity_sum_FITC_A_6, x.intensity_sum_FITC_A_7, x.intensity_sum_FITC_A_8, x.intensity_sum_FITC_A_9, x.intensity_sum_FITC_A_10, x.intensity_sum_FITC_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7j3xXoaHeJs"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'normalized_intensity_PE_Texas_Red_A_2', 'normalized_intensity_PE_Texas_Red_A_3', 'normalized_intensity_PE_Texas_Red_A_4',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_5', 'normalized_intensity_PE_Texas_Red_A_6', 'normalized_intensity_PE_Texas_Red_A_7', 'normalized_intensity_PE_Texas_Red_A_8',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_9','normalized_intensity_PE_Texas_Red_A_10','normalized_intensity_PE_Texas_Red_A_11']] = pop_2_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PE_Texas_Red_A_1, x.intensity_sum_PE_Texas_Red_A_2, x.intensity_sum_PE_Texas_Red_A_3, x.intensity_sum_PE_Texas_Red_A_4, x.intensity_sum_PE_Texas_Red_A_5,\n",
        "                          x.intensity_sum_PE_Texas_Red_A_6, x.intensity_sum_PE_Texas_Red_A_7, x.intensity_sum_PE_Texas_Red_A_8, x.intensity_sum_PE_Texas_Red_A_9, x.intensity_sum_PE_Texas_Red_A_10, x.intensity_sum_PE_Texas_Red_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5IZuducHerE"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'normalized_intensity_Alexa_Fluor_700_A_2', 'normalized_intensity_Alexa_Fluor_700_A_3', 'normalized_intensity_Alexa_Fluor_700_A_4',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_5', 'normalized_intensity_Alexa_Fluor_700_A_6', 'normalized_intensity_Alexa_Fluor_700_A_7', 'normalized_intensity_Alexa_Fluor_700_A_8',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_9', 'normalized_intensity_Alexa_Fluor_700_A_10', 'normalized_intensity_Alexa_Fluor_700_A_11']] = pop_2_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_Alexa_Fluor_700_A_1, x.intensity_sum_Alexa_Fluor_700_A_2, x.intensity_sum_Alexa_Fluor_700_A_3, x.intensity_sum_Alexa_Fluor_700_A_4, x.intensity_sum_Alexa_Fluor_700_A_5,\n",
        "                          x.intensity_sum_Alexa_Fluor_700_A_6, x.intensity_sum_Alexa_Fluor_700_A_7, x.intensity_sum_Alexa_Fluor_700_A_8, x.intensity_sum_Alexa_Fluor_700_A_9, x.intensity_sum_Alexa_Fluor_700_A_10, x.intensity_sum_Alexa_Fluor_700_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRu_3yGqHfRd"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['normalized_intensity_PerCP_A_1', 'normalized_intensity_PerCP_A_2', 'normalized_intensity_PerCP_A_3', 'normalized_intensity_PerCP_A_4',\n",
        "                          'normalized_intensity_PerCP_A_5', 'normalized_intensity_PerCP_A_6', 'normalized_intensity_PerCP_A_7', 'normalized_intensity_PerCP_A_8',\n",
        "                          'normalized_intensity_PerCP_A_9', 'normalized_intensity_PerCP_A_10', 'normalized_intensity_PerCP_A_11']] = pop_2_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PerCP_A_1, x.intensity_sum_PerCP_A_2, x.intensity_sum_PerCP_A_3, x.intensity_sum_PerCP_A_4, x.intensity_sum_PerCP_A_5,\n",
        "                          x.intensity_sum_PerCP_A_6, x.intensity_sum_PerCP_A_7, x.intensity_sum_PerCP_A_8, x.intensity_sum_PerCP_A_9, x.intensity_sum_PerCP_A_10, x.intensity_sum_PerCP_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating the mean uptake intesities per bin"
      ],
      "metadata": {
        "id": "UEArWI1Hfn8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brVxl4vVHfy7"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['mean_intensity_0.5_1', 'mean_intensity_0.5_2', 'mean_intensity_0.5_3', 'mean_intensity_0.5_4',\n",
        "                          'mean_intensity_0.5_5', 'mean_intensity_0.5_6', 'mean_intensity_0.5_7', 'mean_intensity_0.5_8',\n",
        "                          'mean_intensity_0.5_9', 'mean_intensity_0.5_10', 'mean_intensity_0.5_11']] = pop_2_log_grouped.apply(lambda x: average(x.normalized_intensity_BV421_A_1, x.normalized_intensity_BV421_A_2, x.normalized_intensity_BV421_A_3, x.normalized_intensity_BV421_A_4, x.normalized_intensity_BV421_A_5,\n",
        "                          x.normalized_intensity_BV421_A_6, x.normalized_intensity_BV421_A_7, x.normalized_intensity_BV421_A_8, x.normalized_intensity_BV421_A_9, x.normalized_intensity_BV421_A_10, x.normalized_intensity_BV421_A_11,\n",
        "                          x.sum_BV421_A_1, x.sum_BV421_A_2, x.sum_BV421_A_3, x.sum_BV421_A_4, x.sum_BV421_A_5, x.sum_BV421_A_6, x.sum_BV421_A_7, x.sum_BV421_A_8,x.sum_BV421_A_9, x.sum_BV421_A_10, x.sum_BV421_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dYKhW8THgTX"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['mean_intensity_0.8_1', 'mean_intensity_0.8_2', 'mean_intensity_0.8_3', 'mean_intensity_0.8_4',\n",
        "                          'mean_intensity_0.8_5', 'mean_intensity_0.8_6', 'mean_intensity_0.8_7', 'mean_intensity_0.8_8',\n",
        "                          'mean_intensity_0.8_9', 'mean_intensity_0.8_10', 'mean_intensity_0.8_11']] = pop_2_log_grouped.apply(lambda x: average(x.normalized_intensity_FITC_A_1, x.normalized_intensity_FITC_A_2, x.normalized_intensity_FITC_A_3, x.normalized_intensity_FITC_A_4, x.normalized_intensity_FITC_A_5,\n",
        "                          x.normalized_intensity_FITC_A_6, x.normalized_intensity_FITC_A_7, x.normalized_intensity_FITC_A_8, x.normalized_intensity_FITC_A_9, x.normalized_intensity_FITC_A_10, x.normalized_intensity_FITC_A_11,\n",
        "                          x.sum_FITC_A_1, x.sum_FITC_A_2, x.sum_FITC_A_3, x.sum_FITC_A_4, x.sum_FITC_A_5, x.sum_FITC_A_6, x.sum_FITC_A_7, x.sum_FITC_A_8, x.sum_FITC_A_9, x.sum_FITC_A_10, x.sum_FITC_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrfBja40Hg_L"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['mean_intensity_2.4_1', 'mean_intensity_2.4_2', 'mean_intensity_2.4_3', 'mean_intensity_2.4_4',\n",
        "                          'mean_intensity_2.4_5', 'mean_intensity_2.4_6', 'mean_intensity_2.4_7', 'mean_intensity_2.4_8',\n",
        "                          'mean_intensity_2.4_9', 'mean_intensity_2.4_10', 'mean_intensity_2.4_11']] = pop_2_log_grouped.apply(lambda x: average(x.normalized_intensity_PE_Texas_Red_A_1, x.normalized_intensity_PE_Texas_Red_A_2, x.normalized_intensity_PE_Texas_Red_A_3, x.normalized_intensity_PE_Texas_Red_A_4, x.normalized_intensity_PE_Texas_Red_A_5,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_6, x.normalized_intensity_PE_Texas_Red_A_7, x.normalized_intensity_PE_Texas_Red_A_8,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_9, x.normalized_intensity_PE_Texas_Red_A_10, x.normalized_intensity_PE_Texas_Red_A_11,\n",
        "                          x.sum_PE_Texas_Red_A_1, x.sum_PE_Texas_Red_A_2, x.sum_PE_Texas_Red_A_3, x.sum_PE_Texas_Red_A_4, x.sum_PE_Texas_Red_A_5,\n",
        "                          x.sum_PE_Texas_Red_A_6, x.sum_PE_Texas_Red_A_7, x.sum_PE_Texas_Red_A_8, x.sum_PE_Texas_Red_A_9, x.sum_PE_Texas_Red_A_10, x.sum_PE_Texas_Red_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5wr__kgHhbj"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['mean_intensity_3.36_1', 'mean_intensity_3.36_2', 'mean_intensity_3.36_3', 'mean_intensity_3.36_4',\n",
        "                          'mean_intensity_3.36_5', 'mean_intensity_3.36_6', 'mean_intensity_3.36_7', 'mean_intensity_3.36_8', 'mean_intensity_3.36_9', 'mean_intensity_3.36_10', 'mean_intensity_3.36_11']] = pop_2_log_grouped.apply(lambda x: average(x.normalized_intensity_Alexa_Fluor_700_A_1, x.normalized_intensity_Alexa_Fluor_700_A_2, x.normalized_intensity_Alexa_Fluor_700_A_3, x.normalized_intensity_Alexa_Fluor_700_A_4, x.normalized_intensity_Alexa_Fluor_700_A_5,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_6, x.normalized_intensity_Alexa_Fluor_700_A_7, x.normalized_intensity_Alexa_Fluor_700_A_8,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_9, x.normalized_intensity_Alexa_Fluor_700_A_10, x.normalized_intensity_Alexa_Fluor_700_A_11,\n",
        "                          x.sum_Alexa_Fluor_700_A_1, x.sum_Alexa_Fluor_700_A_2, x.sum_Alexa_Fluor_700_A_3, x.sum_Alexa_Fluor_700_A_4, x.sum_Alexa_Fluor_700_A_5,\n",
        "                          x.sum_Alexa_Fluor_700_A_6, x.sum_Alexa_Fluor_700_A_7, x.sum_Alexa_Fluor_700_A_8, x.sum_Alexa_Fluor_700_A_9, x.sum_Alexa_Fluor_700_A_10, x.sum_Alexa_Fluor_700_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VyZ9XDRHiBz"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped[['mean_intensity_0.04_1', 'mean_intensity_0.04_2', 'mean_intensity_0.04_3', 'mean_intensity_0.04_4',\n",
        "                          'mean_intensity_0.04_5', 'mean_intensity_0.04_6', 'mean_intensity_0.04_7', 'mean_intensity_0.04_8',\n",
        "                          'mean_intensity_0.04_9', 'mean_intensity_0.04_10', 'mean_intensity_0.04_11']] = pop_2_log_grouped.apply(lambda x: average(x.normalized_intensity_PerCP_A_1, x.normalized_intensity_PerCP_A_2, x.normalized_intensity_PerCP_A_3, x.normalized_intensity_PerCP_A_4, x.normalized_intensity_PerCP_A_5,\n",
        "                          x.normalized_intensity_PerCP_A_6, x.normalized_intensity_PerCP_A_7, x.normalized_intensity_PerCP_A_8,\n",
        "                          x.normalized_intensity_PerCP_A_9, x.normalized_intensity_PerCP_A_10, x.normalized_intensity_PerCP_A_11,\n",
        "                          x.sum_PerCP_A_1, x.sum_PerCP_A_2, x.sum_PerCP_A_3, x.sum_PerCP_A_4, x.sum_PerCP_A_5,\n",
        "                          x.sum_PerCP_A_6, x.sum_PerCP_A_7, x.sum_PerCP_A_8,x.sum_PerCP_A_9, x.sum_PerCP_A_10, x.sum_PerCP_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcKFd--_61BV"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped.to_csv('H460_cis_res_log_grouped_filtered.csv') # name the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4CuNyPo61BV"
      },
      "outputs": [],
      "source": [
        "# buliding dfs for median calculations\n",
        "df_median_1_1 = pop_2_log_grouped[['normalized_intensity_BV421_A_1', 'population_number']]\n",
        "df_median_1_2 = pop_2_log_grouped[['normalized_intensity_BV421_A_2', 'population_number']]\n",
        "df_median_1_3 = pop_2_log_grouped[['normalized_intensity_BV421_A_3', 'population_number']]\n",
        "df_median_1_4 = pop_2_log_grouped[['normalized_intensity_BV421_A_4', 'population_number']]\n",
        "df_median_1_5 = pop_2_log_grouped[['normalized_intensity_BV421_A_5', 'population_number']]\n",
        "df_median_1_6 = pop_2_log_grouped[['normalized_intensity_BV421_A_6', 'population_number']]\n",
        "df_median_1_7 = pop_2_log_grouped[['normalized_intensity_BV421_A_7', 'population_number']]\n",
        "df_median_1_8 = pop_2_log_grouped[['normalized_intensity_BV421_A_8', 'population_number']]\n",
        "df_median_1_9 = pop_2_log_grouped[['normalized_intensity_BV421_A_9', 'population_number']]\n",
        "df_median_1_10 = pop_2_log_grouped[['normalized_intensity_BV421_A_10', 'population_number']]\n",
        "df_median_1_11 = pop_2_log_grouped[['normalized_intensity_BV421_A_11', 'population_number']]\n",
        "\n",
        "df_median_2_1 = pop_2_log_grouped[['normalized_intensity_FITC_A_1', 'population_number']]\n",
        "df_median_2_2 = pop_2_log_grouped[['normalized_intensity_FITC_A_2', 'population_number']]\n",
        "df_median_2_3 = pop_2_log_grouped[['normalized_intensity_FITC_A_3', 'population_number']]\n",
        "df_median_2_4 = pop_2_log_grouped[['normalized_intensity_FITC_A_4', 'population_number']]\n",
        "df_median_2_5 = pop_2_log_grouped[['normalized_intensity_FITC_A_5', 'population_number']]\n",
        "df_median_2_6 = pop_2_log_grouped[['normalized_intensity_FITC_A_6', 'population_number']]\n",
        "df_median_2_7 = pop_2_log_grouped[['normalized_intensity_FITC_A_7', 'population_number']]\n",
        "df_median_2_8 = pop_2_log_grouped[['normalized_intensity_FITC_A_8', 'population_number']]\n",
        "df_median_2_9 = pop_2_log_grouped[['normalized_intensity_FITC_A_9', 'population_number']]\n",
        "df_median_2_10 = pop_2_log_grouped[['normalized_intensity_FITC_A_10', 'population_number']]\n",
        "df_median_2_11 = pop_2_log_grouped[['normalized_intensity_FITC_A_11', 'population_number']]\n",
        "\n",
        "df_median_3_1 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'population_number']]\n",
        "df_median_3_2 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_2', 'population_number']]\n",
        "df_median_3_3 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_3', 'population_number']]\n",
        "df_median_3_4 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_4', 'population_number']]\n",
        "df_median_3_5 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_5', 'population_number']]\n",
        "df_median_3_6 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_6', 'population_number']]\n",
        "df_median_3_7 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_7', 'population_number']]\n",
        "df_median_3_8 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_8', 'population_number']]\n",
        "df_median_3_9 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_9', 'population_number']]\n",
        "df_median_3_10 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_10', 'population_number']]\n",
        "df_median_3_11 = pop_2_log_grouped[['normalized_intensity_PE_Texas_Red_A_11', 'population_number']]\n",
        "\n",
        "df_median_4_1 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'population_number']]\n",
        "df_median_4_2 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_2', 'population_number']]\n",
        "df_median_4_3 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_3', 'population_number']]\n",
        "df_median_4_4 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_4', 'population_number']]\n",
        "df_median_4_5 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_5', 'population_number']]\n",
        "df_median_4_6 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_6', 'population_number']]\n",
        "df_median_4_7 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_7', 'population_number']]\n",
        "df_median_4_8 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_8', 'population_number']]\n",
        "df_median_4_9 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_9', 'population_number']]\n",
        "df_median_4_10 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_10', 'population_number']]\n",
        "df_median_4_11 = pop_2_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_11', 'population_number']]\n",
        "\n",
        "df_median_5_1 = pop_2_log_grouped[['normalized_intensity_PerCP_A_1', 'population_number']]\n",
        "df_median_5_2 = pop_2_log_grouped[['normalized_intensity_PerCP_A_2', 'population_number']]\n",
        "df_median_5_3 = pop_2_log_grouped[['normalized_intensity_PerCP_A_3', 'population_number']]\n",
        "df_median_5_4 = pop_2_log_grouped[['normalized_intensity_PerCP_A_4', 'population_number']]\n",
        "df_median_5_5 = pop_2_log_grouped[['normalized_intensity_PerCP_A_5', 'population_number']]\n",
        "df_median_5_6 = pop_2_log_grouped[['normalized_intensity_PerCP_A_6', 'population_number']]\n",
        "df_median_5_7 = pop_2_log_grouped[['normalized_intensity_PerCP_A_7', 'population_number']]\n",
        "df_median_5_8 = pop_2_log_grouped[['normalized_intensity_PerCP_A_8', 'population_number']]\n",
        "df_median_5_9 = pop_2_log_grouped[['normalized_intensity_PerCP_A_9', 'population_number']]\n",
        "df_median_5_10 = pop_2_log_grouped[['normalized_intensity_PerCP_A_10', 'population_number']]\n",
        "df_median_5_11 = pop_2_log_grouped[['normalized_intensity_PerCP_A_11', 'population_number']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQc9aH1q61BV"
      },
      "outputs": [],
      "source": [
        "# filtering out zeroes\n",
        "df_median_1_1 = df_median_1_1[df_median_1_1['normalized_intensity_BV421_A_1'] > 0]\n",
        "df_median_1_2 = df_median_1_2[df_median_1_2['normalized_intensity_BV421_A_2'] > 0]\n",
        "df_median_1_3 = df_median_1_3[df_median_1_3['normalized_intensity_BV421_A_3'] > 0]\n",
        "df_median_1_4 = df_median_1_4[df_median_1_4['normalized_intensity_BV421_A_4'] > 0]\n",
        "df_median_1_5 = df_median_1_5[df_median_1_5['normalized_intensity_BV421_A_5'] > 0]\n",
        "df_median_1_6 = df_median_1_6[df_median_1_6['normalized_intensity_BV421_A_6'] > 0]\n",
        "df_median_1_7 = df_median_1_7[df_median_1_7['normalized_intensity_BV421_A_7'] > 0]\n",
        "df_median_1_8 = df_median_1_8[df_median_1_8['normalized_intensity_BV421_A_8'] > 0]\n",
        "df_median_1_9 = df_median_1_9[df_median_1_9['normalized_intensity_BV421_A_9'] > 0]\n",
        "df_median_1_10 = df_median_1_10[df_median_1_10['normalized_intensity_BV421_A_10'] > 0]\n",
        "df_median_1_11 = df_median_1_11[df_median_1_11['normalized_intensity_BV421_A_11'] > 0]\n",
        "\n",
        "df_median_2_1 = df_median_2_1[df_median_2_1['normalized_intensity_FITC_A_1'] > 0]\n",
        "df_median_2_2 = df_median_2_2[df_median_2_2['normalized_intensity_FITC_A_2'] > 0]\n",
        "df_median_2_3 = df_median_2_3[df_median_2_3['normalized_intensity_FITC_A_3'] > 0]\n",
        "df_median_2_4 = df_median_2_4[df_median_2_4['normalized_intensity_FITC_A_4'] > 0]\n",
        "df_median_2_5 = df_median_2_5[df_median_2_5['normalized_intensity_FITC_A_5'] > 0]\n",
        "df_median_2_6 = df_median_2_6[df_median_2_6['normalized_intensity_FITC_A_6'] > 0]\n",
        "df_median_2_7 = df_median_2_7[df_median_2_7['normalized_intensity_FITC_A_7'] > 0]\n",
        "df_median_2_8 = df_median_2_8[df_median_2_8['normalized_intensity_FITC_A_8'] > 0]\n",
        "df_median_2_9 = df_median_2_9[df_median_2_9['normalized_intensity_FITC_A_9'] > 0]\n",
        "df_median_2_10 = df_median_2_10[df_median_2_10['normalized_intensity_FITC_A_10'] > 0]\n",
        "df_median_2_11 = df_median_2_11[df_median_2_11['normalized_intensity_FITC_A_11'] > 0]\n",
        "\n",
        "df_median_3_1 = df_median_3_1[df_median_3_1['normalized_intensity_PE_Texas_Red_A_1'] > 0]\n",
        "df_median_3_2 = df_median_3_2[df_median_3_2['normalized_intensity_PE_Texas_Red_A_2'] > 0]\n",
        "df_median_3_3 = df_median_3_3[df_median_3_3['normalized_intensity_PE_Texas_Red_A_3'] > 0]\n",
        "df_median_3_4 = df_median_3_4[df_median_3_4['normalized_intensity_PE_Texas_Red_A_4'] > 0]\n",
        "df_median_3_5 = df_median_3_5[df_median_3_5['normalized_intensity_PE_Texas_Red_A_5'] > 0]\n",
        "df_median_3_6 = df_median_3_6[df_median_3_6['normalized_intensity_PE_Texas_Red_A_6'] > 0]\n",
        "df_median_3_7 = df_median_3_7[df_median_3_7['normalized_intensity_PE_Texas_Red_A_7'] > 0]\n",
        "df_median_3_8 = df_median_3_8[df_median_3_8['normalized_intensity_PE_Texas_Red_A_8'] > 0]\n",
        "df_median_3_9 = df_median_3_9[df_median_3_9['normalized_intensity_PE_Texas_Red_A_9'] > 0]\n",
        "df_median_3_10 = df_median_3_10[df_median_3_10['normalized_intensity_PE_Texas_Red_A_10'] > 0]\n",
        "df_median_3_11 = df_median_3_11[df_median_3_11['normalized_intensity_PE_Texas_Red_A_11'] > 0]\n",
        "\n",
        "df_median_4_1 = df_median_4_1[df_median_4_1['normalized_intensity_Alexa_Fluor_700_A_1'] > 0]\n",
        "df_median_4_2 = df_median_4_2[df_median_4_2['normalized_intensity_Alexa_Fluor_700_A_2'] > 0]\n",
        "df_median_4_3 = df_median_4_3[df_median_4_3['normalized_intensity_Alexa_Fluor_700_A_3'] > 0]\n",
        "df_median_4_4 = df_median_4_4[df_median_4_4['normalized_intensity_Alexa_Fluor_700_A_4'] > 0]\n",
        "df_median_4_5 = df_median_4_5[df_median_4_5['normalized_intensity_Alexa_Fluor_700_A_5'] > 0]\n",
        "df_median_4_6 = df_median_4_6[df_median_4_6['normalized_intensity_Alexa_Fluor_700_A_6'] > 0]\n",
        "df_median_4_7 = df_median_4_7[df_median_4_7['normalized_intensity_Alexa_Fluor_700_A_7'] > 0]\n",
        "df_median_4_8 = df_median_4_8[df_median_4_8['normalized_intensity_Alexa_Fluor_700_A_8'] > 0]\n",
        "df_median_4_9 = df_median_4_9[df_median_4_9['normalized_intensity_Alexa_Fluor_700_A_9'] > 0]\n",
        "df_median_4_10 = df_median_4_10[df_median_4_10['normalized_intensity_Alexa_Fluor_700_A_10'] > 0]\n",
        "df_median_4_11 = df_median_4_11[df_median_4_11['normalized_intensity_Alexa_Fluor_700_A_11'] > 0]\n",
        "\n",
        "df_median_5_1 = df_median_5_1[df_median_5_1['normalized_intensity_PerCP_A_1'] > 0]\n",
        "df_median_5_2 = df_median_5_2[df_median_5_2['normalized_intensity_PerCP_A_2'] > 0]\n",
        "df_median_5_3 = df_median_5_3[df_median_5_3['normalized_intensity_PerCP_A_3'] > 0]\n",
        "df_median_5_4 = df_median_5_4[df_median_5_4['normalized_intensity_PerCP_A_4'] > 0]\n",
        "df_median_5_5 = df_median_5_5[df_median_5_5['normalized_intensity_PerCP_A_5'] > 0]\n",
        "df_median_5_6 = df_median_5_6[df_median_5_6['normalized_intensity_PerCP_A_6'] > 0]\n",
        "df_median_5_7 = df_median_5_7[df_median_5_7['normalized_intensity_PerCP_A_7'] > 0]\n",
        "df_median_5_8 = df_median_5_8[df_median_5_8['normalized_intensity_PerCP_A_8'] > 0]\n",
        "df_median_5_9 = df_median_5_9[df_median_5_9['normalized_intensity_PerCP_A_9'] > 0]\n",
        "df_median_5_10 = df_median_5_10[df_median_5_10['normalized_intensity_PerCP_A_10'] > 0]\n",
        "df_median_5_11 = df_median_5_11[df_median_5_11['normalized_intensity_PerCP_A_11'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihh31Kjv61BV"
      },
      "outputs": [],
      "source": [
        "# grouping and calculating the median per positive (>0) values per bin\n",
        "df_subset_1_1 = df_median_1_1.loc[:, ['population_number', 'normalized_intensity_BV421_A_1'] ]\n",
        "df_median_1_1_grouped = df_median_1_1.groupby('population_number').median()\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.rename(columns={\"normalized_intensity_BV421_A_1\": \"0.5_1_median_intensity\"})\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.reset_index()\n",
        "df_subset_1_2 = df_median_1_2.loc[:, ['population_number', 'normalized_intensity_BV421_A_2'] ]\n",
        "df_median_1_2_grouped = df_median_1_2.groupby('population_number').median()\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.rename(columns={\"normalized_intensity_BV421_A_2\": \"0.5_2_median_intensity\"})\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.reset_index()\n",
        "df_subset_1_3 = df_median_1_3.loc[:, ['population_number', 'normalized_intensity_BV421_A_3'] ]\n",
        "df_median_1_3_grouped = df_median_1_3.groupby('population_number').median()\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.rename(columns={\"normalized_intensity_BV421_A_3\": \"0.5_3_median_intensity\"})\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.reset_index()\n",
        "df_subset_1_4 = df_median_1_4.loc[:, ['population_number', 'normalized_intensity_BV421_A_4'] ]\n",
        "df_median_1_4_grouped = df_median_1_4.groupby('population_number').median()\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.rename(columns={\"normalized_intensity_BV421_A_4\": \"0.5_4_median_intensity\"})\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.reset_index()\n",
        "df_subset_1_5 = df_median_1_5.loc[:, ['population_number', 'normalized_intensity_BV421_A_5'] ]\n",
        "df_median_1_5_grouped = df_median_1_5.groupby('population_number').median()\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.rename(columns={\"normalized_intensity_BV421_A_5\": \"0.5_5_median_intensity\"})\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.reset_index()\n",
        "df_subset_1_6 = df_median_1_6.loc[:, ['population_number', 'normalized_intensity_BV421_A_6'] ]\n",
        "df_median_1_6_grouped = df_median_1_6.groupby('population_number').median()\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.rename(columns={\"normalized_intensity_BV421_A_6\": \"0.5_6_median_intensity\"})\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.reset_index()\n",
        "df_subset_1_7 = df_median_1_7.loc[:, ['population_number', 'normalized_intensity_BV421_A_7'] ]\n",
        "df_median_1_7_grouped = df_median_1_7.groupby('population_number').median()\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.rename(columns={\"normalized_intensity_BV421_A_7\": \"0.5_7_median_intensity\"})\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.reset_index()\n",
        "df_subset_1_8 = df_median_1_8.loc[:, ['population_number', 'normalized_intensity_BV421_A_8'] ]\n",
        "df_median_1_8_grouped = df_median_1_8.groupby('population_number').median()\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.rename(columns={\"normalized_intensity_BV421_A_8\": \"0.5_8_median_intensity\"})\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.reset_index()\n",
        "df_subset_1_9 = df_median_1_9.loc[:, ['population_number', 'normalized_intensity_BV421_A_9'] ]\n",
        "df_median_1_9_grouped = df_median_1_9.groupby('population_number').median()\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.rename(columns={\"normalized_intensity_BV421_A_9\": \"0.5_9_median_intensity\"})\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.reset_index()\n",
        "df_subset_1_10 = df_median_1_10.loc[:, ['population_number', 'normalized_intensity_BV421_A_10'] ]\n",
        "df_median_1_10_grouped = df_median_1_10.groupby('population_number').median()\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.rename(columns={\"normalized_intensity_BV421_A_10\": \"0.5_10_median_intensity\"})\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.reset_index()\n",
        "df_subset_1_11 = df_median_1_11.loc[:, ['population_number', 'normalized_intensity_BV421_A_11'] ]\n",
        "df_median_1_11_grouped = df_median_1_11.groupby('population_number').median()\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.rename(columns={\"normalized_intensity_BV421_A_11\": \"0.5_11_median_intensity\"})\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.reset_index()\n",
        "\n",
        "df_subset_2_1 = df_median_2_1.loc[:, ['population_number', 'normalized_intensity_FITC_A_1'] ]\n",
        "df_median_2_1_grouped = df_median_2_1.groupby('population_number').median()\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.rename(columns={\"normalized_intensity_FITC_A_1\": \"0.8_1_median_intensity\"})\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.reset_index()\n",
        "df_subset_2_2 = df_median_2_2.loc[:, ['population_number', 'normalized_intensity_FITC_A_2'] ]\n",
        "df_median_2_2_grouped = df_median_2_2.groupby('population_number').median()\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.rename(columns={\"normalized_intensity_FITC_A_2\": \"0.8_2_median_intensity\"})\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.reset_index()\n",
        "df_subset_2_3 = df_median_2_3.loc[:, ['population_number', 'normalized_intensity_FITC_A_3'] ]\n",
        "df_median_2_3_grouped = df_median_2_3.groupby('population_number').median()\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.rename(columns= {\"normalized_intensity_FITC_A_3\": \"0.8_3_median_intensity\"})\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.reset_index()\n",
        "df_subset_2_4 = df_median_2_4.loc[:, ['population_number', 'normalized_intensity_FITC_A_4'] ]\n",
        "df_median_2_4_grouped = df_median_2_4.groupby('population_number').median()\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.rename(columns={\"normalized_intensity_FITC_A_4\": \"0.8_4_median_intensity\"})\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.reset_index()\n",
        "df_subset_2_5 = df_median_2_5.loc[:, ['population_number', 'normalized_intensity_FITC_A_5'] ]\n",
        "df_median_2_5_grouped = df_median_2_5.groupby('population_number').median()\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.rename(columns={\"normalized_intensity_FITC_A_5\": \"0.8_5_median_intensity\"})\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.reset_index()\n",
        "df_subset_2_6 = df_median_2_6.loc[:, ['population_number', 'normalized_intensity_FITC_A_6'] ]\n",
        "df_median_2_6_grouped = df_median_2_6.groupby('population_number').median()\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.rename(columns={\"normalized_intensity_FITC_A_6\": \"0.8_6_median_intensity\"})\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.reset_index()\n",
        "df_subset_2_7 = df_median_2_7.loc[:, ['population_number', 'normalized_intensity_FITC_A_7'] ]\n",
        "df_median_2_7_grouped = df_median_2_7.groupby('population_number').median()\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.rename(columns={\"normalized_intensity_FITC_A_7\": \"0.8_7_median_intensity\"})\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.reset_index()\n",
        "df_subset_2_8 = df_median_2_8.loc[:, ['population_number', 'normalized_intensity_FITC_A_8'] ]\n",
        "df_median_2_8_grouped = df_median_2_8.groupby('population_number').median()\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.rename(columns={\"normalized_intensity_FITC_A_8\": \"0.8_8_median_intensity\"})\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.reset_index()\n",
        "df_subset_2_9 = df_median_2_9.loc[:, ['population_number', 'normalized_intensity_FITC_A_9'] ]\n",
        "df_median_2_9_grouped = df_median_2_9.groupby('population_number').median()\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.rename(columns={\"normalized_intensity_FITC_A_9\": \"0.8_9_median_intensity\"})\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.reset_index()\n",
        "df_subset_2_10 = df_median_2_10.loc[:, ['population_number', 'normalized_intensity_FITC_A_10'] ]\n",
        "df_median_2_10_grouped = df_median_2_10.groupby('population_number').median()\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.rename(columns={\"normalized_intensity_FITC_A_10\": \"0.8_10_median_intensity\"})\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.reset_index()\n",
        "df_subset_2_11 = df_median_2_11.loc[:, ['population_number', 'normalized_intensity_FITC_A_11'] ]\n",
        "df_median_2_11_grouped = df_median_2_11.groupby('population_number').median()\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.rename(columns={\"normalized_intensity_FITC_A_11\": \"0.8_11_median_intensity\"})\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.reset_index()\n",
        "\n",
        "df_subset_3_1 = df_median_3_1.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_1'] ]\n",
        "df_median_3_1_grouped = df_median_3_1.groupby('population_number').median()\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_1\": \"2.4_1_median_intensity\"})\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.reset_index()\n",
        "df_subset_3_2 = df_median_3_2.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_2'] ]\n",
        "df_median_3_2_grouped = df_median_3_2.groupby('population_number').median()\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_2\": \"2.4_2_median_intensity\"})\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.reset_index()\n",
        "df_subset_3_3 = df_median_3_3.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_3'] ]\n",
        "df_median_3_3_grouped = df_median_3_3.groupby('population_number').median()\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.rename(columns= {\"normalized_intensity_PE_Texas_Red_A_3\": \"2.4_3_median_intensity\"})\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.reset_index()\n",
        "df_subset_3_4 = df_median_3_4.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_4'] ]\n",
        "df_median_3_4_grouped = df_median_3_4.groupby('population_number').median()\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_4\": \"2.4_4_median_intensity\"})\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.reset_index()\n",
        "df_subset_3_5 = df_median_3_5.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_5'] ]\n",
        "df_median_3_5_grouped = df_median_3_5.groupby('population_number').median()\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_5\": \"2.4_5_median_intensity\"})\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.reset_index()\n",
        "df_subset_3_6 = df_median_3_6.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_6'] ]\n",
        "df_median_3_6_grouped = df_median_3_6.groupby('population_number').median()\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_6\": \"2.4_6_median_intensity\"})\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.reset_index()\n",
        "df_subset_3_7 = df_median_3_7.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_7'] ]\n",
        "df_median_3_7_grouped = df_median_3_7.groupby('population_number').median()\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_7\": \"2.4_7_median_intensity\"})\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.reset_index()\n",
        "df_subset_3_8 = df_median_3_8.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_8'] ]\n",
        "df_median_3_8_grouped = df_median_3_8.groupby('population_number').median()\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_8\": \"2.4_8_median_intensity\"})\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.reset_index()\n",
        "df_subset_3_9 = df_median_3_9.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_9'] ]\n",
        "df_median_3_9_grouped = df_median_3_9.groupby('population_number').median()\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_9\": \"2.4_9_median_intensity\"})\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.reset_index()\n",
        "df_subset_3_10 = df_median_3_10.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_10'] ]\n",
        "df_median_3_10_grouped = df_median_3_10.groupby('population_number').median()\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_10\": \"2.4_10_median_intensity\"})\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.reset_index()\n",
        "df_subset_3_11 = df_median_3_11.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_11'] ]\n",
        "df_median_3_11_grouped = df_median_3_11.groupby('population_number').median()\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_11\": \"2.4_11_median_intensity\"})\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.reset_index()\n",
        "\n",
        "df_subset_4_1 = df_median_4_1.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_1'] ]\n",
        "df_median_4_1_grouped = df_median_4_1.groupby('population_number').median()\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_1\": \"3.36_1_median_intensity\"})\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.reset_index()\n",
        "df_subset_4_2 = df_median_4_2.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_2'] ]\n",
        "df_median_4_2_grouped = df_median_4_2.groupby('population_number').median()\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_2\": \"3.36_2_median_intensity\"})\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.reset_index()\n",
        "df_subset_4_3 = df_median_4_3.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_3'] ]\n",
        "df_median_4_3_grouped = df_median_4_3.groupby('population_number').median()\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.rename(columns= {\"normalized_intensity_Alexa_Fluor_700_A_3\": \"3.36_3_median_intensity\"})\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.reset_index()\n",
        "df_subset_4_4 = df_median_4_4.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_4'] ]\n",
        "df_median_4_4_grouped = df_median_4_4.groupby('population_number').median()\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_4\": \"3.36_4_median_intensity\"})\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.reset_index()\n",
        "df_subset_4_5 = df_median_4_5.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_5'] ]\n",
        "df_median_4_5_grouped = df_median_4_5.groupby('population_number').median()\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_5\": \"3.36_5_median_intensity\"})\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.reset_index()\n",
        "df_subset_4_6 = df_median_4_6.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_6'] ]\n",
        "df_median_4_6_grouped = df_median_4_6.groupby('population_number').median()\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_6\": \"3.36_6_median_intensity\"})\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.reset_index()\n",
        "df_subset_4_7 = df_median_4_7.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_7'] ]\n",
        "df_median_4_7_grouped = df_median_4_7.groupby('population_number').median()\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_7\": \"3.36_7_median_intensity\"})\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.reset_index()\n",
        "df_subset_4_8 = df_median_4_8.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_8'] ]\n",
        "df_median_4_8_grouped = df_median_4_8.groupby('population_number').median()\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_8\": \"3.36_8_median_intensity\"})\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.reset_index()\n",
        "df_subset_4_9 = df_median_4_9.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_9'] ]\n",
        "df_median_4_9_grouped = df_median_4_9.groupby('population_number').median()\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_9\": \"3.36_9_median_intensity\"})\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.reset_index()\n",
        "df_subset_4_10 = df_median_4_10.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_10'] ]\n",
        "df_median_4_10_grouped = df_median_4_10.groupby('population_number').median()\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_10\": \"3.36_10_median_intensity\"})\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.reset_index()\n",
        "df_subset_4_11 = df_median_4_11.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_11'] ]\n",
        "df_median_4_11_grouped = df_median_4_11.groupby('population_number').median()\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_11\": \"3.36_11_median_intensity\"})\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.reset_index()\n",
        "\n",
        "df_subset_5_1 = df_median_5_1.loc[:, ['population_number', 'normalized_intensity_PerCP_A_1']]\n",
        "df_median_5_1_grouped = df_median_5_1.groupby('population_number').median()\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.rename(columns={\"normalized_intensity_PerCP_A_1\": \"0.04_1_median_intensity\"})\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.reset_index()\n",
        "df_subset_5_2 = df_median_5_2.loc[:, ['population_number', 'normalized_intensity_PerCP_A_2'] ]\n",
        "df_median_5_2_grouped = df_median_5_2.groupby('population_number').median()\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.rename(columns={\"normalized_intensity_PerCP_A_2\": \"0.04_2_median_intensity\"})\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.reset_index()\n",
        "df_subset_5_3 = df_median_5_3.loc[:, ['population_number', 'normalized_intensity_PerCP_A_3'] ]\n",
        "df_median_5_3_grouped = df_median_5_3.groupby('population_number').median()\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.rename(columns= {\"normalized_intensity_PerCP_A_3\": \"0.04_3_median_intensity\"})\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.reset_index()\n",
        "df_subset_5_4 = df_median_5_4.loc[:, ['population_number', 'normalized_intensity_PerCP_A_4'] ]\n",
        "df_median_5_4_grouped = df_median_5_4.groupby('population_number').median()\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.rename(columns={\"normalized_intensity_PerCP_A_4\": \"0.04_4_median_intensity\"})\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.reset_index()\n",
        "df_subset_5_5 = df_median_5_5.loc[:, ['population_number', 'normalized_intensity_PerCP_A_5'] ]\n",
        "df_median_5_5_grouped = df_median_5_5.groupby('population_number').median()\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.rename(columns={\"normalized_intensity_PerCP_A_5\": \"0.04_5_median_intensity\"})\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.reset_index()\n",
        "df_subset_5_6 = df_median_5_6.loc[:, ['population_number', 'normalized_intensity_PerCP_A_6'] ]\n",
        "df_median_5_6_grouped = df_median_5_6.groupby('population_number').median()\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.rename(columns={\"normalized_intensity_PerCP_A_6\": \"0.04_6_median_intensity\"})\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.reset_index()\n",
        "df_subset_5_7 = df_median_5_7.loc[:, ['population_number', 'normalized_intensity_PerCP_A_7'] ]\n",
        "df_median_5_7_grouped = df_median_5_7.groupby('population_number').median()\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.rename(columns={\"normalized_intensity_PerCP_A_7\": \"0.04_7_median_intensity\"})\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.reset_index()\n",
        "df_subset_5_8 = df_median_5_8.loc[:, ['population_number', 'normalized_intensity_PerCP_A_8'] ]\n",
        "df_median_5_8_grouped = df_median_5_8.groupby('population_number').median()\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.rename(columns={\"normalized_intensity_PerCP_A_8\": \"0.04_8_median_intensity\"})\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.reset_index()\n",
        "df_subset_5_9 = df_median_5_9.loc[:, ['population_number', 'normalized_intensity_PerCP_A_9'] ]\n",
        "df_median_5_9_grouped = df_median_5_9.groupby('population_number').median()\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.rename(columns={\"normalized_intensity_PerCP_A_9\": \"0.04_9_median_intensity\"})\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.reset_index()\n",
        "df_subset_5_10 = df_median_5_10.loc[:, ['population_number', 'normalized_intensity_PerCP_A_10'] ]\n",
        "df_median_5_10_grouped = df_median_5_10.groupby('population_number').median()\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.rename(columns={\"normalized_intensity_PerCP_A_10\": \"0.04_10_median_intensity\"})\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.reset_index()\n",
        "df_subset_5_11 = df_median_5_11.loc[:, ['population_number', 'normalized_intensity_PerCP_A_11'] ]\n",
        "df_median_5_11_grouped = df_median_5_11.groupby('population_number').median()\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.rename(columns={\"normalized_intensity_PerCP_A_11\": \"0.04_11_median_intensity\"})\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu5THDdU61BW"
      },
      "outputs": [],
      "source": [
        "# returning the zeros that were filtered out to the dfs\n",
        "pop_2_log_grouped[['0.5_1_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_1_grouped.population_number, df_median_1_1_grouped['0.5_1_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_2_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_2_grouped.population_number, df_median_1_2_grouped['0.5_2_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_3_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_3_grouped.population_number, df_median_1_3_grouped['0.5_3_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_4_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_4_grouped.population_number, df_median_1_4_grouped['0.5_4_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_5_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_5_grouped.population_number, df_median_1_5_grouped['0.5_5_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_6_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_6_grouped.population_number, df_median_1_6_grouped['0.5_6_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_7_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_7_grouped.population_number, df_median_1_7_grouped['0.5_7_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_8_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_8_grouped.population_number, df_median_1_8_grouped['0.5_8_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_9_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_9_grouped.population_number, df_median_1_9_grouped['0.5_9_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_10_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_10_grouped.population_number, df_median_1_10_grouped['0.5_10_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.5_11_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_11_grouped.population_number, df_median_1_11_grouped['0.5_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_log_grouped[['0.8_1_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_1_grouped.population_number, df_median_2_1_grouped['0.8_1_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_2_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_2_grouped.population_number, df_median_2_2_grouped['0.8_2_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_3_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_3_grouped.population_number, df_median_2_3_grouped['0.8_3_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_4_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_4_grouped.population_number, df_median_2_4_grouped['0.8_4_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_5_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_5_grouped.population_number, df_median_2_5_grouped['0.8_5_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_6_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_6_grouped.population_number, df_median_2_6_grouped['0.8_6_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_7_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_7_grouped.population_number, df_median_2_7_grouped['0.8_7_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_8_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_8_grouped.population_number, df_median_2_8_grouped['0.8_8_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_9_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_9_grouped.population_number, df_median_2_9_grouped['0.8_9_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_10_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_10_grouped.population_number, df_median_2_10_grouped['0.8_10_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.8_11_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_11_grouped.population_number, df_median_2_11_grouped['0.8_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_log_grouped[['2.4_1_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_1_grouped.population_number, df_median_3_1_grouped['2.4_1_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_2_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_2_grouped.population_number, df_median_3_2_grouped['2.4_2_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_3_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_3_grouped.population_number, df_median_3_3_grouped['2.4_3_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_4_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_4_grouped.population_number, df_median_3_4_grouped['2.4_4_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_5_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_5_grouped.population_number, df_median_3_5_grouped['2.4_5_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_6_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_6_grouped.population_number, df_median_3_6_grouped['2.4_6_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_7_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_7_grouped.population_number, df_median_3_7_grouped['2.4_7_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_8_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_8_grouped.population_number, df_median_3_8_grouped['2.4_8_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_9_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_9_grouped.population_number, df_median_3_9_grouped['2.4_9_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_10_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_10_grouped.population_number, df_median_3_10_grouped['2.4_10_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['2.4_11_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_11_grouped.population_number, df_median_3_11_grouped['2.4_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_log_grouped[['3.36_1_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_1_grouped.population_number, df_median_4_1_grouped['3.36_1_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_2_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_2_grouped.population_number, df_median_4_2_grouped['3.36_2_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_3_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_3_grouped.population_number, df_median_4_3_grouped['3.36_3_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_4_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_4_grouped.population_number, df_median_4_4_grouped['3.36_4_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_5_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_5_grouped.population_number, df_median_4_5_grouped['3.36_5_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_6_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_6_grouped.population_number, df_median_4_6_grouped['3.36_6_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_7_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_7_grouped.population_number, df_median_4_7_grouped['3.36_7_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_8_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_8_grouped.population_number, df_median_4_8_grouped['3.36_8_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_9_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_9_grouped.population_number, df_median_4_9_grouped['3.36_9_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_10_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_10_grouped.population_number, df_median_4_10_grouped['3.36_10_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['3.36_11_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_11_grouped.population_number, df_median_4_11_grouped['3.36_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_log_grouped[['0.04_1_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_1_grouped.population_number, df_median_5_1_grouped['0.04_1_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_2_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_2_grouped.population_number, df_median_5_2_grouped['0.04_2_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_3_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_3_grouped.population_number, df_median_5_3_grouped['0.04_3_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_4_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_4_grouped.population_number, df_median_5_4_grouped['0.04_4_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_5_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_5_grouped.population_number, df_median_5_5_grouped['0.04_5_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_6_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_6_grouped.population_number, df_median_5_6_grouped['0.04_6_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_7_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_7_grouped.population_number, df_median_5_7_grouped['0.04_7_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_8_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_8_grouped.population_number, df_median_5_8_grouped['0.04_8_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_9_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_9_grouped.population_number, df_median_5_9_grouped['0.04_9_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_10_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_10_grouped.population_number, df_median_5_10_grouped['0.04_10_median_intensity']), axis = 1)\n",
        "pop_2_log_grouped[['0.04_11_median_intensity']] = pop_2_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_11_grouped.population_number, df_median_5_11_grouped['0.04_11_median_intensity']), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UCwp8_L61BW"
      },
      "outputs": [],
      "source": [
        "#removing float groups values\n",
        "pop_2_log_grouped.drop(pop_2_log_grouped[pop_2_log_grouped['group'] > pop_2_log_grouped['group'].apply(np.floor)].index, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA9z17zs61BW"
      },
      "outputs": [],
      "source": [
        "pop_2_log_grouped.to_csv('H460_cis_res_log_grouped_filtered_final.csv') # name the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Rs44Lu61BX"
      },
      "outputs": [],
      "source": [
        "# merging the training data of the two populations\n",
        "populations_df_list = [pop_1_log_grouped, pop_2_log_grouped]\n",
        "Merge_population = pd.DataFrame()\n",
        "for df in populations_df_list:\n",
        "    Merge_population = pd.concat([Merge_population, df], ignore_index=True)\n",
        "\n",
        "Merge_population.to_csv('Merge_population_H460_filtered.csv') # name the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the training preprocessed data as a pickle\n",
        "model_name = 'Merge_population_H460_train_filtered' # name the data\n",
        "pickle.dump(Merge_population, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "bSVEbzAwjquu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 2 test data"
      ],
      "metadata": {
        "id": "pkwefFAeddAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA_OpX3siTbH"
      },
      "outputs": [],
      "source": [
        "# apply bins function on the different particle features\n",
        "Total_pop_2_test_log[['BV421_A_1', 'BV421_A_2', 'BV421_A_3', 'BV421_A_4', 'BV421_A_5', 'BV421_A_6', 'BV421_A_7', 'BV421_A_8', 'BV421_A_9', 'BV421_A_10', 'BV421_A_11']] = Total_pop_2_test_log['BV421_A'].apply(bins)\n",
        "Total_pop_2_test_log[['FITC_A_1', 'FITC_A_2', 'FITC_A_3', 'FITC_A_4', 'FITC_A_5', 'FITC_A_6', 'FITC_A_7', 'FITC_A_8', 'FITC_A_9', 'FITC_A_10', 'FITC_A_11']] = Total_pop_2_test_log['FITC_A'].apply(bins)\n",
        "Total_pop_2_test_log[['PE_Texas_Red_A_1', 'PE_Texas_Red_A_2', 'PE_Texas_Red_A_3', 'PE_Texas_Red_A_4', 'PE_Texas_Red_A_5', 'PE_Texas_Red_A_6', 'PE_Texas_Red_A_7', 'PE_Texas_Red_A_8', 'PE_Texas_Red_A_9', 'PE_Texas_Red_A_10', 'PE_Texas_Red_A_11']] = Total_pop_2_test_log['PE_Texas_Red_A'].apply(bins)\n",
        "Total_pop_2_test_log[['Alexa_Fluor_700_A_1', 'Alexa_Fluor_700_A_2', 'Alexa_Fluor_700_A_3', 'Alexa_Fluor_700_A_4', 'Alexa_Fluor_700_A_5', 'Alexa_Fluor_700_A_6', 'Alexa_Fluor_700_A_7', 'Alexa_Fluor_700_A_8', 'Alexa_Fluor_700_A_9', 'Alexa_Fluor_700_A_10', 'Alexa_Fluor_700_A_11']] = Total_pop_2_test_log['Alexa_Fluor_700_A'].apply(bins)\n",
        "Total_pop_2_test_log[['PerCP_A_1', 'PerCP_A_2', 'PerCP_A_3', 'PerCP_A_4', 'PerCP_A_5', 'PerCP_A_6', 'PerCP_A_7', 'PerCP_A_8', 'PerCP_A_9', 'PerCP_A_10', 'PerCP_A_11']] = Total_pop_2_test_log['PerCP_A'].apply(bins)\n",
        "Total_pop_2_test_log.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQIJFUSpiTbH"
      },
      "outputs": [],
      "source": [
        "# applying bins intensity function on the particle features\n",
        "Total_pop_2_test_log[['0.5_1_intensity', '0.5_2_intensity', '0.5_3_intensity', '0.5_4_intensity', '0.5_5_intensity', '0.5_6_intensity', '0.5_7_intensity', '0.5_8_intensity', '0.5_9_intensity', '0.5_10_intensity', '0.5_11_intensity']] = Total_pop_2_test_log.apply(lambda x: bins_intensity(x.BV421_A, x.BV421_A_1, x.BV421_A_2, x.BV421_A_3, x.BV421_A_4, x.BV421_A_5, x.BV421_A_6, x.BV421_A_7, x.BV421_A_8, x.BV421_A_9, x.BV421_A_10, x.BV421_A_11), axis=1)\n",
        "Total_pop_2_test_log[['0.8_1_intensity', '0.8_2_intensity', '0.8_3_intensity', '0.8_4_intensity', '0.8_5_intensity', '0.8_6_intensity', '0.8_7_intensity', '0.8_8_intensity', '0.8_9_intensity', '0.8_10_intensity', '0.8_11_intensity']] = Total_pop_2_test_log.apply(lambda x: bins_intensity(x.FITC_A, x.FITC_A_1, x.FITC_A_2, x.FITC_A_3, x.FITC_A_4, x.FITC_A_5, x.FITC_A_6, x.FITC_A_7, x.FITC_A_8, x.FITC_A_9, x.FITC_A_10, x.FITC_A_11), axis=1)\n",
        "Total_pop_2_test_log[['2.4_1_intensity', '2.4_2_intensity', '2.4_3_intensity', '2.4_4_intensity', '2.4_5_intensity', '2.4_6_intensity', '2.4_7_intensity', '2.4_8_intensity', '2.4_9_intensity', '2.4_10_intensity', '2.4_11_intensity']] = Total_pop_2_test_log.apply(lambda x: bins_intensity(x.PE_Texas_Red_A, x.PE_Texas_Red_A_1, x.PE_Texas_Red_A_2, x.PE_Texas_Red_A_3, x.PE_Texas_Red_A_4, x.PE_Texas_Red_A_5, x.PE_Texas_Red_A_6, x.PE_Texas_Red_A_7, x.PE_Texas_Red_A_8, x.PE_Texas_Red_A_9, x.PE_Texas_Red_A_10, x.PE_Texas_Red_A_11), axis=1)\n",
        "Total_pop_2_test_log[['3.36_1_intensity', '3.36_2_intensity', '3.36_3_intensity', '3.36_4_intensity', '3.36_5_intensity', '3.36_6_intensity', '3.36_7_intensity', '3.36_8_intensity', '3.36_9_intensity', '3.36_10_intensity', '3.36_11_intensity']] = Total_pop_2_test_log.apply(lambda x: bins_intensity(x.Alexa_Fluor_700_A, x.Alexa_Fluor_700_A_1, x.Alexa_Fluor_700_A_2, x.Alexa_Fluor_700_A_3, x.Alexa_Fluor_700_A_4, x.Alexa_Fluor_700_A_5, x.Alexa_Fluor_700_A_6, x.Alexa_Fluor_700_A_7, x.Alexa_Fluor_700_A_8, x.Alexa_Fluor_700_A_9, x.Alexa_Fluor_700_A_10, x.Alexa_Fluor_700_A_11), axis=1)\n",
        "Total_pop_2_test_log[['0.04_1_intensity', '0.04_2_intensity', '0.04_3_intensity', '0.04_4_intensity', '0.04_5_intensity', '0.04_6_intensity', '0.04_7_intensity', '0.04_8_intensity', '0.04_9_intensity', '0.04_10_intensity', '0.04_11_intensity']] = Total_pop_2_test_log.apply(lambda x: bins_intensity(x.PerCP_A, x.PerCP_A_1, x.PerCP_A_2, x.PerCP_A_3, x.PerCP_A_4, x.PerCP_A_5, x.PerCP_A_6, x.PerCP_A_7, x.PerCP_A_8, x.PerCP_A_9, x.PerCP_A_10, x.PerCP_A_11), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIuBsCKoiTbH"
      },
      "outputs": [],
      "source": [
        "# grouping all the relevant data per population and building new df's that include only the relevant data for analysis\n",
        "grouper = Total_pop_2_test_log.groupby(pd.Grouper(key='population_number'))\n",
        "BV421_A_1_intensity_sum = grouper['0.5_1_intensity'].sum().to_frame(name='intensity_sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_intensity_sum = grouper['0.5_2_intensity'].sum().to_frame(name='intensity_sum_BV421_A_2').reset_index()\n",
        "BV421_A_3_intensity_sum = grouper['0.5_3_intensity'].sum().to_frame(name='intensity_sum_BV421_A_3').reset_index()\n",
        "BV421_A_4_intensity_sum = grouper['0.5_4_intensity'].sum().to_frame(name='intensity_sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_intensity_sum = grouper['0.5_5_intensity'].sum().to_frame(name='intensity_sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_intensity_sum = grouper['0.5_6_intensity'].sum().to_frame(name='intensity_sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_intensity_sum = grouper['0.5_7_intensity'].sum().to_frame(name='intensity_sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_intensity_sum = grouper['0.5_8_intensity'].sum().to_frame(name='intensity_sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_intensity_sum = grouper['0.5_9_intensity'].sum().to_frame(name='intensity_sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_intensity_sum = grouper['0.5_10_intensity'].sum().to_frame(name='intensity_sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_intensity_sum = grouper['0.5_11_intensity'].sum().to_frame(name='intensity_sum_BV421_A_11').reset_index()\n",
        "BV421_A_1_sum = grouper['BV421_A_1'].sum().to_frame(name='sum_BV421_A_1').reset_index()\n",
        "BV421_A_2_sum = grouper['BV421_A_2'].sum().to_frame(name='sum_BV421_A_2').reset_index()\n",
        "BV421_A_4_sum = grouper['BV421_A_4'].sum().to_frame(name='sum_BV421_A_4').reset_index()\n",
        "BV421_A_5_sum = grouper['BV421_A_5'].sum().to_frame(name='sum_BV421_A_5').reset_index()\n",
        "BV421_A_6_sum = grouper['BV421_A_6'].sum().to_frame(name='sum_BV421_A_6').reset_index()\n",
        "BV421_A_7_sum = grouper['BV421_A_7'].sum().to_frame(name='sum_BV421_A_7').reset_index()\n",
        "BV421_A_8_sum = grouper['BV421_A_8'].sum().to_frame(name='sum_BV421_A_8').reset_index()\n",
        "BV421_A_9_sum = grouper['BV421_A_9'].sum().to_frame(name='sum_BV421_A_9').reset_index()\n",
        "BV421_A_10_sum = grouper['BV421_A_10'].sum().to_frame(name='sum_BV421_A_10').reset_index()\n",
        "BV421_A_11_sum = grouper['BV421_A_11'].sum().to_frame(name='sum_BV421_A_11').reset_index()\n",
        "\n",
        "FITC_A_1_intensity_sum = grouper['0.8_1_intensity'].sum().to_frame(name='intensity_sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_intensity_sum = grouper['0.8_2_intensity'].sum().to_frame(name='intensity_sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_intensity_sum = grouper['0.8_3_intensity'].sum().to_frame(name='intensity_sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_intensity_sum = grouper['0.8_4_intensity'].sum().to_frame(name='intensity_sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_intensity_sum = grouper['0.8_5_intensity'].sum().to_frame(name='intensity_sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_intensity_sum = grouper['0.8_6_intensity'].sum().to_frame(name='intensity_sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_intensity_sum = grouper['0.8_7_intensity'].sum().to_frame(name='intensity_sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_intensity_sum = grouper['0.8_8_intensity'].sum().to_frame(name='intensity_sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_intensity_sum = grouper['0.8_9_intensity'].sum().to_frame(name='intensity_sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_intensity_sum = grouper['0.8_10_intensity'].sum().to_frame(name='intensity_sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_intensity_sum = grouper['0.8_11_intensity'].sum().to_frame(name='intensity_sum_FITC_A_11').reset_index()\n",
        "FITC_A_1_sum = grouper['FITC_A_1'].sum().to_frame(name='sum_FITC_A_1').reset_index()\n",
        "FITC_A_2_sum = grouper['FITC_A_2'].sum().to_frame(name='sum_FITC_A_2').reset_index()\n",
        "FITC_A_3_sum = grouper['FITC_A_3'].sum().to_frame(name='sum_FITC_A_3').reset_index()\n",
        "FITC_A_4_sum = grouper['FITC_A_4'].sum().to_frame(name='sum_FITC_A_4').reset_index()\n",
        "FITC_A_5_sum = grouper['FITC_A_5'].sum().to_frame(name='sum_FITC_A_5').reset_index()\n",
        "FITC_A_6_sum = grouper['FITC_A_6'].sum().to_frame(name='sum_FITC_A_6').reset_index()\n",
        "FITC_A_7_sum = grouper['FITC_A_7'].sum().to_frame(name='sum_FITC_A_7').reset_index()\n",
        "FITC_A_8_sum = grouper['FITC_A_8'].sum().to_frame(name='sum_FITC_A_8').reset_index()\n",
        "FITC_A_9_sum = grouper['FITC_A_9'].sum().to_frame(name='sum_FITC_A_9').reset_index()\n",
        "FITC_A_10_sum = grouper['FITC_A_10'].sum().to_frame(name='sum_FITC_A_10').reset_index()\n",
        "FITC_A_11_sum = grouper['FITC_A_11'].sum().to_frame(name='sum_FITC_A_11').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_1_intensity_sum = grouper['2.4_1_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_intensity_sum = grouper['2.4_2_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_intensity_sum = grouper['2.4_3_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_intensity_sum = grouper['2.4_4_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_intensity_sum = grouper['2.4_5_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_intensity_sum = grouper['2.4_6_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_intensity_sum = grouper['2.4_7_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_intensity_sum = grouper['2.4_8_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_intensity_sum = grouper['2.4_9_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_intensity_sum = grouper['2.4_10_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_intensity_sum = grouper['2.4_11_intensity'].sum().to_frame(name='intensity_sum_PE_Texas_Red_A_11').reset_index()\n",
        "PE_Texas_Red_A_1_sum = grouper['PE_Texas_Red_A_1'].sum().to_frame(name='sum_PE_Texas_Red_A_1').reset_index()\n",
        "PE_Texas_Red_A_2_sum = grouper['PE_Texas_Red_A_2'].sum().to_frame(name='sum_PE_Texas_Red_A_2').reset_index()\n",
        "PE_Texas_Red_A_3_sum = grouper['PE_Texas_Red_A_3'].sum().to_frame(name='sum_PE_Texas_Red_A_3').reset_index()\n",
        "PE_Texas_Red_A_4_sum = grouper['PE_Texas_Red_A_4'].sum().to_frame(name='sum_PE_Texas_Red_A_4').reset_index()\n",
        "PE_Texas_Red_A_5_sum = grouper['PE_Texas_Red_A_5'].sum().to_frame(name='sum_PE_Texas_Red_A_5').reset_index()\n",
        "PE_Texas_Red_A_6_sum = grouper['PE_Texas_Red_A_6'].sum().to_frame(name='sum_PE_Texas_Red_A_6').reset_index()\n",
        "PE_Texas_Red_A_7_sum = grouper['PE_Texas_Red_A_7'].sum().to_frame(name='sum_PE_Texas_Red_A_7').reset_index()\n",
        "PE_Texas_Red_A_8_sum = grouper['PE_Texas_Red_A_8'].sum().to_frame(name='sum_PE_Texas_Red_A_8').reset_index()\n",
        "PE_Texas_Red_A_9_sum = grouper['PE_Texas_Red_A_9'].sum().to_frame(name='sum_PE_Texas_Red_A_9').reset_index()\n",
        "PE_Texas_Red_A_10_sum = grouper['PE_Texas_Red_A_10'].sum().to_frame(name='sum_PE_Texas_Red_A_10').reset_index()\n",
        "PE_Texas_Red_A_11_sum = grouper['PE_Texas_Red_A_11'].sum().to_frame(name='sum_PE_Texas_Red_A_11').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_1_intensity_sum = grouper['3.36_1_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_intensity_sum = grouper['3.36_2_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_intensity_sum = grouper['3.36_3_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_intensity_sum = grouper['3.36_4_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_intensity_sum = grouper['3.36_5_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_intensity_sum = grouper['3.36_6_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_intensity_sum = grouper['3.36_7_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_intensity_sum = grouper['3.36_8_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_intensity_sum = grouper['3.36_9_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_intensity_sum = grouper['3.36_10_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_intensity_sum = grouper['3.36_11_intensity'].sum().to_frame(name='intensity_sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "Alexa_Fluor_700_A_1_sum = grouper['Alexa_Fluor_700_A_1'].sum().to_frame(name='sum_Alexa_Fluor_700_A_1').reset_index()\n",
        "Alexa_Fluor_700_A_2_sum = grouper['Alexa_Fluor_700_A_2'].sum().to_frame(name='sum_Alexa_Fluor_700_A_2').reset_index()\n",
        "Alexa_Fluor_700_A_3_sum = grouper['Alexa_Fluor_700_A_3'].sum().to_frame(name='sum_Alexa_Fluor_700_A_3').reset_index()\n",
        "Alexa_Fluor_700_A_4_sum = grouper['Alexa_Fluor_700_A_4'].sum().to_frame(name='sum_Alexa_Fluor_700_A_4').reset_index()\n",
        "Alexa_Fluor_700_A_5_sum = grouper['Alexa_Fluor_700_A_5'].sum().to_frame(name='sum_Alexa_Fluor_700_A_5').reset_index()\n",
        "Alexa_Fluor_700_A_6_sum = grouper['Alexa_Fluor_700_A_6'].sum().to_frame(name='sum_Alexa_Fluor_700_A_6').reset_index()\n",
        "Alexa_Fluor_700_A_7_sum = grouper['Alexa_Fluor_700_A_7'].sum().to_frame(name='sum_Alexa_Fluor_700_A_7').reset_index()\n",
        "Alexa_Fluor_700_A_8_sum = grouper['Alexa_Fluor_700_A_8'].sum().to_frame(name='sum_Alexa_Fluor_700_A_8').reset_index()\n",
        "Alexa_Fluor_700_A_9_sum = grouper['Alexa_Fluor_700_A_9'].sum().to_frame(name='sum_Alexa_Fluor_700_A_9').reset_index()\n",
        "Alexa_Fluor_700_A_10_sum = grouper['Alexa_Fluor_700_A_10'].sum().to_frame(name='sum_Alexa_Fluor_700_A_10').reset_index()\n",
        "Alexa_Fluor_700_A_11_sum = grouper['Alexa_Fluor_700_A_11'].sum().to_frame(name='sum_Alexa_Fluor_700_A_11').reset_index()\n",
        "\n",
        "PerCP_A_1_intensity_sum = grouper['0.04_1_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_intensity_sum = grouper['0.04_2_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_intensity_sum = grouper['0.04_3_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_intensity_sum = grouper['0.04_4_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_intensity_sum = grouper['0.04_5_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_intensity_sum = grouper['0.04_6_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_intensity_sum = grouper['0.04_7_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_intensity_sum = grouper['0.04_8_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_intensity_sum = grouper['0.04_9_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_intensity_sum = grouper['0.04_10_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_intensity_sum = grouper['0.04_11_intensity'].sum().to_frame(name='intensity_sum_PerCP_A_11').reset_index()\n",
        "PerCP_A_1_sum = grouper['PerCP_A_1'].sum().to_frame(name='sum_PerCP_A_1').reset_index()\n",
        "PerCP_A_2_sum = grouper['PerCP_A_2'].sum().to_frame(name='sum_PerCP_A_2').reset_index()\n",
        "PerCP_A_3_sum = grouper['PerCP_A_3'].sum().to_frame(name='sum_PerCP_A_3').reset_index()\n",
        "PerCP_A_4_sum = grouper['PerCP_A_4'].sum().to_frame(name='sum_PerCP_A_4').reset_index()\n",
        "PerCP_A_5_sum = grouper['PerCP_A_5'].sum().to_frame(name='sum_PerCP_A_5').reset_index()\n",
        "PerCP_A_6_sum = grouper['PerCP_A_6'].sum().to_frame(name='sum_PerCP_A_6').reset_index()\n",
        "PerCP_A_7_sum = grouper['PerCP_A_7'].sum().to_frame(name='sum_PerCP_A_7').reset_index()\n",
        "PerCP_A_8_sum = grouper['PerCP_A_8'].sum().to_frame(name='sum_PerCP_A_8').reset_index()\n",
        "PerCP_A_9_sum = grouper['PerCP_A_9'].sum().to_frame(name='sum_PerCP_A_9').reset_index()\n",
        "PerCP_A_10_sum = grouper['PerCP_A_10'].sum().to_frame(name='sum_PerCP_A_10').reset_index()\n",
        "PerCP_A_11_sum = grouper['PerCP_A_11'].sum().to_frame(name='sum_PerCP_A_11').reset_index()\n",
        "Cell_line = grouper['Cell_line'].mean().to_frame(name='Cell_line').reset_index()\n",
        "\n",
        "mean_FSC_A = grouper['FSC_A'].mean().to_frame(name='FSC_A_mean').reset_index()\n",
        "mean_FSC_H = grouper['FSC_H'].mean().to_frame(name='FSC_H_mean').reset_index()\n",
        "mean_FSC_W = grouper['FSC_W'].mean().to_frame(name='FSC_W_mean').reset_index()\n",
        "mean_SSC_A = grouper['SSC_A'].mean().to_frame(name='SSC_A_mean').reset_index()\n",
        "mean_SSC_H = grouper['SSC_H'].mean().to_frame(name='SSC_H_mean').reset_index()\n",
        "mean_SSC_W = grouper['SSC_W'].mean().to_frame(name='SSC_W_mean').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTKmrdR8iTbI"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_2_test_log_grouper = [BV421_A_1_intensity_sum, BV421_A_2_intensity_sum, BV421_A_3_intensity_sum, BV421_A_4_intensity_sum, BV421_A_5_intensity_sum,\n",
        "         BV421_A_6_intensity_sum, BV421_A_7_intensity_sum, BV421_A_8_intensity_sum, BV421_A_9_intensity_sum, BV421_A_10_intensity_sum, BV421_A_11_intensity_sum,\n",
        "         BV421_A_1_sum, BV421_A_2_sum, BV421_A_3_sum, BV421_A_4_sum, BV421_A_5_sum, BV421_A_6_sum, BV421_A_7_sum, BV421_A_8_sum, BV421_A_9_sum, BV421_A_10_sum, BV421_A_11_sum,\n",
        "         FITC_A_1_intensity_sum, FITC_A_2_intensity_sum, FITC_A_3_intensity_sum, FITC_A_4_intensity_sum, FITC_A_5_intensity_sum, FITC_A_6_intensity_sum, FITC_A_7_intensity_sum,\n",
        "         FITC_A_8_intensity_sum, FITC_A_9_intensity_sum, FITC_A_10_intensity_sum, FITC_A_11_intensity_sum, FITC_A_1_sum, FITC_A_2_sum, FITC_A_3_sum, FITC_A_4_sum, FITC_A_5_sum,\n",
        "         FITC_A_6_sum, FITC_A_7_sum, FITC_A_8_sum, FITC_A_9_sum, FITC_A_10_sum, FITC_A_11_sum,PE_Texas_Red_A_1_intensity_sum, PE_Texas_Red_A_2_intensity_sum, PE_Texas_Red_A_3_intensity_sum,\n",
        "         PE_Texas_Red_A_4_intensity_sum, PE_Texas_Red_A_5_intensity_sum, PE_Texas_Red_A_6_intensity_sum, PE_Texas_Red_A_7_intensity_sum, PE_Texas_Red_A_8_intensity_sum, PE_Texas_Red_A_9_intensity_sum,\n",
        "         PE_Texas_Red_A_10_intensity_sum, PE_Texas_Red_A_11_intensity_sum, PE_Texas_Red_A_1_sum, PE_Texas_Red_A_2_sum, PE_Texas_Red_A_3_sum, PE_Texas_Red_A_4_sum, PE_Texas_Red_A_5_sum,\n",
        "         PE_Texas_Red_A_6_sum, PE_Texas_Red_A_7_sum, PE_Texas_Red_A_8_sum, PE_Texas_Red_A_9_sum, PE_Texas_Red_A_10_sum, PE_Texas_Red_A_11_sum,\n",
        "         Alexa_Fluor_700_A_1_intensity_sum, Alexa_Fluor_700_A_2_intensity_sum, Alexa_Fluor_700_A_3_intensity_sum, Alexa_Fluor_700_A_4_intensity_sum, Alexa_Fluor_700_A_5_intensity_sum,\n",
        "         Alexa_Fluor_700_A_6_intensity_sum, Alexa_Fluor_700_A_7_intensity_sum, Alexa_Fluor_700_A_8_intensity_sum, Alexa_Fluor_700_A_9_intensity_sum, Alexa_Fluor_700_A_10_intensity_sum, Alexa_Fluor_700_A_11_intensity_sum,\n",
        "         Alexa_Fluor_700_A_1_sum, Alexa_Fluor_700_A_2_sum, Alexa_Fluor_700_A_3_sum, Alexa_Fluor_700_A_4_sum, Alexa_Fluor_700_A_5_sum, Alexa_Fluor_700_A_6_sum, Alexa_Fluor_700_A_7_sum, Alexa_Fluor_700_A_8_sum,\n",
        "         Alexa_Fluor_700_A_9_sum, Alexa_Fluor_700_A_10_sum, Alexa_Fluor_700_A_11_sum, PerCP_A_1_intensity_sum, PerCP_A_2_intensity_sum, PerCP_A_3_intensity_sum, PerCP_A_4_intensity_sum, PerCP_A_5_intensity_sum,\n",
        "         PerCP_A_6_intensity_sum, PerCP_A_7_intensity_sum, PerCP_A_8_intensity_sum, PerCP_A_9_intensity_sum, PerCP_A_10_intensity_sum, PerCP_A_11_intensity_sum,\n",
        "         PerCP_A_1_sum, PerCP_A_2_sum, PerCP_A_3_sum, PerCP_A_4_sum, PerCP_A_5_sum, PerCP_A_6_sum, PerCP_A_7_sum, PerCP_A_8_sum, PerCP_A_9_sum, PerCP_A_10_sum, PerCP_A_11_sum,\n",
        "         mean_FSC_A, mean_FSC_H, mean_FSC_W, mean_SSC_A, mean_SSC_H, mean_SSC_W, Cell_line]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEVZ3RiPiTbI"
      },
      "outputs": [],
      "source": [
        "# building a new dfs of the grouped data\n",
        "pop_2_test_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_2_test_log_grouper)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalizing the uptake intencities per bin"
      ],
      "metadata": {
        "id": "mXdDVl-uhNjw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXP2V8poiTbJ"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['normalized_intensity_BV421_A_1', 'normalized_intensity_BV421_A_2', 'normalized_intensity_BV421_A_3', 'normalized_intensity_BV421_A_4'\n",
        "                  , 'normalized_intensity_BV421_A_5', 'normalized_intensity_BV421_A_6', 'normalized_intensity_BV421_A_7', 'normalized_intensity_BV421_A_8'\n",
        "                  , 'normalized_intensity_BV421_A_9', 'normalized_intensity_BV421_A_10', 'normalized_intensity_BV421_A_11']] = pop_2_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_BV421_A_1, x.intensity_sum_BV421_A_2, x.intensity_sum_BV421_A_3, x.intensity_sum_BV421_A_4, x.intensity_sum_BV421_A_5,\n",
        "                  x.intensity_sum_BV421_A_6, x.intensity_sum_BV421_A_7, x.intensity_sum_BV421_A_8, x.intensity_sum_BV421_A_9, x.intensity_sum_BV421_A_10, x.intensity_sum_BV421_A_11,\n",
        "                  x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBiqGi5UiTbJ"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['normalized_intensity_FITC_A_1', 'normalized_intensity_FITC_A_2', 'normalized_intensity_FITC_A_3', 'normalized_intensity_FITC_A_4',\n",
        "                          'normalized_intensity_FITC_A_5', 'normalized_intensity_FITC_A_6', 'normalized_intensity_FITC_A_7', 'normalized_intensity_FITC_A_8',\n",
        "                          'normalized_intensity_FITC_A_9', 'normalized_intensity_FITC_A_10', 'normalized_intensity_FITC_A_11']] = pop_2_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_FITC_A_1, x.intensity_sum_FITC_A_2, x.intensity_sum_FITC_A_3, x.intensity_sum_FITC_A_4, x.intensity_sum_FITC_A_5,\n",
        "                          x.intensity_sum_FITC_A_6, x.intensity_sum_FITC_A_7, x.intensity_sum_FITC_A_8, x.intensity_sum_FITC_A_9, x.intensity_sum_FITC_A_10, x.intensity_sum_FITC_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HRe4UiViTbJ"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'normalized_intensity_PE_Texas_Red_A_2', 'normalized_intensity_PE_Texas_Red_A_3', 'normalized_intensity_PE_Texas_Red_A_4',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_5', 'normalized_intensity_PE_Texas_Red_A_6', 'normalized_intensity_PE_Texas_Red_A_7', 'normalized_intensity_PE_Texas_Red_A_8',\n",
        "                          'normalized_intensity_PE_Texas_Red_A_9','normalized_intensity_PE_Texas_Red_A_10','normalized_intensity_PE_Texas_Red_A_11']] = pop_2_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PE_Texas_Red_A_1, x.intensity_sum_PE_Texas_Red_A_2, x.intensity_sum_PE_Texas_Red_A_3, x.intensity_sum_PE_Texas_Red_A_4, x.intensity_sum_PE_Texas_Red_A_5,\n",
        "                          x.intensity_sum_PE_Texas_Red_A_6, x.intensity_sum_PE_Texas_Red_A_7, x.intensity_sum_PE_Texas_Red_A_8, x.intensity_sum_PE_Texas_Red_A_9,\n",
        "                          x.intensity_sum_PE_Texas_Red_A_10, x.intensity_sum_PE_Texas_Red_A_11, x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C8RoYK5iTbJ"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'normalized_intensity_Alexa_Fluor_700_A_2', 'normalized_intensity_Alexa_Fluor_700_A_3', 'normalized_intensity_Alexa_Fluor_700_A_4',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_5', 'normalized_intensity_Alexa_Fluor_700_A_6', 'normalized_intensity_Alexa_Fluor_700_A_7', 'normalized_intensity_Alexa_Fluor_700_A_8',\n",
        "                          'normalized_intensity_Alexa_Fluor_700_A_9', 'normalized_intensity_Alexa_Fluor_700_A_10', 'normalized_intensity_Alexa_Fluor_700_A_11']] = pop_2_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_Alexa_Fluor_700_A_1, x.intensity_sum_Alexa_Fluor_700_A_2, x.intensity_sum_Alexa_Fluor_700_A_3, x.intensity_sum_Alexa_Fluor_700_A_4, x.intensity_sum_Alexa_Fluor_700_A_5,\n",
        "                          x.intensity_sum_Alexa_Fluor_700_A_6, x.intensity_sum_Alexa_Fluor_700_A_7, x.intensity_sum_Alexa_Fluor_700_A_8, x.intensity_sum_Alexa_Fluor_700_A_9, x.intensity_sum_Alexa_Fluor_700_A_10, x.intensity_sum_Alexa_Fluor_700_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SChhVP3kiTbJ"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['normalized_intensity_PerCP_A_1', 'normalized_intensity_PerCP_A_2', 'normalized_intensity_PerCP_A_3', 'normalized_intensity_PerCP_A_4',\n",
        "                          'normalized_intensity_PerCP_A_5', 'normalized_intensity_PerCP_A_6', 'normalized_intensity_PerCP_A_7', 'normalized_intensity_PerCP_A_8',\n",
        "                          'normalized_intensity_PerCP_A_9', 'normalized_intensity_PerCP_A_10', 'normalized_intensity_PerCP_A_11']] = pop_2_test_log_grouped.apply(lambda x: normalized_intensity(x.intensity_sum_PerCP_A_1, x.intensity_sum_PerCP_A_2, x.intensity_sum_PerCP_A_3, x.intensity_sum_PerCP_A_4, x.intensity_sum_PerCP_A_5,\n",
        "                          x.intensity_sum_PerCP_A_6, x.intensity_sum_PerCP_A_7, x.intensity_sum_PerCP_A_8, x.intensity_sum_PerCP_A_9, x.intensity_sum_PerCP_A_10, x.intensity_sum_PerCP_A_11,\n",
        "                          x.FSC_A_mean), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating the mean uptake intensities per bin"
      ],
      "metadata": {
        "id": "cgXlZSavhUr5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRtMq2A1iTbJ"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['mean_intensity_0.5_1', 'mean_intensity_0.5_2', 'mean_intensity_0.5_3', 'mean_intensity_0.5_4',\n",
        "                          'mean_intensity_0.5_5', 'mean_intensity_0.5_6', 'mean_intensity_0.5_7', 'mean_intensity_0.5_8',\n",
        "                          'mean_intensity_0.5_9', 'mean_intensity_0.5_10', 'mean_intensity_0.5_11']] = pop_2_test_log_grouped.apply(lambda x: average(x.normalized_intensity_BV421_A_1, x.normalized_intensity_BV421_A_2, x.normalized_intensity_BV421_A_3, x.normalized_intensity_BV421_A_4, x.normalized_intensity_BV421_A_5,\n",
        "                          x.normalized_intensity_BV421_A_6, x.normalized_intensity_BV421_A_7, x.normalized_intensity_BV421_A_8, x.normalized_intensity_BV421_A_9, x.normalized_intensity_BV421_A_10, x.normalized_intensity_BV421_A_11,\n",
        "                          x.sum_BV421_A_1, x.sum_BV421_A_2, x.sum_BV421_A_3, x.sum_BV421_A_4, x.sum_BV421_A_5, x.sum_BV421_A_6, x.sum_BV421_A_7, x.sum_BV421_A_8,x.sum_BV421_A_9, x.sum_BV421_A_10, x.sum_BV421_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXIr7BhyiTbJ"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['mean_intensity_0.8_1', 'mean_intensity_0.8_2', 'mean_intensity_0.8_3', 'mean_intensity_0.8_4',\n",
        "                          'mean_intensity_0.8_5', 'mean_intensity_0.8_6', 'mean_intensity_0.8_7', 'mean_intensity_0.8_8',\n",
        "                          'mean_intensity_0.8_9', 'mean_intensity_0.8_10', 'mean_intensity_0.8_11']] = pop_2_test_log_grouped.apply(lambda x: average(x.normalized_intensity_FITC_A_1, x.normalized_intensity_FITC_A_2, x.normalized_intensity_FITC_A_3, x.normalized_intensity_FITC_A_4, x.normalized_intensity_FITC_A_5,\n",
        "                          x.normalized_intensity_FITC_A_6, x.normalized_intensity_FITC_A_7, x.normalized_intensity_FITC_A_8, x.normalized_intensity_FITC_A_9, x.normalized_intensity_FITC_A_10, x.normalized_intensity_FITC_A_11,\n",
        "                          x.sum_FITC_A_1, x.sum_FITC_A_2, x.sum_FITC_A_3, x.sum_FITC_A_4, x.sum_FITC_A_5, x.sum_FITC_A_6, x.sum_FITC_A_7, x.sum_FITC_A_8, x.sum_FITC_A_9, x.sum_FITC_A_10, x.sum_FITC_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nHy-6TciTbK"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['mean_intensity_2.4_1', 'mean_intensity_2.4_2', 'mean_intensity_2.4_3', 'mean_intensity_2.4_4',\n",
        "                          'mean_intensity_2.4_5', 'mean_intensity_2.4_6', 'mean_intensity_2.4_7', 'mean_intensity_2.4_8',\n",
        "                          'mean_intensity_2.4_9', 'mean_intensity_2.4_10', 'mean_intensity_2.4_11']] = pop_2_test_log_grouped.apply(lambda x: average(x.normalized_intensity_PE_Texas_Red_A_1, x.normalized_intensity_PE_Texas_Red_A_2, x.normalized_intensity_PE_Texas_Red_A_3, x.normalized_intensity_PE_Texas_Red_A_4, x.normalized_intensity_PE_Texas_Red_A_5,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_6, x.normalized_intensity_PE_Texas_Red_A_7, x.normalized_intensity_PE_Texas_Red_A_8,\n",
        "                          x.normalized_intensity_PE_Texas_Red_A_9, x.normalized_intensity_PE_Texas_Red_A_10, x.normalized_intensity_PE_Texas_Red_A_11,\n",
        "                          x.sum_PE_Texas_Red_A_1, x.sum_PE_Texas_Red_A_2, x.sum_PE_Texas_Red_A_3, x.sum_PE_Texas_Red_A_4, x.sum_PE_Texas_Red_A_5,\n",
        "                          x.sum_PE_Texas_Red_A_6, x.sum_PE_Texas_Red_A_7, x.sum_PE_Texas_Red_A_8, x.sum_PE_Texas_Red_A_9, x.sum_PE_Texas_Red_A_10, x.sum_PE_Texas_Red_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxBluFvHiTbK"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['mean_intensity_3.36_1', 'mean_intensity_3.36_2', 'mean_intensity_3.36_3', 'mean_intensity_3.36_4',\n",
        "                          'mean_intensity_3.36_5', 'mean_intensity_3.36_6', 'mean_intensity_3.36_7', 'mean_intensity_3.36_8',\n",
        "                          'mean_intensity_3.36_9', 'mean_intensity_3.36_10', 'mean_intensity_3.36_11']] = pop_2_test_log_grouped.apply(lambda x: average(x.normalized_intensity_Alexa_Fluor_700_A_1, x.normalized_intensity_Alexa_Fluor_700_A_2, x.normalized_intensity_Alexa_Fluor_700_A_3, x.normalized_intensity_Alexa_Fluor_700_A_4, x.normalized_intensity_Alexa_Fluor_700_A_5,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_6, x.normalized_intensity_Alexa_Fluor_700_A_7, x.normalized_intensity_Alexa_Fluor_700_A_8,\n",
        "                          x.normalized_intensity_Alexa_Fluor_700_A_9, x.normalized_intensity_Alexa_Fluor_700_A_10, x.normalized_intensity_Alexa_Fluor_700_A_11,\n",
        "                          x.sum_Alexa_Fluor_700_A_1, x.sum_Alexa_Fluor_700_A_2, x.sum_Alexa_Fluor_700_A_3, x.sum_Alexa_Fluor_700_A_4, x.sum_Alexa_Fluor_700_A_5,\n",
        "                          x.sum_Alexa_Fluor_700_A_6, x.sum_Alexa_Fluor_700_A_7, x.sum_Alexa_Fluor_700_A_8, x.sum_Alexa_Fluor_700_A_9, x.sum_Alexa_Fluor_700_A_10, x.sum_Alexa_Fluor_700_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaeiLW1YiTbK"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped[['mean_intensity_0.04_1', 'mean_intensity_0.04_2', 'mean_intensity_0.04_3', 'mean_intensity_0.04_4',\n",
        "                          'mean_intensity_0.04_5', 'mean_intensity_0.04_6', 'mean_intensity_0.04_7', 'mean_intensity_0.04_8',\n",
        "                          'mean_intensity_0.04_9', 'mean_intensity_0.04_10', 'mean_intensity_0.04_11']] = pop_2_test_log_grouped.apply(lambda x: average(x.normalized_intensity_PerCP_A_1, x.normalized_intensity_PerCP_A_2, x.normalized_intensity_PerCP_A_3, x.normalized_intensity_PerCP_A_4, x.normalized_intensity_PerCP_A_5,\n",
        "                          x.normalized_intensity_PerCP_A_6, x.normalized_intensity_PerCP_A_7, x.normalized_intensity_PerCP_A_8,\n",
        "                          x.normalized_intensity_PerCP_A_9, x.normalized_intensity_PerCP_A_10, x.normalized_intensity_PerCP_A_11,\n",
        "                          x.sum_PerCP_A_1, x.sum_PerCP_A_2, x.sum_PerCP_A_3, x.sum_PerCP_A_4, x.sum_PerCP_A_5,\n",
        "                          x.sum_PerCP_A_6, x.sum_PerCP_A_7, x.sum_PerCP_A_8,x.sum_PerCP_A_9, x.sum_PerCP_A_10, x.sum_PerCP_A_11), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifl8kPUPiTbK"
      },
      "outputs": [],
      "source": [
        "# buliding dfs for median calculations\n",
        "df_median_1_1 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_1', 'population_number']]\n",
        "df_median_1_2 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_2', 'population_number']]\n",
        "df_median_1_3 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_3', 'population_number']]\n",
        "df_median_1_4 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_4', 'population_number']]\n",
        "df_median_1_5 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_5', 'population_number']]\n",
        "df_median_1_6 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_6', 'population_number']]\n",
        "df_median_1_7 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_7', 'population_number']]\n",
        "df_median_1_8 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_8', 'population_number']]\n",
        "df_median_1_9 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_9', 'population_number']]\n",
        "df_median_1_10 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_10', 'population_number']]\n",
        "df_median_1_11 = pop_1_test_log_grouped[['normalized_intensity_BV421_A_11', 'population_number']]\n",
        "\n",
        "df_median_2_1 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_1', 'population_number']]\n",
        "df_median_2_2 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_2', 'population_number']]\n",
        "df_median_2_3 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_3', 'population_number']]\n",
        "df_median_2_4 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_4', 'population_number']]\n",
        "df_median_2_5 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_5', 'population_number']]\n",
        "df_median_2_6 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_6', 'population_number']]\n",
        "df_median_2_7 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_7', 'population_number']]\n",
        "df_median_2_8 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_8', 'population_number']]\n",
        "df_median_2_9 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_9', 'population_number']]\n",
        "df_median_2_10 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_10', 'population_number']]\n",
        "df_median_2_11 = pop_1_test_log_grouped[['normalized_intensity_FITC_A_11', 'population_number']]\n",
        "\n",
        "df_median_3_1 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_1', 'population_number']]\n",
        "df_median_3_2 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_2', 'population_number']]\n",
        "df_median_3_3 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_3', 'population_number']]\n",
        "df_median_3_4 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_4', 'population_number']]\n",
        "df_median_3_5 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_5', 'population_number']]\n",
        "df_median_3_6 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_6', 'population_number']]\n",
        "df_median_3_7 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_7', 'population_number']]\n",
        "df_median_3_8 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_8', 'population_number']]\n",
        "df_median_3_9 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_9', 'population_number']]\n",
        "df_median_3_10 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_10', 'population_number']]\n",
        "df_median_3_11 = pop_1_test_log_grouped[['normalized_intensity_PE_Texas_Red_A_11', 'population_number']]\n",
        "\n",
        "df_median_4_1 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_1', 'population_number']]\n",
        "df_median_4_2 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_2', 'population_number']]\n",
        "df_median_4_3 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_3', 'population_number']]\n",
        "df_median_4_4 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_4', 'population_number']]\n",
        "df_median_4_5 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_5', 'population_number']]\n",
        "df_median_4_6 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_6', 'population_number']]\n",
        "df_median_4_7 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_7', 'population_number']]\n",
        "df_median_4_8 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_8', 'population_number']]\n",
        "df_median_4_9 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_9', 'population_number']]\n",
        "df_median_4_10 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_10', 'population_number']]\n",
        "df_median_4_11 = pop_1_test_log_grouped[['normalized_intensity_Alexa_Fluor_700_A_11', 'population_number']]\n",
        "\n",
        "df_median_5_1 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_1', 'population_number']]\n",
        "df_median_5_2 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_2', 'population_number']]\n",
        "df_median_5_3 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_3', 'population_number']]\n",
        "df_median_5_4 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_4', 'population_number']]\n",
        "df_median_5_5 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_5', 'population_number']]\n",
        "df_median_5_6 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_6', 'population_number']]\n",
        "df_median_5_7 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_7', 'population_number']]\n",
        "df_median_5_8 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_8', 'population_number']]\n",
        "df_median_5_9 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_9', 'population_number']]\n",
        "df_median_5_10 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_10', 'population_number']]\n",
        "df_median_5_11 = pop_1_test_log_grouped[['normalized_intensity_PerCP_A_11', 'population_number']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU_JfYgAiTbL"
      },
      "outputs": [],
      "source": [
        "# filtering out zeroes\n",
        "df_median_1_1 = df_median_1_1[df_median_1_1['normalized_intensity_BV421_A_1'] > 0]\n",
        "df_median_1_2 = df_median_1_2[df_median_1_2['normalized_intensity_BV421_A_2'] > 0]\n",
        "df_median_1_3 = df_median_1_3[df_median_1_3['normalized_intensity_BV421_A_3'] > 0]\n",
        "df_median_1_4 = df_median_1_4[df_median_1_4['normalized_intensity_BV421_A_4'] > 0]\n",
        "df_median_1_5 = df_median_1_5[df_median_1_5['normalized_intensity_BV421_A_5'] > 0]\n",
        "df_median_1_6 = df_median_1_6[df_median_1_6['normalized_intensity_BV421_A_6'] > 0]\n",
        "df_median_1_7 = df_median_1_7[df_median_1_7['normalized_intensity_BV421_A_7'] > 0]\n",
        "df_median_1_8 = df_median_1_8[df_median_1_8['normalized_intensity_BV421_A_8'] > 0]\n",
        "df_median_1_9 = df_median_1_9[df_median_1_9['normalized_intensity_BV421_A_9'] > 0]\n",
        "df_median_1_10 = df_median_1_10[df_median_1_10['normalized_intensity_BV421_A_10'] > 0]\n",
        "df_median_1_11 = df_median_1_11[df_median_1_11['normalized_intensity_BV421_A_11'] > 0]\n",
        "\n",
        "df_median_2_1 = df_median_2_1[df_median_2_1['normalized_intensity_FITC_A_1'] > 0]\n",
        "df_median_2_2 = df_median_2_2[df_median_2_2['normalized_intensity_FITC_A_2'] > 0]\n",
        "df_median_2_3 = df_median_2_3[df_median_2_3['normalized_intensity_FITC_A_3'] > 0]\n",
        "df_median_2_4 = df_median_2_4[df_median_2_4['normalized_intensity_FITC_A_4'] > 0]\n",
        "df_median_2_5 = df_median_2_5[df_median_2_5['normalized_intensity_FITC_A_5'] > 0]\n",
        "df_median_2_6 = df_median_2_6[df_median_2_6['normalized_intensity_FITC_A_6'] > 0]\n",
        "df_median_2_7 = df_median_2_7[df_median_2_7['normalized_intensity_FITC_A_7'] > 0]\n",
        "df_median_2_8 = df_median_2_8[df_median_2_8['normalized_intensity_FITC_A_8'] > 0]\n",
        "df_median_2_9 = df_median_2_9[df_median_2_9['normalized_intensity_FITC_A_9'] > 0]\n",
        "df_median_2_10 = df_median_2_10[df_median_2_10['normalized_intensity_FITC_A_10'] > 0]\n",
        "df_median_2_11 = df_median_2_11[df_median_2_11['normalized_intensity_FITC_A_11'] > 0]\n",
        "\n",
        "df_median_3_1 = df_median_3_1[df_median_3_1['normalized_intensity_PE_Texas_Red_A_1'] > 0]\n",
        "df_median_3_2 = df_median_3_2[df_median_3_2['normalized_intensity_PE_Texas_Red_A_2'] > 0]\n",
        "df_median_3_3 = df_median_3_3[df_median_3_3['normalized_intensity_PE_Texas_Red_A_3'] > 0]\n",
        "df_median_3_4 = df_median_3_4[df_median_3_4['normalized_intensity_PE_Texas_Red_A_4'] > 0]\n",
        "df_median_3_5 = df_median_3_5[df_median_3_5['normalized_intensity_PE_Texas_Red_A_5'] > 0]\n",
        "df_median_3_6 = df_median_3_6[df_median_3_6['normalized_intensity_PE_Texas_Red_A_6'] > 0]\n",
        "df_median_3_7 = df_median_3_7[df_median_3_7['normalized_intensity_PE_Texas_Red_A_7'] > 0]\n",
        "df_median_3_8 = df_median_3_8[df_median_3_8['normalized_intensity_PE_Texas_Red_A_8'] > 0]\n",
        "df_median_3_9 = df_median_3_9[df_median_3_9['normalized_intensity_PE_Texas_Red_A_9'] > 0]\n",
        "df_median_3_10 = df_median_3_10[df_median_3_10['normalized_intensity_PE_Texas_Red_A_10'] > 0]\n",
        "df_median_3_11 = df_median_3_11[df_median_3_11['normalized_intensity_PE_Texas_Red_A_11'] > 0]\n",
        "\n",
        "df_median_4_1 = df_median_4_1[df_median_4_1['normalized_intensity_Alexa_Fluor_700_A_1'] > 0]\n",
        "df_median_4_2 = df_median_4_2[df_median_4_2['normalized_intensity_Alexa_Fluor_700_A_2'] > 0]\n",
        "df_median_4_3 = df_median_4_3[df_median_4_3['normalized_intensity_Alexa_Fluor_700_A_3'] > 0]\n",
        "df_median_4_4 = df_median_4_4[df_median_4_4['normalized_intensity_Alexa_Fluor_700_A_4'] > 0]\n",
        "df_median_4_5 = df_median_4_5[df_median_4_5['normalized_intensity_Alexa_Fluor_700_A_5'] > 0]\n",
        "df_median_4_6 = df_median_4_6[df_median_4_6['normalized_intensity_Alexa_Fluor_700_A_6'] > 0]\n",
        "df_median_4_7 = df_median_4_7[df_median_4_7['normalized_intensity_Alexa_Fluor_700_A_7'] > 0]\n",
        "df_median_4_8 = df_median_4_8[df_median_4_8['normalized_intensity_Alexa_Fluor_700_A_8'] > 0]\n",
        "df_median_4_9 = df_median_4_9[df_median_4_9['normalized_intensity_Alexa_Fluor_700_A_9'] > 0]\n",
        "df_median_4_10 = df_median_4_10[df_median_4_10['normalized_intensity_Alexa_Fluor_700_A_10'] > 0]\n",
        "df_median_4_11 = df_median_4_11[df_median_4_11['normalized_intensity_Alexa_Fluor_700_A_11'] > 0]\n",
        "\n",
        "df_median_5_1 = df_median_5_1[df_median_5_1['normalized_intensity_PerCP_A_1'] > 0]\n",
        "df_median_5_2 = df_median_5_2[df_median_5_2['normalized_intensity_PerCP_A_2'] > 0]\n",
        "df_median_5_3 = df_median_5_3[df_median_5_3['normalized_intensity_PerCP_A_3'] > 0]\n",
        "df_median_5_4 = df_median_5_4[df_median_5_4['normalized_intensity_PerCP_A_4'] > 0]\n",
        "df_median_5_5 = df_median_5_5[df_median_5_5['normalized_intensity_PerCP_A_5'] > 0]\n",
        "df_median_5_6 = df_median_5_6[df_median_5_6['normalized_intensity_PerCP_A_6'] > 0]\n",
        "df_median_5_7 = df_median_5_7[df_median_5_7['normalized_intensity_PerCP_A_7'] > 0]\n",
        "df_median_5_8 = df_median_5_8[df_median_5_8['normalized_intensity_PerCP_A_8'] > 0]\n",
        "df_median_5_9 = df_median_5_9[df_median_5_9['normalized_intensity_PerCP_A_9'] > 0]\n",
        "df_median_5_10 = df_median_5_10[df_median_5_10['normalized_intensity_PerCP_A_10'] > 0]\n",
        "df_median_5_11 = df_median_5_11[df_median_5_11['normalized_intensity_PerCP_A_11'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOCXC82wiTbL"
      },
      "outputs": [],
      "source": [
        "# grouping and calculating the median per positive (>0) values per bin\n",
        "df_subset_1_1 = df_median_1_1.loc[:, ['population_number', 'normalized_intensity_BV421_A_1'] ]\n",
        "df_median_1_1_grouped = df_median_1_1.groupby('population_number').median()\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.rename(columns={\"normalized_intensity_BV421_A_1\": \"0.5_1_median_intensity\"})\n",
        "df_median_1_1_grouped = df_median_1_1_grouped.reset_index()\n",
        "df_subset_1_2 = df_median_1_2.loc[:, ['population_number', 'normalized_intensity_BV421_A_2'] ]\n",
        "df_median_1_2_grouped = df_median_1_2.groupby('population_number').median()\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.rename(columns={\"normalized_intensity_BV421_A_2\": \"0.5_2_median_intensity\"})\n",
        "df_median_1_2_grouped = df_median_1_2_grouped.reset_index()\n",
        "df_subset_1_3 = df_median_1_3.loc[:, ['population_number', 'normalized_intensity_BV421_A_3'] ]\n",
        "df_median_1_3_grouped = df_median_1_3.groupby('population_number').median()\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.rename(columns={\"normalized_intensity_BV421_A_3\": \"0.5_3_median_intensity\"})\n",
        "df_median_1_3_grouped = df_median_1_3_grouped.reset_index()\n",
        "df_subset_1_4 = df_median_1_4.loc[:, ['population_number', 'normalized_intensity_BV421_A_4'] ]\n",
        "df_median_1_4_grouped = df_median_1_4.groupby('population_number').median()\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.rename(columns={\"normalized_intensity_BV421_A_4\": \"0.5_4_median_intensity\"})\n",
        "df_median_1_4_grouped = df_median_1_4_grouped.reset_index()\n",
        "df_subset_1_5 = df_median_1_5.loc[:, ['population_number', 'normalized_intensity_BV421_A_5'] ]\n",
        "df_median_1_5_grouped = df_median_1_5.groupby('population_number').median()\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.rename(columns={\"normalized_intensity_BV421_A_5\": \"0.5_5_median_intensity\"})\n",
        "df_median_1_5_grouped = df_median_1_5_grouped.reset_index()\n",
        "df_subset_1_6 = df_median_1_6.loc[:, ['population_number', 'normalized_intensity_BV421_A_6'] ]\n",
        "df_median_1_6_grouped = df_median_1_6.groupby('population_number').median()\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.rename(columns={\"normalized_intensity_BV421_A_6\": \"0.5_6_median_intensity\"})\n",
        "df_median_1_6_grouped = df_median_1_6_grouped.reset_index()\n",
        "df_subset_1_7 = df_median_1_7.loc[:, ['population_number', 'normalized_intensity_BV421_A_7'] ]\n",
        "df_median_1_7_grouped = df_median_1_7.groupby('population_number').median()\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.rename(columns={\"normalized_intensity_BV421_A_7\": \"0.5_7_median_intensity\"})\n",
        "df_median_1_7_grouped = df_median_1_7_grouped.reset_index()\n",
        "df_subset_1_8 = df_median_1_8.loc[:, ['population_number', 'normalized_intensity_BV421_A_8'] ]\n",
        "df_median_1_8_grouped = df_median_1_8.groupby('population_number').median()\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.rename(columns={\"normalized_intensity_BV421_A_8\": \"0.5_8_median_intensity\"})\n",
        "df_median_1_8_grouped = df_median_1_8_grouped.reset_index()\n",
        "df_subset_1_9 = df_median_1_9.loc[:, ['population_number', 'normalized_intensity_BV421_A_9'] ]\n",
        "df_median_1_9_grouped = df_median_1_9.groupby('population_number').median()\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.rename(columns={\"normalized_intensity_BV421_A_9\": \"0.5_9_median_intensity\"})\n",
        "df_median_1_9_grouped = df_median_1_9_grouped.reset_index()\n",
        "df_subset_1_10 = df_median_1_10.loc[:, ['population_number', 'normalized_intensity_BV421_A_10'] ]\n",
        "df_median_1_10_grouped = df_median_1_10.groupby('population_number').median()\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.rename(columns={\"normalized_intensity_BV421_A_10\": \"0.5_10_median_intensity\"})\n",
        "df_median_1_10_grouped = df_median_1_10_grouped.reset_index()\n",
        "df_subset_1_11 = df_median_1_11.loc[:, ['population_number', 'normalized_intensity_BV421_A_11'] ]\n",
        "df_median_1_11_grouped = df_median_1_11.groupby('population_number').median()\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.rename(columns={\"normalized_intensity_BV421_A_11\": \"0.5_11_median_intensity\"})\n",
        "df_median_1_11_grouped = df_median_1_11_grouped.reset_index()\n",
        "\n",
        "df_subset_2_1 = df_median_2_1.loc[:, ['population_number', 'normalized_intensity_FITC_A_1'] ]\n",
        "df_median_2_1_grouped = df_median_2_1.groupby('population_number').median()\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.rename(columns={\"normalized_intensity_FITC_A_1\": \"0.8_1_median_intensity\"})\n",
        "df_median_2_1_grouped = df_median_2_1_grouped.reset_index()\n",
        "df_subset_2_2 = df_median_2_2.loc[:, ['population_number', 'normalized_intensity_FITC_A_2'] ]\n",
        "df_median_2_2_grouped = df_median_2_2.groupby('population_number').median()\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.rename(columns={\"normalized_intensity_FITC_A_2\": \"0.8_2_median_intensity\"})\n",
        "df_median_2_2_grouped = df_median_2_2_grouped.reset_index()\n",
        "df_subset_2_3 = df_median_2_3.loc[:, ['population_number', 'normalized_intensity_FITC_A_3'] ]\n",
        "df_median_2_3_grouped = df_median_2_3.groupby('population_number').median()\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.rename(columns= {\"normalized_intensity_FITC_A_3\": \"0.8_3_median_intensity\"})\n",
        "df_median_2_3_grouped = df_median_2_3_grouped.reset_index()\n",
        "df_subset_2_4 = df_median_2_4.loc[:, ['population_number', 'normalized_intensity_FITC_A_4'] ]\n",
        "df_median_2_4_grouped = df_median_2_4.groupby('population_number').median()\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.rename(columns={\"normalized_intensity_FITC_A_4\": \"0.8_4_median_intensity\"})\n",
        "df_median_2_4_grouped = df_median_2_4_grouped.reset_index()\n",
        "df_subset_2_5 = df_median_2_5.loc[:, ['population_number', 'normalized_intensity_FITC_A_5'] ]\n",
        "df_median_2_5_grouped = df_median_2_5.groupby('population_number').median()\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.rename(columns={\"normalized_intensity_FITC_A_5\": \"0.8_5_median_intensity\"})\n",
        "df_median_2_5_grouped = df_median_2_5_grouped.reset_index()\n",
        "df_subset_2_6 = df_median_2_6.loc[:, ['population_number', 'normalized_intensity_FITC_A_6'] ]\n",
        "df_median_2_6_grouped = df_median_2_6.groupby('population_number').median()\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.rename(columns={\"normalized_intensity_FITC_A_6\": \"0.8_6_median_intensity\"})\n",
        "df_median_2_6_grouped = df_median_2_6_grouped.reset_index()\n",
        "df_subset_2_7 = df_median_2_7.loc[:, ['population_number', 'normalized_intensity_FITC_A_7'] ]\n",
        "df_median_2_7_grouped = df_median_2_7.groupby('population_number').median()\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.rename(columns={\"normalized_intensity_FITC_A_7\": \"0.8_7_median_intensity\"})\n",
        "df_median_2_7_grouped = df_median_2_7_grouped.reset_index()\n",
        "df_subset_2_8 = df_median_2_8.loc[:, ['population_number', 'normalized_intensity_FITC_A_8'] ]\n",
        "df_median_2_8_grouped = df_median_2_8.groupby('population_number').median()\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.rename(columns={\"normalized_intensity_FITC_A_8\": \"0.8_8_median_intensity\"})\n",
        "df_median_2_8_grouped = df_median_2_8_grouped.reset_index()\n",
        "df_subset_2_9 = df_median_2_9.loc[:, ['population_number', 'normalized_intensity_FITC_A_9'] ]\n",
        "df_median_2_9_grouped = df_median_2_9.groupby('population_number').median()\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.rename(columns={\"normalized_intensity_FITC_A_9\": \"0.8_9_median_intensity\"})\n",
        "df_median_2_9_grouped = df_median_2_9_grouped.reset_index()\n",
        "df_subset_2_10 = df_median_2_10.loc[:, ['population_number', 'normalized_intensity_FITC_A_10'] ]\n",
        "df_median_2_10_grouped = df_median_2_10.groupby('population_number').median()\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.rename(columns={\"normalized_intensity_FITC_A_10\": \"0.8_10_median_intensity\"})\n",
        "df_median_2_10_grouped = df_median_2_10_grouped.reset_index()\n",
        "df_subset_2_11 = df_median_2_11.loc[:, ['population_number', 'normalized_intensity_FITC_A_11'] ]\n",
        "df_median_2_11_grouped = df_median_2_11.groupby('population_number').median()\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.rename(columns={\"normalized_intensity_FITC_A_11\": \"0.8_11_median_intensity\"})\n",
        "df_median_2_11_grouped = df_median_2_11_grouped.reset_index()\n",
        "\n",
        "df_subset_3_1 = df_median_3_1.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_1'] ]\n",
        "df_median_3_1_grouped = df_median_3_1.groupby('population_number').median()\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_1\": \"2.4_1_median_intensity\"})\n",
        "df_median_3_1_grouped = df_median_3_1_grouped.reset_index()\n",
        "df_subset_3_2 = df_median_3_2.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_2'] ]\n",
        "df_median_3_2_grouped = df_median_3_2.groupby('population_number').median()\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_2\": \"2.4_2_median_intensity\"})\n",
        "df_median_3_2_grouped = df_median_3_2_grouped.reset_index()\n",
        "df_subset_3_3 = df_median_3_3.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_3'] ]\n",
        "df_median_3_3_grouped = df_median_3_3.groupby('population_number').median()\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.rename(columns= {\"normalized_intensity_PE_Texas_Red_A_3\": \"2.4_3_median_intensity\"})\n",
        "df_median_3_3_grouped = df_median_3_3_grouped.reset_index()\n",
        "df_subset_3_4 = df_median_3_4.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_4'] ]\n",
        "df_median_3_4_grouped = df_median_3_4.groupby('population_number').median()\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_4\": \"2.4_4_median_intensity\"})\n",
        "df_median_3_4_grouped = df_median_3_4_grouped.reset_index()\n",
        "df_subset_3_5 = df_median_3_5.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_5'] ]\n",
        "df_median_3_5_grouped = df_median_3_5.groupby('population_number').median()\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_5\": \"2.4_5_median_intensity\"})\n",
        "df_median_3_5_grouped = df_median_3_5_grouped.reset_index()\n",
        "df_subset_3_6 = df_median_3_6.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_6'] ]\n",
        "df_median_3_6_grouped = df_median_3_6.groupby('population_number').median()\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_6\": \"2.4_6_median_intensity\"})\n",
        "df_median_3_6_grouped = df_median_3_6_grouped.reset_index()\n",
        "df_subset_3_7 = df_median_3_7.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_7'] ]\n",
        "df_median_3_7_grouped = df_median_3_7.groupby('population_number').median()\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_7\": \"2.4_7_median_intensity\"})\n",
        "df_median_3_7_grouped = df_median_3_7_grouped.reset_index()\n",
        "df_subset_3_8 = df_median_3_8.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_8'] ]\n",
        "df_median_3_8_grouped = df_median_3_8.groupby('population_number').median()\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_8\": \"2.4_8_median_intensity\"})\n",
        "df_median_3_8_grouped = df_median_3_8_grouped.reset_index()\n",
        "df_subset_3_9 = df_median_3_9.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_9'] ]\n",
        "df_median_3_9_grouped = df_median_3_9.groupby('population_number').median()\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_9\": \"2.4_9_median_intensity\"})\n",
        "df_median_3_9_grouped = df_median_3_9_grouped.reset_index()\n",
        "df_subset_3_10 = df_median_3_10.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_10'] ]\n",
        "df_median_3_10_grouped = df_median_3_10.groupby('population_number').median()\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_10\": \"2.4_10_median_intensity\"})\n",
        "df_median_3_10_grouped = df_median_3_10_grouped.reset_index()\n",
        "df_subset_3_11 = df_median_3_11.loc[:, ['population_number', 'normalized_intensity_PE_Texas_Red_A_11'] ]\n",
        "df_median_3_11_grouped = df_median_3_11.groupby('population_number').median()\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.rename(columns={\"normalized_intensity_PE_Texas_Red_A_11\": \"2.4_11_median_intensity\"})\n",
        "df_median_3_11_grouped = df_median_3_11_grouped.reset_index()\n",
        "\n",
        "df_subset_4_1 = df_median_4_1.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_1'] ]\n",
        "df_median_4_1_grouped = df_median_4_1.groupby('population_number').median()\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_1\": \"3.36_1_median_intensity\"})\n",
        "df_median_4_1_grouped = df_median_4_1_grouped.reset_index()\n",
        "df_subset_4_2 = df_median_4_2.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_2'] ]\n",
        "df_median_4_2_grouped = df_median_4_2.groupby('population_number').median()\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_2\": \"3.36_2_median_intensity\"})\n",
        "df_median_4_2_grouped = df_median_4_2_grouped.reset_index()\n",
        "df_subset_4_3 = df_median_4_3.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_3'] ]\n",
        "df_median_4_3_grouped = df_median_4_3.groupby('population_number').median()\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.rename(columns= {\"normalized_intensity_Alexa_Fluor_700_A_3\": \"3.36_3_median_intensity\"})\n",
        "df_median_4_3_grouped = df_median_4_3_grouped.reset_index()\n",
        "df_subset_4_4 = df_median_4_4.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_4'] ]\n",
        "df_median_4_4_grouped = df_median_4_4.groupby('population_number').median()\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_4\": \"3.36_4_median_intensity\"})\n",
        "df_median_4_4_grouped = df_median_4_4_grouped.reset_index()\n",
        "df_subset_4_5 = df_median_4_5.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_5'] ]\n",
        "df_median_4_5_grouped = df_median_4_5.groupby('population_number').median()\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_5\": \"3.36_5_median_intensity\"})\n",
        "df_median_4_5_grouped = df_median_4_5_grouped.reset_index()\n",
        "df_subset_4_6 = df_median_4_6.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_6'] ]\n",
        "df_median_4_6_grouped = df_median_4_6.groupby('population_number').median()\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_6\": \"3.36_6_median_intensity\"})\n",
        "df_median_4_6_grouped = df_median_4_6_grouped.reset_index()\n",
        "df_subset_4_7 = df_median_4_7.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_7'] ]\n",
        "df_median_4_7_grouped = df_median_4_7.groupby('population_number').median()\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_7\": \"3.36_7_median_intensity\"})\n",
        "df_median_4_7_grouped = df_median_4_7_grouped.reset_index()\n",
        "df_subset_4_8 = df_median_4_8.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_8'] ]\n",
        "df_median_4_8_grouped = df_median_4_8.groupby('population_number').median()\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_8\": \"3.36_8_median_intensity\"})\n",
        "df_median_4_8_grouped = df_median_4_8_grouped.reset_index()\n",
        "df_subset_4_9 = df_median_4_9.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_9'] ]\n",
        "df_median_4_9_grouped = df_median_4_9.groupby('population_number').median()\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_9\": \"3.36_9_median_intensity\"})\n",
        "df_median_4_9_grouped = df_median_4_9_grouped.reset_index()\n",
        "df_subset_4_10 = df_median_4_10.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_10'] ]\n",
        "df_median_4_10_grouped = df_median_4_10.groupby('population_number').median()\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_10\": \"3.36_10_median_intensity\"})\n",
        "df_median_4_10_grouped = df_median_4_10_grouped.reset_index()\n",
        "df_subset_4_11 = df_median_4_11.loc[:, ['population_number', 'normalized_intensity_Alexa_Fluor_700_A_11'] ]\n",
        "df_median_4_11_grouped = df_median_4_11.groupby('population_number').median()\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.rename(columns={\"normalized_intensity_Alexa_Fluor_700_A_11\": \"3.36_11_median_intensity\"})\n",
        "df_median_4_11_grouped = df_median_4_11_grouped.reset_index()\n",
        "\n",
        "df_subset_5_1 = df_median_5_1.loc[:, ['population_number', 'normalized_intensity_PerCP_A_1']]\n",
        "df_median_5_1_grouped = df_median_5_1.groupby('population_number').median()\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.rename(columns={\"normalized_intensity_PerCP_A_1\": \"0.04_1_median_intensity\"})\n",
        "df_median_5_1_grouped = df_median_5_1_grouped.reset_index()\n",
        "df_subset_5_2 = df_median_5_2.loc[:, ['population_number', 'normalized_intensity_PerCP_A_2'] ]\n",
        "df_median_5_2_grouped = df_median_5_2.groupby('population_number').median()\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.rename(columns={\"normalized_intensity_PerCP_A_2\": \"0.04_2_median_intensity\"})\n",
        "df_median_5_2_grouped = df_median_5_2_grouped.reset_index()\n",
        "df_subset_5_3 = df_median_5_3.loc[:, ['population_number', 'normalized_intensity_PerCP_A_3'] ]\n",
        "df_median_5_3_grouped = df_median_5_3.groupby('population_number').median()\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.rename(columns= {\"normalized_intensity_PerCP_A_3\": \"0.04_3_median_intensity\"})\n",
        "df_median_5_3_grouped = df_median_5_3_grouped.reset_index()\n",
        "df_subset_5_4 = df_median_5_4.loc[:, ['population_number', 'normalized_intensity_PerCP_A_4'] ]\n",
        "df_median_5_4_grouped = df_median_5_4.groupby('population_number').median()\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.rename(columns={\"normalized_intensity_PerCP_A_4\": \"0.04_4_median_intensity\"})\n",
        "df_median_5_4_grouped = df_median_5_4_grouped.reset_index()\n",
        "df_subset_5_5 = df_median_5_5.loc[:, ['population_number', 'normalized_intensity_PerCP_A_5'] ]\n",
        "df_median_5_5_grouped = df_median_5_5.groupby('population_number').median()\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.rename(columns={\"normalized_intensity_PerCP_A_5\": \"0.04_5_median_intensity\"})\n",
        "df_median_5_5_grouped = df_median_5_5_grouped.reset_index()\n",
        "df_subset_5_6 = df_median_5_6.loc[:, ['population_number', 'normalized_intensity_PerCP_A_6'] ]\n",
        "df_median_5_6_grouped = df_median_5_6.groupby('population_number').median()\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.rename(columns={\"normalized_intensity_PerCP_A_6\": \"0.04_6_median_intensity\"})\n",
        "df_median_5_6_grouped = df_median_5_6_grouped.reset_index()\n",
        "df_subset_5_7 = df_median_5_7.loc[:, ['population_number', 'normalized_intensity_PerCP_A_7'] ]\n",
        "df_median_5_7_grouped = df_median_5_7.groupby('population_number').median()\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.rename(columns={\"normalized_intensity_PerCP_A_7\": \"0.04_7_median_intensity\"})\n",
        "df_median_5_7_grouped = df_median_5_7_grouped.reset_index()\n",
        "df_subset_5_8 = df_median_5_8.loc[:, ['population_number', 'normalized_intensity_PerCP_A_8'] ]\n",
        "df_median_5_8_grouped = df_median_5_8.groupby('population_number').median()\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.rename(columns={\"normalized_intensity_PerCP_A_8\": \"0.04_8_median_intensity\"})\n",
        "df_median_5_8_grouped = df_median_5_8_grouped.reset_index()\n",
        "df_subset_5_9 = df_median_5_9.loc[:, ['population_number', 'normalized_intensity_PerCP_A_9'] ]\n",
        "df_median_5_9_grouped = df_median_5_9.groupby('population_number').median()\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.rename(columns={\"normalized_intensity_PerCP_A_9\": \"0.04_9_median_intensity\"})\n",
        "df_median_5_9_grouped = df_median_5_9_grouped.reset_index()\n",
        "df_subset_5_10 = df_median_5_10.loc[:, ['population_number', 'normalized_intensity_PerCP_A_10'] ]\n",
        "df_median_5_10_grouped = df_median_5_10.groupby('population_number').median()\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.rename(columns={\"normalized_intensity_PerCP_A_10\": \"0.04_10_median_intensity\"})\n",
        "df_median_5_10_grouped = df_median_5_10_grouped.reset_index()\n",
        "df_subset_5_11 = df_median_5_11.loc[:, ['population_number', 'normalized_intensity_PerCP_A_11'] ]\n",
        "df_median_5_11_grouped = df_median_5_11.groupby('population_number').median()\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.rename(columns={\"normalized_intensity_PerCP_A_11\": \"0.04_11_median_intensity\"})\n",
        "df_median_5_11_grouped = df_median_5_11_grouped.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4bJ9eIGiTbM"
      },
      "outputs": [],
      "source": [
        "# returning the zeros that were filtered out to the dfs\n",
        "pop_2_test_log_grouped[['0.5_1_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_1_grouped.population_number, df_median_1_1_grouped['0.5_1_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_2_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_2_grouped.population_number, df_median_1_2_grouped['0.5_2_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_3_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_3_grouped.population_number, df_median_1_3_grouped['0.5_3_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_4_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_4_grouped.population_number, df_median_1_4_grouped['0.5_4_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_5_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_5_grouped.population_number, df_median_1_5_grouped['0.5_5_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_6_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_6_grouped.population_number, df_median_1_6_grouped['0.5_6_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_7_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_7_grouped.population_number, df_median_1_7_grouped['0.5_7_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_8_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_8_grouped.population_number, df_median_1_8_grouped['0.5_8_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_9_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_9_grouped.population_number, df_median_1_9_grouped['0.5_9_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_10_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_10_grouped.population_number, df_median_1_10_grouped['0.5_10_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.5_11_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_1_11_grouped.population_number, df_median_1_11_grouped['0.5_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_test_log_grouped[['0.8_1_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_1_grouped.population_number, df_median_2_1_grouped['0.8_1_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_2_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_2_grouped.population_number, df_median_2_2_grouped['0.8_2_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_3_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_3_grouped.population_number, df_median_2_3_grouped['0.8_3_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_4_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_4_grouped.population_number, df_median_2_4_grouped['0.8_4_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_5_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_5_grouped.population_number, df_median_2_5_grouped['0.8_5_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_6_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_6_grouped.population_number, df_median_2_6_grouped['0.8_6_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_7_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_7_grouped.population_number, df_median_2_7_grouped['0.8_7_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_8_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_8_grouped.population_number, df_median_2_8_grouped['0.8_8_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_9_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_9_grouped.population_number, df_median_2_9_grouped['0.8_9_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_10_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_10_grouped.population_number, df_median_2_10_grouped['0.8_10_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.8_11_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_2_11_grouped.population_number, df_median_2_11_grouped['0.8_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_test_log_grouped[['2.4_1_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_1_grouped.population_number, df_median_3_1_grouped['2.4_1_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_2_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_2_grouped.population_number, df_median_3_2_grouped['2.4_2_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_3_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_3_grouped.population_number, df_median_3_3_grouped['2.4_3_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_4_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_4_grouped.population_number, df_median_3_4_grouped['2.4_4_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_5_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_5_grouped.population_number, df_median_3_5_grouped['2.4_5_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_6_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_6_grouped.population_number, df_median_3_6_grouped['2.4_6_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_7_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_7_grouped.population_number, df_median_3_7_grouped['2.4_7_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_8_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_8_grouped.population_number, df_median_3_8_grouped['2.4_8_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_9_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_9_grouped.population_number, df_median_3_9_grouped['2.4_9_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_10_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_10_grouped.population_number, df_median_3_10_grouped['2.4_10_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['2.4_11_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_3_11_grouped.population_number, df_median_3_11_grouped['2.4_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_test_log_grouped[['3.36_1_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_1_grouped.population_number, df_median_4_1_grouped['3.36_1_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_2_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_2_grouped.population_number, df_median_4_2_grouped['3.36_2_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_3_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_3_grouped.population_number, df_median_4_3_grouped['3.36_3_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_4_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_4_grouped.population_number, df_median_4_4_grouped['3.36_4_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_5_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_5_grouped.population_number, df_median_4_5_grouped['3.36_5_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_6_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_6_grouped.population_number, df_median_4_6_grouped['3.36_6_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_7_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_7_grouped.population_number, df_median_4_7_grouped['3.36_7_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_8_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_8_grouped.population_number, df_median_4_8_grouped['3.36_8_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_9_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_9_grouped.population_number, df_median_4_9_grouped['3.36_9_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_10_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_10_grouped.population_number, df_median_4_10_grouped['3.36_10_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['3.36_11_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_4_11_grouped.population_number, df_median_4_11_grouped['3.36_11_median_intensity']), axis = 1)\n",
        "\n",
        "pop_2_test_log_grouped[['0.04_1_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_1_grouped.population_number, df_median_5_1_grouped['0.04_1_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_2_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_2_grouped.population_number, df_median_5_2_grouped['0.04_2_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_3_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_3_grouped.population_number, df_median_5_3_grouped['0.04_3_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_4_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_4_grouped.population_number, df_median_5_4_grouped['0.04_4_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_5_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_5_grouped.population_number, df_median_5_5_grouped['0.04_5_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_6_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_6_grouped.population_number, df_median_5_6_grouped['0.04_6_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_7_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_7_grouped.population_number, df_median_5_7_grouped['0.04_7_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_8_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_8_grouped.population_number, df_median_5_8_grouped['0.04_8_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_9_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_9_grouped.population_number, df_median_5_9_grouped['0.04_9_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_10_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_10_grouped.population_number, df_median_5_10_grouped['0.04_10_median_intensity']), axis = 1)\n",
        "pop_2_test_log_grouped[['0.04_11_median_intensity']] = pop_2_test_log_grouped.apply(lambda x: fix_median(x.population_number, df_median_5_11_grouped.population_number, df_median_5_11_grouped['0.04_11_median_intensity']), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSw62d7DiTbN"
      },
      "outputs": [],
      "source": [
        "pop_2_test_log_grouped.to_csv('H460_cis_res_test_log_grouped_filtered_final.csv') # name the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah7rP4mjkoPF"
      },
      "outputs": [],
      "source": [
        "# filling missing values with zeros\n",
        "pop_1_test_log_grouped = pop_1_test_log_grouped.fillna(0)\n",
        "pop_2_test_log_grouped = pop_2_test_log_grouped.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merging the testing datasets of the two populations\n",
        "populations_df_test_list = [pop_1_test_log_grouped, pop_2_test_log_grouped]\n",
        "Merge_population_test = pd.DataFrame()\n",
        "for df in populations_df_test_list:\n",
        "    Merge_population_test = pd.concat([Merge_population_test, df], ignore_index=True)\n",
        "\n",
        "Merge_population_test.to_csv('Merge_population_H460_test_filtered.csv')"
      ],
      "metadata": {
        "id": "4oEzETSniOiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'Merge_population_H460_test_filtered' # name the data\n",
        "pickle.dump(Merge_population_test, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "wHV0bWz8jcLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opening the saved datasets\n",
        "\n",
        "# test\n",
        "model_name = 'Merge_population_H460_test_filtered' # name the data\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "Merge_population_test = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# test\n",
        "model_name = 'Merge_population_H460_train_filtered' # name the data\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "Merge_population = pickle.load(a_file)\n",
        "a_file.close()\n"
      ],
      "metadata": {
        "id": "b-mgwi6BQSEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9Ve9TVT61BX"
      },
      "source": [
        "## Uptake analysis - without bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEXoE2q161BX"
      },
      "outputs": [],
      "source": [
        "# training data\n",
        "Merge_population_uptake = Merge_population.drop(columns=['intensity_sum_BV421_A_1', 'intensity_sum_BV421_A_2', 'intensity_sum_BV421_A_3', 'intensity_sum_BV421_A_4',\n",
        "                      'intensity_sum_BV421_A_5', 'intensity_sum_BV421_A_6', 'intensity_sum_BV421_A_7', 'intensity_sum_BV421_A_8', 'intensity_sum_BV421_A_9', 'intensity_sum_BV421_A_10', 'intensity_sum_BV421_A_11',\n",
        "                      'intensity_sum_FITC_A_1', 'intensity_sum_FITC_A_2', 'intensity_sum_FITC_A_3', 'intensity_sum_FITC_A_4',\n",
        "                      'intensity_sum_FITC_A_5', 'intensity_sum_FITC_A_6', 'intensity_sum_FITC_A_7', 'intensity_sum_FITC_A_8', 'intensity_sum_FITC_A_8','intensity_sum_FITC_A_9', 'intensity_sum_FITC_A_10', 'intensity_sum_FITC_A_11',\n",
        "                      'intensity_sum_PE_Texas_Red_A_1', 'intensity_sum_PE_Texas_Red_A_2', 'intensity_sum_PE_Texas_Red_A_3', 'intensity_sum_PE_Texas_Red_A_4',\n",
        "                      'intensity_sum_PE_Texas_Red_A_5', 'intensity_sum_PE_Texas_Red_A_6', 'intensity_sum_PE_Texas_Red_A_7', 'intensity_sum_PE_Texas_Red_A_8', 'intensity_sum_PE_Texas_Red_A_9', 'intensity_sum_PE_Texas_Red_A_10', 'intensity_sum_PE_Texas_Red_A_11',\n",
        "                      'intensity_sum_Alexa_Fluor_700_A_1', 'intensity_sum_Alexa_Fluor_700_A_2', 'intensity_sum_Alexa_Fluor_700_A_3', 'intensity_sum_Alexa_Fluor_700_A_4',\n",
        "                      'intensity_sum_Alexa_Fluor_700_A_5', 'intensity_sum_Alexa_Fluor_700_A_6', 'intensity_sum_Alexa_Fluor_700_A_7', 'intensity_sum_Alexa_Fluor_700_A_8', 'intensity_sum_Alexa_Fluor_700_A_9', 'intensity_sum_Alexa_Fluor_700_A_10', 'intensity_sum_Alexa_Fluor_700_A_11',\n",
        "                      'intensity_sum_PerCP_A_1', 'intensity_sum_PerCP_A_2', 'intensity_sum_PerCP_A_3', 'intensity_sum_PerCP_A_4',\n",
        "                      'intensity_sum_PerCP_A_5', 'intensity_sum_PerCP_A_6', 'intensity_sum_PerCP_A_7', 'intensity_sum_PerCP_A_8', 'intensity_sum_PerCP_A_9', 'intensity_sum_PerCP_A_10', 'intensity_sum_PerCP_A_11','population_number',\n",
        "                      'FSC_A_mean', 'FSC_H_mean', 'FSC_W_mean', 'SSC_A_mean', 'SSC_H_mean', 'SSC_W_mean', 'population_number']\n",
        "                       , axis = 1)\n",
        "\n",
        "Merge_population_uptake = Merge_population_uptake.fillna(0) # filling missing values with zeros\n",
        "Merge_population_uptake.to_csv('Merge_population_H460_filtered.csv') #saving the clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D10SJLYlkvS"
      },
      "outputs": [],
      "source": [
        "# testing data\n",
        "Merge_population_test_uptake = Merge_population_test.drop(columns=['intensity_sum_BV421_A_1', 'intensity_sum_BV421_A_2', 'intensity_sum_BV421_A_3', 'intensity_sum_BV421_A_4',\n",
        "                      'intensity_sum_BV421_A_5', 'intensity_sum_BV421_A_6', 'intensity_sum_BV421_A_7', 'intensity_sum_BV421_A_8', 'intensity_sum_BV421_A_9', 'intensity_sum_BV421_A_10', 'intensity_sum_BV421_A_11',\n",
        "                      'intensity_sum_FITC_A_1', 'intensity_sum_FITC_A_2', 'intensity_sum_FITC_A_3', 'intensity_sum_FITC_A_4',\n",
        "                      'intensity_sum_FITC_A_5', 'intensity_sum_FITC_A_6', 'intensity_sum_FITC_A_7', 'intensity_sum_FITC_A_8', 'intensity_sum_FITC_A_8','intensity_sum_FITC_A_9', 'intensity_sum_FITC_A_10', 'intensity_sum_FITC_A_11',\n",
        "                      'intensity_sum_PE_Texas_Red_A_1', 'intensity_sum_PE_Texas_Red_A_2', 'intensity_sum_PE_Texas_Red_A_3', 'intensity_sum_PE_Texas_Red_A_4',\n",
        "                      'intensity_sum_PE_Texas_Red_A_5', 'intensity_sum_PE_Texas_Red_A_6', 'intensity_sum_PE_Texas_Red_A_7', 'intensity_sum_PE_Texas_Red_A_8', 'intensity_sum_PE_Texas_Red_A_9', 'intensity_sum_PE_Texas_Red_A_10', 'intensity_sum_PE_Texas_Red_A_11',\n",
        "                      'intensity_sum_Alexa_Fluor_700_A_1', 'intensity_sum_Alexa_Fluor_700_A_2', 'intensity_sum_Alexa_Fluor_700_A_3', 'intensity_sum_Alexa_Fluor_700_A_4',\n",
        "                      'intensity_sum_Alexa_Fluor_700_A_5', 'intensity_sum_Alexa_Fluor_700_A_6', 'intensity_sum_Alexa_Fluor_700_A_7', 'intensity_sum_Alexa_Fluor_700_A_8', 'intensity_sum_Alexa_Fluor_700_A_9', 'intensity_sum_Alexa_Fluor_700_A_10', 'intensity_sum_Alexa_Fluor_700_A_11',\n",
        "                      'intensity_sum_PerCP_A_1', 'intensity_sum_PerCP_A_2', 'intensity_sum_PerCP_A_3', 'intensity_sum_PerCP_A_4',\n",
        "                      'intensity_sum_PerCP_A_5', 'intensity_sum_PerCP_A_6', 'intensity_sum_PerCP_A_7', 'intensity_sum_PerCP_A_8', 'intensity_sum_PerCP_A_9', 'intensity_sum_PerCP_A_10', 'intensity_sum_PerCP_A_11','population_number',\n",
        "                      'FSC_A_mean', 'FSC_H_mean', 'FSC_W_mean', 'SSC_A_mean', 'SSC_H_mean', 'SSC_W_mean', 'population_number']\n",
        "                       , axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvhXpfxs61BY"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the lables\n",
        "\n",
        "cols = [col for col in Merge_population_uptake.columns if col not in ['Cell_line']]\n",
        "population_data_uptake = Merge_population_uptake[cols] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_uptake = Merge_population_uptake['Cell_line']# labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if the data includes one sample only for each population\n",
        "\n",
        "## split to training and testing\n",
        "#X_train, X_test, y_train, y_test = train_test_split(All_data_train, target, test_size=0.3, random_state=42) # make sure to change names as needed"
      ],
      "metadata": {
        "id": "oYTYdEAkpn39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAfoD8NPliSx"
      },
      "outputs": [],
      "source": [
        "# separating the testing features from the lables\n",
        "cols_test = [col for col in Merge_population_test_uptake.columns if col not in ['Cell_line']]\n",
        "population_data_test_uptake = Merge_population_test_uptake[cols_test] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_test_uptake = Merge_population_test_uptake['Cell_line'] # labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWv_vLWM61BY"
      },
      "outputs": [],
      "source": [
        "# Using GroupKFold to prevent data leakage during cross validation analysis\n",
        "groups = Merge_population['group']\n",
        "cv = list(GroupKFold(n_splits=10).split(population_data_uptake, target_uptake, groups=groups))\n",
        "population_data_uptake.drop(['group'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "hoEOXoyvlJmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "e872Ca39lEra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_uptake = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [500, 1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_uptake,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(population_data_uptake, target_uptake)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "fLir0txSKBci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model with the best parameters results"
      ],
      "metadata": {
        "id": "aPaJCp8gl_-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7Ju_DD861BY"
      },
      "outputs": [],
      "source": [
        "# cross validation of the model on the training data\n",
        "random_forest_uptake = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators =500, n_jobs=-1)\n",
        "scores = cross_val_score(random_forest_uptake, population_data_uptake, target_uptake, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Croos validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1NrXQvslZTU"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_RF_grouped_uptake = random_forest_uptake.fit(population_data_uptake, target_uptake)\n",
        "print ('Random Forest training Score: {0:2.5f}'.format(my_model_RF_grouped_uptake.score(population_data_uptake, target_uptake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEarnpoC61BY"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_RF_grouped_uptake = my_model_RF_grouped_uptake.predict(population_data_test_uptake)\n",
        "heatconmat(target_test_uptake,pred_RF_grouped_uptake)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_uptake.score(population_data_test_uptake, target_test_uptake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmpnlGWm61BY"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm = PermutationImportance(my_model_RF_grouped_uptake).fit(population_data_uptake, target_uptake)\n",
        "eli5.show_weights(perm, feature_names = population_data_uptake.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_rf_uptake_500' # name the model\n",
        "pickle.dump(my_model_RF_grouped_uptake, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "5H8SM4VaSPIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "DjREDmFTnJkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "kCgJv54snL6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Sizrrm61BZ"
      },
      "outputs": [],
      "source": [
        "xgboost_classification = XGBClassifier(n_jobs=-1)\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(population_data_uptake,target_uptake)\n",
        "\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model with the best parameters results"
      ],
      "metadata": {
        "id": "xgvd9slCnYDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "xgboost_classification = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "scores = cross_val_score(xgboost_classification, population_data_uptake, target_uptake, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "bGPn-fQeWyds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVDOi1E261BZ"
      },
      "outputs": [],
      "source": [
        "# Fitting the model on all the training data\n",
        "my_model_xgboost_grouped_uptake = xgboost_classification.fit(population_data_uptake, target_uptake)\n",
        "print ('xgboost training Score: {0:2.5f}'.format(my_model_xgboost_grouped_uptake.score(population_data_uptake, target_uptake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAikigHy61BZ"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost = my_model_xgboost_grouped_uptake.predict(population_data_test_uptake)\n",
        "heatconmat(target_test_uptake,pred_xgboost)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_grouped_uptake.score(population_data_test_uptake, target_test_uptake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqwx4xdX61BZ"
      },
      "outputs": [],
      "source": [
        "# permutainon analysis to find features importance\n",
        "perm_xgboost = PermutationImportance(my_model_xgboost_grouped_uptake, random_state=1).fit(population_data_uptake, target_uptake)\n",
        "eli5.show_weights(perm_xgboost, feature_names = population_data_uptake.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_xgboost_uptake_500' # name the model\n",
        "pickle.dump(my_model_xgboost_grouped_uptake, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "kfIITVvUXx_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "rbZP3Ehdoe2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "htY8E649ogM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(population_data_uptake, target_uptake)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "k_kyv8PO6kzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model with the best parameters results"
      ],
      "metadata": {
        "id": "PzLxBhtGputa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "SVM_uptake = svm.SVC(kernel='linear', C = 0.001, gamma = 0.001)\n",
        "scores = cross_val_score(SVM_uptake, population_data_uptake, target_uptake, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "QI_kMidskiQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH8R33e-61Ba"
      },
      "outputs": [],
      "source": [
        "# fitting the data on all the training data\n",
        "my_model_SVM_grouped_uptake = SVM_uptake.fit(population_data_uptake, target_uptake)\n",
        "print ('SVM training Score: {0:2.5f}'.format(my_model_SVM_grouped_uptake.score(population_data_uptake, target_uptake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zWxCaI661Ba"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM = my_model_SVM_grouped_uptake.predict(population_data_test_uptake)\n",
        "heatconmat(target_test_uptake,pred_SVM)\n",
        "print ('SVM testing Score: {0:2.5f}'.format(my_model_SVM_grouped_uptake.score(population_data_test_uptake, target_test_uptake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNnkAZ361Ba"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_SVM = PermutationImportance(my_model_SVM_grouped_uptake, random_state=1).fit(population_data_test_uptake, target_test_uptake)\n",
        "eli5.show_weights(perm_SVM, feature_names = population_data_test_uptake.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_SVM_uptake_500' # name the model\n",
        "pickle.dump(my_model_SVM_grouped_uptake, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "rI8EJyL9X5qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrB9JUqQ61Ba"
      },
      "source": [
        "### new experiment testing - Batch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOK8eDTT61Bb"
      },
      "outputs": [],
      "source": [
        "# reading the batch data - for this example the batch data was already preprocessed\n",
        "Merge_population_batch_uptake = pd.read_csv(data_path +'Merge_population_H460_test_500.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmwh_xN-61Bb"
      },
      "outputs": [],
      "source": [
        "# separating the testing features from the lables\n",
        "cols_batch = [col for col in Merge_population_batch_uptake.columns if col not in ['Cell_line']]\n",
        "population_data_batch_uptake = Merge_population_batch_uptake[cols_batch]\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_batch = Merge_population_batch_uptake['Cell_line']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "NggCbr-Er7UE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKFKjNbb61Bb"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_random_forest_uptake_batch = my_model_RF_grouped_uptake.predict(population_data_batch_uptake)\n",
        "heatconmat(target_batch,pred_random_forest_uptake_batch)\n",
        "accuracy = accuracy_score(pred_random_forest_uptake_batch.astype(int), target_batch.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "7KpQwXnNr_LP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90CPl6Ri61Bc"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost_uptake_batch = my_model_xgboost_grouped_uptake.predict(population_data_batch_uptake)\n",
        "heatconmat(target_batch,pred_xgboost_uptake_batch)\n",
        "accuracy = accuracy_score(pred_xgboost_uptake_batch.astype(int), target_batch.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jtBCc6x61Bc"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_batch_uptake_SVM = my_model_SVM_grouped_uptake.predict(population_data_batch_uptake)\n",
        "heatconmat(target_batch,pred_batch_uptake_SVM)\n",
        "accuracy = accuracy_score(pred_batch_uptake_SVM.astype(int), target_batch.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimention reduction using principal component analysis PCA"
      ],
      "metadata": {
        "id": "-LUGVJrtsWze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bloYzeg661Bd"
      },
      "outputs": [],
      "source": [
        "# rescale the data\n",
        "train_data_rescaled = scaler.fit_transform(population_data_uptake)\n",
        "test_data_rescaled = scaler.fit_transform(population_data_test_uptake)\n",
        "batch_data_rescaled = scaler.fit_transform(population_data_batch_uptake)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1727-0Vn61Bd"
      },
      "outputs": [],
      "source": [
        "# displaying the componantes and the data varience they includes for choosing the number of componnets to reduced to\n",
        "cov_mat = np.cov(train_data_rescaled.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "# plot explained variances\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "plt.bar(range(1,221), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,221), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Juv5i5gx61Bd"
      },
      "outputs": [],
      "source": [
        "pca_uptake = PCA(n_components=38) # choosing the number of dimentions\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca_uptake = pca_uptake.fit_transform(train_data_rescaled)\n",
        "X_test_pca_uptake = pca_uptake.transform(test_data_rescaled)\n",
        "X_test_pca_batch = pca_uptake.transform(batch_data_rescaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYAmFwypS5LS"
      },
      "outputs": [],
      "source": [
        "# How much each original feature contribute to the components\n",
        "print(pd.DataFrame(np.abs(pca_uptake.components_),columns=population_data_uptake.columns ,index = ['PC-1', 'PC-2', 'PC-3', 'PC-4', 'PC-5', 'PC-6', 'PC-7', 'PC-8', 'PC-9', 'PC-10',\n",
        "                                                                                                   'PC-11', 'PC-12', 'PC-13', 'PC-14', 'PC-15', 'PC-16', 'PC-17', 'PC-18', 'PC-19', 'PC-20',\n",
        "                                                                                                   'PC-21', 'PC-22', 'PC-23', 'PC-24', 'PC-25', 'PC-26', 'PC-27', 'PC-28', 'PC-29', 'PC-30',\n",
        "                                                                                                   'PC-31', 'PC-32', 'PC-33', 'PC-34', 'PC-35', 'PC-36', 'PC-37', 'PC-38'])) # adjust the number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bBrwVhlTS2A"
      },
      "outputs": [],
      "source": [
        "# printing the most contributing original feature for each component\n",
        "most_important = np.abs(pca_uptake.components_).argmax(axis=1)\n",
        "initial_feature_names = population_data_uptake.columns\n",
        "most_important_names = initial_feature_names[most_important]\n",
        "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(38)} # adjust the number of components\n",
        "pca_results = pd.DataFrame(dic.items())\n",
        "print(pca_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pBibVQ3GtMOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forset"
      ],
      "metadata": {
        "id": "6HiyGIbJtNed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "2Ncr4E6ZtTzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_uptake_pca= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [500, 1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_uptake_pca,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(X_train_pca_uptake, target_uptake)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "SAit9I9WWJx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running and fitting the model using the best parameters results"
      ],
      "metadata": {
        "id": "VKAY1M3AtuYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_uptake_pca= RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 500, n_jobs=-1)\n",
        "my_model_RF_PCA_uptake = random_forest_grouped_uptake_pca.fit(X_train_pca_uptake, target_uptake)\n",
        "print ('random_forest testing Score: {0:2.5f}'.format(my_model_RF_PCA_uptake.score(X_test_pca_uptake, target_test_uptake)))"
      ],
      "metadata": {
        "id": "rgNgWrzJWJx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8pDZ2dSAuFg"
      },
      "outputs": [],
      "source": [
        "# prediction on Batch data\n",
        "pred_random_forest_uptake_PCA_batch = my_model_RF_PCA_uptake.predict(X_test_pca_batch)\n",
        "accuracy = accuracy_score(pred_random_forest_uptake_PCA_batch.astype(int), target_batch.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "9cQsSf5ruKio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "Z6PrYgF-uNb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgboost_classification_uptake_pca = XGBClassifier(n_jobs=-1)\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_uptake_pca,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(X_train_pca_uptake,target_uptake)\n",
        "print(model_gs.best_params_)"
      ],
      "metadata": {
        "id": "yFnnxiOBSfQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running and fitting the model using the best parameters results"
      ],
      "metadata": {
        "id": "dQxzqswgueTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgboost_classification_uptake_pca = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "my_model_xgboost_PCA_uptake = xgboost_classification_uptake_pca.fit(X_train_pca_uptake, target_uptake)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_PCA_uptake.score(X_test_pca_uptake, target_test_uptake)))"
      ],
      "metadata": {
        "id": "3yv5B-_rT8YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsbLk5XuAuM0"
      },
      "outputs": [],
      "source": [
        "# prediction on Batch data\n",
        "pred_xgboost_uptake_PCA_batch = my_model_xgboost_PCA_uptake.predict(X_test_pca_batch)\n",
        "accuracy = accuracy_score(pred_xgboost_uptake_PCA_batch.astype(int), target_batch.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "lICNrBSPvD2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "ovoTwDyIvF38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(X_train_pca_uptake, target_uptake)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "LaTxVpE9VQrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running and fitting the model using the best parameters results"
      ],
      "metadata": {
        "id": "fF30MAWxuuLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59_V2O1AVQrE"
      },
      "outputs": [],
      "source": [
        "SVM_uptake_pca = svm.SVC(C = 0.001, gamma = 0.001, kernel = 'linear')\n",
        "my_model_SVM_PCA_uptake = SVM_uptake_pca.fit(X_train_pca_uptake, target_uptake)\n",
        "print ('SVM testing Score: {0:2.5f}'.format(my_model_SVM_PCA_uptake.score(X_test_pca_uptake, target_test_uptake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjRo5E2oFp8L"
      },
      "outputs": [],
      "source": [
        "# prediction on Batch data\n",
        "pred_SVM_uptake_PCA_batch = my_model_SVM_PCA_uptake.predict(X_test_pca_batch)\n",
        "accuracy = accuracy_score(pred_SVM_uptake_PCA_batch.astype(int), target_batch.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmP6Ytk761Bd"
      },
      "source": [
        "## Physiological features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 1 training data"
      ],
      "metadata": {
        "id": "ya4N1Eeivqq4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3XsOj7U61Bd"
      },
      "outputs": [],
      "source": [
        "# deviding FSC and SSC intensities to bins\n",
        "Total_pop_1_log[['FSC_A_1', 'FSC_A_2', 'FSC_A_3', 'FSC_A_4', 'FSC_A_5', 'FSC_A_6', 'FSC_A_7']] = Total_pop_1_log['FSC_A'].apply(bins_phys)\n",
        "Total_pop_1_log[['FSC_H_1', 'FSC_H_2', 'FSC_H_3', 'FSC_H_4', 'FSC_H_5', 'FSC_H_6', 'FSC_H_7']] = Total_pop_1_log['FSC_H'].apply(bins_phys)\n",
        "Total_pop_1_log[['FSC_W_1', 'FSC_W_2', 'FSC_W_3', 'FSC_W_4', 'FSC_W_5', 'FSC_W_6', 'FSC_W_7']] = Total_pop_1_log['FSC_W'].apply(bins_phys)\n",
        "Total_pop_1_log[['SSC_A_1', 'SSC_A_2', 'SSC_A_3', 'SSC_A_4', 'SSC_A_5', 'SSC_A_6', 'SSC_A_7']] = Total_pop_1_log['SSC_A'].apply(bins_phys)\n",
        "Total_pop_1_log[['SSC_H_1', 'SSC_H_2', 'SSC_H_3', 'SSC_H_4', 'SSC_H_5', 'SSC_H_6', 'SSC_H_7']] = Total_pop_1_log['SSC_H'].apply(bins_phys)\n",
        "Total_pop_1_log[['SSC_W_1', 'SSC_W_2', 'SSC_W_3', 'SSC_W_4', 'SSC_W_5', 'SSC_W_6', 'SSC_W_7']] = Total_pop_1_log['SSC_W'].apply(bins_phys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYRqZYv561Be"
      },
      "outputs": [],
      "source": [
        "# grouping based on population number\n",
        "grouper = Total_pop_1_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_1_sum = grouper['FSC_A_1'].sum().to_frame(name='sum_FSC_A_1').reset_index()\n",
        "FSC_A_2_sum = grouper['FSC_A_2'].sum().to_frame(name='sum_FSC_A_2').reset_index()\n",
        "FSC_A_3_sum = grouper['FSC_A_3'].sum().to_frame(name='sum_FSC_A_3').reset_index()\n",
        "FSC_A_4_sum = grouper['FSC_A_4'].sum().to_frame(name='sum_FSC_A_4').reset_index()\n",
        "FSC_A_5_sum = grouper['FSC_A_5'].sum().to_frame(name='sum_FSC_A_5').reset_index()\n",
        "FSC_A_6_sum = grouper['FSC_A_6'].sum().to_frame(name='sum_FSC_A_6').reset_index()\n",
        "FSC_A_7_sum = grouper['FSC_A_7'].sum().to_frame(name='sum_FSC_A_7').reset_index()\n",
        "\n",
        "FSC_H_1_sum = grouper['FSC_H_1'].sum().to_frame(name='sum_FSC_H_1').reset_index()\n",
        "FSC_H_2_sum = grouper['FSC_H_2'].sum().to_frame(name='sum_FSC_H_2').reset_index()\n",
        "FSC_H_3_sum = grouper['FSC_H_3'].sum().to_frame(name='sum_FSC_H_3').reset_index()\n",
        "FSC_H_4_sum = grouper['FSC_H_4'].sum().to_frame(name='sum_FSC_H_4').reset_index()\n",
        "FSC_H_5_sum = grouper['FSC_H_5'].sum().to_frame(name='sum_FSC_H_5').reset_index()\n",
        "FSC_H_6_sum = grouper['FSC_H_6'].sum().to_frame(name='sum_FSC_H_6').reset_index()\n",
        "FSC_H_7_sum = grouper['FSC_H_7'].sum().to_frame(name='sum_FSC_H_7').reset_index()\n",
        "\n",
        "FSC_W_1_sum = grouper['FSC_W_1'].sum().to_frame(name='sum_FSC_W_1').reset_index()\n",
        "FSC_W_2_sum = grouper['FSC_W_2'].sum().to_frame(name='sum_FSC_W_2').reset_index()\n",
        "FSC_W_3_sum = grouper['FSC_W_3'].sum().to_frame(name='sum_FSC_W_3').reset_index()\n",
        "FSC_W_4_sum = grouper['FSC_W_4'].sum().to_frame(name='sum_FSC_W_4').reset_index()\n",
        "FSC_W_5_sum = grouper['FSC_W_5'].sum().to_frame(name='sum_FSC_W_5').reset_index()\n",
        "FSC_W_6_sum = grouper['FSC_W_6'].sum().to_frame(name='sum_FSC_W_6').reset_index()\n",
        "FSC_W_7_sum = grouper['FSC_W_7'].sum().to_frame(name='sum_FSC_W_7').reset_index()\n",
        "\n",
        "SSC_A_1_sum = grouper['SSC_A_1'].sum().to_frame(name='sum_SSC_A_1').reset_index()\n",
        "SSC_A_2_sum = grouper['SSC_A_2'].sum().to_frame(name='sum_SSC_A_2').reset_index()\n",
        "SSC_A_3_sum = grouper['SSC_A_3'].sum().to_frame(name='sum_SSC_A_3').reset_index()\n",
        "SSC_A_4_sum = grouper['SSC_A_4'].sum().to_frame(name='sum_SSC_A_4').reset_index()\n",
        "SSC_A_5_sum = grouper['SSC_A_5'].sum().to_frame(name='sum_SSC_A_5').reset_index()\n",
        "SSC_A_6_sum = grouper['SSC_A_6'].sum().to_frame(name='sum_SSC_A_6').reset_index()\n",
        "SSC_A_7_sum = grouper['SSC_A_7'].sum().to_frame(name='sum_SSC_A_7').reset_index()\n",
        "\n",
        "SSC_H_1_sum = grouper['SSC_H_1'].sum().to_frame(name='sum_SSC_H_1').reset_index()\n",
        "SSC_H_2_sum = grouper['SSC_H_2'].sum().to_frame(name='sum_SSC_H_2').reset_index()\n",
        "SSC_H_3_sum = grouper['SSC_H_3'].sum().to_frame(name='sum_SSC_H_3').reset_index()\n",
        "SSC_H_4_sum = grouper['SSC_H_4'].sum().to_frame(name='sum_SSC_H_4').reset_index()\n",
        "SSC_H_5_sum = grouper['SSC_H_5'].sum().to_frame(name='sum_SSC_H_5').reset_index()\n",
        "SSC_H_6_sum = grouper['SSC_H_6'].sum().to_frame(name='sum_SSC_H_6').reset_index()\n",
        "SSC_H_7_sum = grouper['SSC_H_7'].sum().to_frame(name='sum_SSC_H_7').reset_index()\n",
        "\n",
        "SSC_W_1_sum = grouper['SSC_W_1'].sum().to_frame(name='sum_SSC_W_1').reset_index()\n",
        "SSC_W_2_sum = grouper['SSC_W_2'].sum().to_frame(name='sum_SSC_W_2').reset_index()\n",
        "SSC_W_3_sum = grouper['SSC_W_3'].sum().to_frame(name='sum_SSC_W_3').reset_index()\n",
        "SSC_W_4_sum = grouper['SSC_W_4'].sum().to_frame(name='sum_SSC_W_4').reset_index()\n",
        "SSC_W_5_sum = grouper['SSC_W_5'].sum().to_frame(name='sum_SSC_W_5').reset_index()\n",
        "SSC_W_6_sum = grouper['SSC_W_6'].sum().to_frame(name='sum_SSC_W_6').reset_index()\n",
        "SSC_W_7_sum = grouper['SSC_W_7'].sum().to_frame(name='sum_SSC_W_7').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7YjtoFv61Be"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_1_phys_log_grouper_phys = [FSC_A_1_sum, FSC_A_2_sum, FSC_A_3_sum, FSC_A_4_sum, FSC_A_5_sum, FSC_A_6_sum, FSC_A_7_sum,\n",
        "                      FSC_H_1_sum, FSC_H_2_sum, FSC_H_3_sum, FSC_H_4_sum, FSC_H_5_sum, FSC_H_6_sum, FSC_H_7_sum,\n",
        "                      FSC_W_1_sum, FSC_W_2_sum, FSC_W_3_sum, FSC_W_4_sum, FSC_W_5_sum, FSC_W_6_sum, FSC_W_7_sum,\n",
        "                      SSC_A_1_sum, SSC_A_2_sum, SSC_A_3_sum, SSC_A_4_sum, SSC_A_5_sum, SSC_A_6_sum, SSC_A_7_sum,\n",
        "                      SSC_H_1_sum, SSC_H_2_sum, SSC_H_3_sum, SSC_H_4_sum, SSC_H_5_sum, SSC_H_6_sum, SSC_H_7_sum,\n",
        "                      SSC_W_1_sum, SSC_W_2_sum, SSC_W_3_sum, SSC_W_4_sum, SSC_W_5_sum, SSC_W_6_sum, SSC_W_7_sum,\n",
        "                      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8W4mz5R61Be"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_1_phys_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_1_phys_log_grouper_phys)\n",
        "pop_1_phys_log_grouped['Cell_line'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 2 - training data"
      ],
      "metadata": {
        "id": "X9XAoKJdxsoA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdd9F-z661Bf"
      },
      "outputs": [],
      "source": [
        "# deviding FSC and SSC intensities to bins\n",
        "Total_pop_2_log[['FSC_A_1', 'FSC_A_2', 'FSC_A_3', 'FSC_A_4', 'FSC_A_5', 'FSC_A_6', 'FSC_A_7']] = Total_pop_2_log['FSC_A'].apply(bins_phys)\n",
        "Total_pop_2_log[['FSC_H_1', 'FSC_H_2', 'FSC_H_3', 'FSC_H_4', 'FSC_H_5', 'FSC_H_6', 'FSC_H_7']] = Total_pop_2_log['FSC_H'].apply(bins_phys)\n",
        "Total_pop_2_log[['FSC_W_1', 'FSC_W_2', 'FSC_W_3', 'FSC_W_4', 'FSC_W_5', 'FSC_W_6', 'FSC_W_7']] = Total_pop_2_log['FSC_W'].apply(bins_phys)\n",
        "Total_pop_2_log[['SSC_A_1', 'SSC_A_2', 'SSC_A_3', 'SSC_A_4', 'SSC_A_5', 'SSC_A_6', 'SSC_A_7']] = Total_pop_2_log['SSC_A'].apply(bins_phys)\n",
        "Total_pop_2_log[['SSC_H_1', 'SSC_H_2', 'SSC_H_3', 'SSC_H_4', 'SSC_H_5', 'SSC_H_6', 'SSC_H_7']] = Total_pop_2_log['SSC_H'].apply(bins_phys)\n",
        "Total_pop_2_log[['SSC_W_1', 'SSC_W_2', 'SSC_W_3', 'SSC_W_4', 'SSC_W_5', 'SSC_W_6', 'SSC_W_7']] = Total_pop_2_log['SSC_W'].apply(bins_phys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIjNIoxp61Bf"
      },
      "outputs": [],
      "source": [
        "# grouping the sunm of the intensitys per bin based on population number\n",
        "grouper = Total_pop_2_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_1_sum = grouper['FSC_A_1'].sum().to_frame(name='sum_FSC_A_1').reset_index()\n",
        "FSC_A_2_sum = grouper['FSC_A_2'].sum().to_frame(name='sum_FSC_A_2').reset_index()\n",
        "FSC_A_3_sum = grouper['FSC_A_3'].sum().to_frame(name='sum_FSC_A_3').reset_index()\n",
        "FSC_A_4_sum = grouper['FSC_A_4'].sum().to_frame(name='sum_FSC_A_4').reset_index()\n",
        "FSC_A_5_sum = grouper['FSC_A_5'].sum().to_frame(name='sum_FSC_A_5').reset_index()\n",
        "FSC_A_6_sum = grouper['FSC_A_6'].sum().to_frame(name='sum_FSC_A_6').reset_index()\n",
        "FSC_A_7_sum = grouper['FSC_A_7'].sum().to_frame(name='sum_FSC_A_7').reset_index()\n",
        "\n",
        "FSC_H_1_sum = grouper['FSC_H_1'].sum().to_frame(name='sum_FSC_H_1').reset_index()\n",
        "FSC_H_2_sum = grouper['FSC_H_2'].sum().to_frame(name='sum_FSC_H_2').reset_index()\n",
        "FSC_H_3_sum = grouper['FSC_H_3'].sum().to_frame(name='sum_FSC_H_3').reset_index()\n",
        "FSC_H_4_sum = grouper['FSC_H_4'].sum().to_frame(name='sum_FSC_H_4').reset_index()\n",
        "FSC_H_5_sum = grouper['FSC_H_5'].sum().to_frame(name='sum_FSC_H_5').reset_index()\n",
        "FSC_H_6_sum = grouper['FSC_H_6'].sum().to_frame(name='sum_FSC_H_6').reset_index()\n",
        "FSC_H_7_sum = grouper['FSC_H_7'].sum().to_frame(name='sum_FSC_H_7').reset_index()\n",
        "\n",
        "FSC_W_1_sum = grouper['FSC_W_1'].sum().to_frame(name='sum_FSC_W_1').reset_index()\n",
        "FSC_W_2_sum = grouper['FSC_W_2'].sum().to_frame(name='sum_FSC_W_2').reset_index()\n",
        "FSC_W_3_sum = grouper['FSC_W_3'].sum().to_frame(name='sum_FSC_W_3').reset_index()\n",
        "FSC_W_4_sum = grouper['FSC_W_4'].sum().to_frame(name='sum_FSC_W_4').reset_index()\n",
        "FSC_W_5_sum = grouper['FSC_W_5'].sum().to_frame(name='sum_FSC_W_5').reset_index()\n",
        "FSC_W_6_sum = grouper['FSC_W_6'].sum().to_frame(name='sum_FSC_W_6').reset_index()\n",
        "FSC_W_7_sum = grouper['FSC_W_7'].sum().to_frame(name='sum_FSC_W_7').reset_index()\n",
        "\n",
        "SSC_A_1_sum = grouper['SSC_A_1'].sum().to_frame(name='sum_SSC_A_1').reset_index()\n",
        "SSC_A_2_sum = grouper['SSC_A_2'].sum().to_frame(name='sum_SSC_A_2').reset_index()\n",
        "SSC_A_3_sum = grouper['SSC_A_3'].sum().to_frame(name='sum_SSC_A_3').reset_index()\n",
        "SSC_A_4_sum = grouper['SSC_A_4'].sum().to_frame(name='sum_SSC_A_4').reset_index()\n",
        "SSC_A_5_sum = grouper['SSC_A_5'].sum().to_frame(name='sum_SSC_A_5').reset_index()\n",
        "SSC_A_6_sum = grouper['SSC_A_6'].sum().to_frame(name='sum_SSC_A_6').reset_index()\n",
        "SSC_A_7_sum = grouper['SSC_A_7'].sum().to_frame(name='sum_SSC_A_7').reset_index()\n",
        "\n",
        "SSC_H_1_sum = grouper['SSC_H_1'].sum().to_frame(name='sum_SSC_H_1').reset_index()\n",
        "SSC_H_2_sum = grouper['SSC_H_2'].sum().to_frame(name='sum_SSC_H_2').reset_index()\n",
        "SSC_H_3_sum = grouper['SSC_H_3'].sum().to_frame(name='sum_SSC_H_3').reset_index()\n",
        "SSC_H_4_sum = grouper['SSC_H_4'].sum().to_frame(name='sum_SSC_H_4').reset_index()\n",
        "SSC_H_5_sum = grouper['SSC_H_5'].sum().to_frame(name='sum_SSC_H_5').reset_index()\n",
        "SSC_H_6_sum = grouper['SSC_H_6'].sum().to_frame(name='sum_SSC_H_6').reset_index()\n",
        "SSC_H_7_sum = grouper['SSC_H_7'].sum().to_frame(name='sum_SSC_H_7').reset_index()\n",
        "\n",
        "SSC_W_1_sum = grouper['SSC_W_1'].sum().to_frame(name='sum_SSC_W_1').reset_index()\n",
        "SSC_W_2_sum = grouper['SSC_W_2'].sum().to_frame(name='sum_SSC_W_2').reset_index()\n",
        "SSC_W_3_sum = grouper['SSC_W_3'].sum().to_frame(name='sum_SSC_W_3').reset_index()\n",
        "SSC_W_4_sum = grouper['SSC_W_4'].sum().to_frame(name='sum_SSC_W_4').reset_index()\n",
        "SSC_W_5_sum = grouper['SSC_W_5'].sum().to_frame(name='sum_SSC_W_5').reset_index()\n",
        "SSC_W_6_sum = grouper['SSC_W_6'].sum().to_frame(name='sum_SSC_W_6').reset_index()\n",
        "SSC_W_7_sum = grouper['SSC_W_7'].sum().to_frame(name='sum_SSC_W_7').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM758MI961Bg"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_2_phys_log_grouper_phys = [FSC_A_1_sum, FSC_A_2_sum, FSC_A_3_sum, FSC_A_4_sum, FSC_A_5_sum, FSC_A_6_sum, FSC_A_7_sum,\n",
        "                      FSC_H_1_sum, FSC_H_2_sum, FSC_H_3_sum, FSC_H_4_sum, FSC_H_5_sum, FSC_H_6_sum, FSC_H_7_sum,\n",
        "                      FSC_W_1_sum, FSC_W_2_sum, FSC_W_3_sum, FSC_W_4_sum, FSC_W_5_sum, FSC_W_6_sum, FSC_W_7_sum,\n",
        "                      SSC_A_1_sum, SSC_A_2_sum, SSC_A_3_sum, SSC_A_4_sum, SSC_A_5_sum, SSC_A_6_sum, SSC_A_7_sum,\n",
        "                      SSC_H_1_sum, SSC_H_2_sum, SSC_H_3_sum, SSC_H_4_sum, SSC_H_5_sum, SSC_H_6_sum, SSC_H_7_sum,\n",
        "                      SSC_W_1_sum, SSC_W_2_sum, SSC_W_3_sum, SSC_W_4_sum, SSC_W_5_sum, SSC_W_6_sum, SSC_W_7_sum,\n",
        "                      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4CcyX5f61Bg"
      },
      "outputs": [],
      "source": [
        "# building a new dfs of the grouped data\n",
        "pop_2_phys_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_2_phys_log_grouper_phys)\n",
        "pop_2_phys_log_grouped['Cell_line'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeWK0UAid7n6"
      },
      "outputs": [],
      "source": [
        "# merging the training datasets of the two populations\n",
        "populations_df_phys_list = [pop_1_phys_log_grouped, pop_2_phys_log_grouped]\n",
        "Merge_population_phys = pd.DataFrame()\n",
        "for df in populations_df_phys_list:\n",
        "    Merge_population_phys = pd.concat([Merge_population_phys, df], ignore_index=True)\n",
        "\n",
        "Merge_population_phys = Merge_population_phys.drop('population_number', axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 1 - testing data"
      ],
      "metadata": {
        "id": "gtDh4ewGzRLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2yRJx_Wtpu8"
      },
      "outputs": [],
      "source": [
        "# deviding FSC and SSC intensities to bins\n",
        "Total_pop_1_test_log[['FSC_A_1', 'FSC_A_2', 'FSC_A_3', 'FSC_A_4', 'FSC_A_5', 'FSC_A_6', 'FSC_A_7']] = Total_pop_1_test_log['FSC_A'].apply(bins_phys)\n",
        "Total_pop_1_test_log[['FSC_H_1', 'FSC_H_2', 'FSC_H_3', 'FSC_H_4', 'FSC_H_5', 'FSC_H_6', 'FSC_H_7']] = Total_pop_1_test_log['FSC_H'].apply(bins_phys)\n",
        "Total_pop_1_test_log[['FSC_W_1', 'FSC_W_2', 'FSC_W_3', 'FSC_W_4', 'FSC_W_5', 'FSC_W_6', 'FSC_W_7']] = Total_pop_1_test_log['FSC_W'].apply(bins_phys)\n",
        "Total_pop_1_test_log[['SSC_A_1', 'SSC_A_2', 'SSC_A_3', 'SSC_A_4', 'SSC_A_5', 'SSC_A_6', 'SSC_A_7']] = Total_pop_1_test_log['SSC_A'].apply(bins_phys)\n",
        "Total_pop_1_test_log[['SSC_H_1', 'SSC_H_2', 'SSC_H_3', 'SSC_H_4', 'SSC_H_5', 'SSC_H_6', 'SSC_H_7']] = Total_pop_1_test_log['SSC_H'].apply(bins_phys)\n",
        "Total_pop_1_test_log[['SSC_W_1', 'SSC_W_2', 'SSC_W_3', 'SSC_W_4', 'SSC_W_5', 'SSC_W_6', 'SSC_W_7']] = Total_pop_1_test_log['SSC_W'].apply(bins_phys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ8IfiDHtpu9"
      },
      "outputs": [],
      "source": [
        "# grouping based on population number\n",
        "grouper = Total_pop_1_test_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_1_sum = grouper['FSC_A_1'].sum().to_frame(name='sum_FSC_A_1').reset_index()\n",
        "FSC_A_2_sum = grouper['FSC_A_2'].sum().to_frame(name='sum_FSC_A_2').reset_index()\n",
        "FSC_A_3_sum = grouper['FSC_A_3'].sum().to_frame(name='sum_FSC_A_3').reset_index()\n",
        "FSC_A_4_sum = grouper['FSC_A_4'].sum().to_frame(name='sum_FSC_A_4').reset_index()\n",
        "FSC_A_5_sum = grouper['FSC_A_5'].sum().to_frame(name='sum_FSC_A_5').reset_index()\n",
        "FSC_A_6_sum = grouper['FSC_A_6'].sum().to_frame(name='sum_FSC_A_6').reset_index()\n",
        "FSC_A_7_sum = grouper['FSC_A_7'].sum().to_frame(name='sum_FSC_A_7').reset_index()\n",
        "\n",
        "FSC_H_1_sum = grouper['FSC_H_1'].sum().to_frame(name='sum_FSC_H_1').reset_index()\n",
        "FSC_H_2_sum = grouper['FSC_H_2'].sum().to_frame(name='sum_FSC_H_2').reset_index()\n",
        "FSC_H_3_sum = grouper['FSC_H_3'].sum().to_frame(name='sum_FSC_H_3').reset_index()\n",
        "FSC_H_4_sum = grouper['FSC_H_4'].sum().to_frame(name='sum_FSC_H_4').reset_index()\n",
        "FSC_H_5_sum = grouper['FSC_H_5'].sum().to_frame(name='sum_FSC_H_5').reset_index()\n",
        "FSC_H_6_sum = grouper['FSC_H_6'].sum().to_frame(name='sum_FSC_H_6').reset_index()\n",
        "FSC_H_7_sum = grouper['FSC_H_7'].sum().to_frame(name='sum_FSC_H_7').reset_index()\n",
        "\n",
        "FSC_W_1_sum = grouper['FSC_W_1'].sum().to_frame(name='sum_FSC_W_1').reset_index()\n",
        "FSC_W_2_sum = grouper['FSC_W_2'].sum().to_frame(name='sum_FSC_W_2').reset_index()\n",
        "FSC_W_3_sum = grouper['FSC_W_3'].sum().to_frame(name='sum_FSC_W_3').reset_index()\n",
        "FSC_W_4_sum = grouper['FSC_W_4'].sum().to_frame(name='sum_FSC_W_4').reset_index()\n",
        "FSC_W_5_sum = grouper['FSC_W_5'].sum().to_frame(name='sum_FSC_W_5').reset_index()\n",
        "FSC_W_6_sum = grouper['FSC_W_6'].sum().to_frame(name='sum_FSC_W_6').reset_index()\n",
        "FSC_W_7_sum = grouper['FSC_W_7'].sum().to_frame(name='sum_FSC_W_7').reset_index()\n",
        "\n",
        "SSC_A_1_sum = grouper['SSC_A_1'].sum().to_frame(name='sum_SSC_A_1').reset_index()\n",
        "SSC_A_2_sum = grouper['SSC_A_2'].sum().to_frame(name='sum_SSC_A_2').reset_index()\n",
        "SSC_A_3_sum = grouper['SSC_A_3'].sum().to_frame(name='sum_SSC_A_3').reset_index()\n",
        "SSC_A_4_sum = grouper['SSC_A_4'].sum().to_frame(name='sum_SSC_A_4').reset_index()\n",
        "SSC_A_5_sum = grouper['SSC_A_5'].sum().to_frame(name='sum_SSC_A_5').reset_index()\n",
        "SSC_A_6_sum = grouper['SSC_A_6'].sum().to_frame(name='sum_SSC_A_6').reset_index()\n",
        "SSC_A_7_sum = grouper['SSC_A_7'].sum().to_frame(name='sum_SSC_A_7').reset_index()\n",
        "\n",
        "SSC_H_1_sum = grouper['SSC_H_1'].sum().to_frame(name='sum_SSC_H_1').reset_index()\n",
        "SSC_H_2_sum = grouper['SSC_H_2'].sum().to_frame(name='sum_SSC_H_2').reset_index()\n",
        "SSC_H_3_sum = grouper['SSC_H_3'].sum().to_frame(name='sum_SSC_H_3').reset_index()\n",
        "SSC_H_4_sum = grouper['SSC_H_4'].sum().to_frame(name='sum_SSC_H_4').reset_index()\n",
        "SSC_H_5_sum = grouper['SSC_H_5'].sum().to_frame(name='sum_SSC_H_5').reset_index()\n",
        "SSC_H_6_sum = grouper['SSC_H_6'].sum().to_frame(name='sum_SSC_H_6').reset_index()\n",
        "SSC_H_7_sum = grouper['SSC_H_7'].sum().to_frame(name='sum_SSC_H_7').reset_index()\n",
        "\n",
        "SSC_W_1_sum = grouper['SSC_W_1'].sum().to_frame(name='sum_SSC_W_1').reset_index()\n",
        "SSC_W_2_sum = grouper['SSC_W_2'].sum().to_frame(name='sum_SSC_W_2').reset_index()\n",
        "SSC_W_3_sum = grouper['SSC_W_3'].sum().to_frame(name='sum_SSC_W_3').reset_index()\n",
        "SSC_W_4_sum = grouper['SSC_W_4'].sum().to_frame(name='sum_SSC_W_4').reset_index()\n",
        "SSC_W_5_sum = grouper['SSC_W_5'].sum().to_frame(name='sum_SSC_W_5').reset_index()\n",
        "SSC_W_6_sum = grouper['SSC_W_6'].sum().to_frame(name='sum_SSC_W_6').reset_index()\n",
        "SSC_W_7_sum = grouper['SSC_W_7'].sum().to_frame(name='sum_SSC_W_7').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yon3iNXntpu9"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_1_test_phys_log_grouper_phys = [FSC_A_1_sum, FSC_A_2_sum, FSC_A_3_sum, FSC_A_4_sum, FSC_A_5_sum, FSC_A_6_sum, FSC_A_7_sum,\n",
        "                      FSC_H_1_sum, FSC_H_2_sum, FSC_H_3_sum, FSC_H_4_sum, FSC_H_5_sum, FSC_H_6_sum, FSC_H_7_sum,\n",
        "                      FSC_W_1_sum, FSC_W_2_sum, FSC_W_3_sum, FSC_W_4_sum, FSC_W_5_sum, FSC_W_6_sum, FSC_W_7_sum,\n",
        "                      SSC_A_1_sum, SSC_A_2_sum, SSC_A_3_sum, SSC_A_4_sum, SSC_A_5_sum, SSC_A_6_sum, SSC_A_7_sum,\n",
        "                      SSC_H_1_sum, SSC_H_2_sum, SSC_H_3_sum, SSC_H_4_sum, SSC_H_5_sum, SSC_H_6_sum, SSC_H_7_sum,\n",
        "                      SSC_W_1_sum, SSC_W_2_sum, SSC_W_3_sum, SSC_W_4_sum, SSC_W_5_sum, SSC_W_6_sum, SSC_W_7_sum,\n",
        "                      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azGPEnlqtpu9"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_1_test_phys_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_1_test_phys_log_grouper_phys)\n",
        "pop_1_test_phys_log_grouped['Cell_line'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 2 - testing data"
      ],
      "metadata": {
        "id": "3-Dleu_u1jyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v1Nf1yftpu-"
      },
      "outputs": [],
      "source": [
        "# deviding FSC and SSC intensities to bins\n",
        "Total_pop_2_test_log[['FSC_A_1', 'FSC_A_2', 'FSC_A_3', 'FSC_A_4', 'FSC_A_5', 'FSC_A_6', 'FSC_A_7']] = Total_pop_2_test_log['FSC_A'].apply(bins_phys)\n",
        "Total_pop_2_test_log[['FSC_H_1', 'FSC_H_2', 'FSC_H_3', 'FSC_H_4', 'FSC_H_5', 'FSC_H_6', 'FSC_H_7']] = Total_pop_2_test_log['FSC_H'].apply(bins_phys)\n",
        "Total_pop_2_test_log[['FSC_W_1', 'FSC_W_2', 'FSC_W_3', 'FSC_W_4', 'FSC_W_5', 'FSC_W_6', 'FSC_W_7']] = Total_pop_2_test_log['FSC_W'].apply(bins_phys)\n",
        "Total_pop_2_test_log[['SSC_A_1', 'SSC_A_2', 'SSC_A_3', 'SSC_A_4', 'SSC_A_5', 'SSC_A_6', 'SSC_A_7']] = Total_pop_2_test_log['SSC_A'].apply(bins_phys)\n",
        "Total_pop_2_test_log[['SSC_H_1', 'SSC_H_2', 'SSC_H_3', 'SSC_H_4', 'SSC_H_5', 'SSC_H_6', 'SSC_H_7']] = Total_pop_2_test_log['SSC_H'].apply(bins_phys)\n",
        "Total_pop_2_test_log[['SSC_W_1', 'SSC_W_2', 'SSC_W_3', 'SSC_W_4', 'SSC_W_5', 'SSC_W_6', 'SSC_W_7']] = Total_pop_2_test_log['SSC_W'].apply(bins_phys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kQ2zkPUtpu-"
      },
      "outputs": [],
      "source": [
        "# grouping based on population number\n",
        "grouper = Total_pop_2_test_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_1_sum = grouper['FSC_A_1'].sum().to_frame(name='sum_FSC_A_1').reset_index()\n",
        "FSC_A_2_sum = grouper['FSC_A_2'].sum().to_frame(name='sum_FSC_A_2').reset_index()\n",
        "FSC_A_3_sum = grouper['FSC_A_3'].sum().to_frame(name='sum_FSC_A_3').reset_index()\n",
        "FSC_A_4_sum = grouper['FSC_A_4'].sum().to_frame(name='sum_FSC_A_4').reset_index()\n",
        "FSC_A_5_sum = grouper['FSC_A_5'].sum().to_frame(name='sum_FSC_A_5').reset_index()\n",
        "FSC_A_6_sum = grouper['FSC_A_6'].sum().to_frame(name='sum_FSC_A_6').reset_index()\n",
        "FSC_A_7_sum = grouper['FSC_A_7'].sum().to_frame(name='sum_FSC_A_7').reset_index()\n",
        "\n",
        "FSC_H_1_sum = grouper['FSC_H_1'].sum().to_frame(name='sum_FSC_H_1').reset_index()\n",
        "FSC_H_2_sum = grouper['FSC_H_2'].sum().to_frame(name='sum_FSC_H_2').reset_index()\n",
        "FSC_H_3_sum = grouper['FSC_H_3'].sum().to_frame(name='sum_FSC_H_3').reset_index()\n",
        "FSC_H_4_sum = grouper['FSC_H_4'].sum().to_frame(name='sum_FSC_H_4').reset_index()\n",
        "FSC_H_5_sum = grouper['FSC_H_5'].sum().to_frame(name='sum_FSC_H_5').reset_index()\n",
        "FSC_H_6_sum = grouper['FSC_H_6'].sum().to_frame(name='sum_FSC_H_6').reset_index()\n",
        "FSC_H_7_sum = grouper['FSC_H_7'].sum().to_frame(name='sum_FSC_H_7').reset_index()\n",
        "\n",
        "FSC_W_1_sum = grouper['FSC_W_1'].sum().to_frame(name='sum_FSC_W_1').reset_index()\n",
        "FSC_W_2_sum = grouper['FSC_W_2'].sum().to_frame(name='sum_FSC_W_2').reset_index()\n",
        "FSC_W_3_sum = grouper['FSC_W_3'].sum().to_frame(name='sum_FSC_W_3').reset_index()\n",
        "FSC_W_4_sum = grouper['FSC_W_4'].sum().to_frame(name='sum_FSC_W_4').reset_index()\n",
        "FSC_W_5_sum = grouper['FSC_W_5'].sum().to_frame(name='sum_FSC_W_5').reset_index()\n",
        "FSC_W_6_sum = grouper['FSC_W_6'].sum().to_frame(name='sum_FSC_W_6').reset_index()\n",
        "FSC_W_7_sum = grouper['FSC_W_7'].sum().to_frame(name='sum_FSC_W_7').reset_index()\n",
        "\n",
        "SSC_A_1_sum = grouper['SSC_A_1'].sum().to_frame(name='sum_SSC_A_1').reset_index()\n",
        "SSC_A_2_sum = grouper['SSC_A_2'].sum().to_frame(name='sum_SSC_A_2').reset_index()\n",
        "SSC_A_3_sum = grouper['SSC_A_3'].sum().to_frame(name='sum_SSC_A_3').reset_index()\n",
        "SSC_A_4_sum = grouper['SSC_A_4'].sum().to_frame(name='sum_SSC_A_4').reset_index()\n",
        "SSC_A_5_sum = grouper['SSC_A_5'].sum().to_frame(name='sum_SSC_A_5').reset_index()\n",
        "SSC_A_6_sum = grouper['SSC_A_6'].sum().to_frame(name='sum_SSC_A_6').reset_index()\n",
        "SSC_A_7_sum = grouper['SSC_A_7'].sum().to_frame(name='sum_SSC_A_7').reset_index()\n",
        "\n",
        "SSC_H_1_sum = grouper['SSC_H_1'].sum().to_frame(name='sum_SSC_H_1').reset_index()\n",
        "SSC_H_2_sum = grouper['SSC_H_2'].sum().to_frame(name='sum_SSC_H_2').reset_index()\n",
        "SSC_H_3_sum = grouper['SSC_H_3'].sum().to_frame(name='sum_SSC_H_3').reset_index()\n",
        "SSC_H_4_sum = grouper['SSC_H_4'].sum().to_frame(name='sum_SSC_H_4').reset_index()\n",
        "SSC_H_5_sum = grouper['SSC_H_5'].sum().to_frame(name='sum_SSC_H_5').reset_index()\n",
        "SSC_H_6_sum = grouper['SSC_H_6'].sum().to_frame(name='sum_SSC_H_6').reset_index()\n",
        "SSC_H_7_sum = grouper['SSC_H_7'].sum().to_frame(name='sum_SSC_H_7').reset_index()\n",
        "\n",
        "SSC_W_1_sum = grouper['SSC_W_1'].sum().to_frame(name='sum_SSC_W_1').reset_index()\n",
        "SSC_W_2_sum = grouper['SSC_W_2'].sum().to_frame(name='sum_SSC_W_2').reset_index()\n",
        "SSC_W_3_sum = grouper['SSC_W_3'].sum().to_frame(name='sum_SSC_W_3').reset_index()\n",
        "SSC_W_4_sum = grouper['SSC_W_4'].sum().to_frame(name='sum_SSC_W_4').reset_index()\n",
        "SSC_W_5_sum = grouper['SSC_W_5'].sum().to_frame(name='sum_SSC_W_5').reset_index()\n",
        "SSC_W_6_sum = grouper['SSC_W_6'].sum().to_frame(name='sum_SSC_W_6').reset_index()\n",
        "SSC_W_7_sum = grouper['SSC_W_7'].sum().to_frame(name='sum_SSC_W_7').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MYcevJ-tpu-"
      },
      "outputs": [],
      "source": [
        "# setting an array of all the grouped data\n",
        "pop_2_test_phys_log_grouper_phys = [FSC_A_1_sum, FSC_A_2_sum, FSC_A_3_sum, FSC_A_4_sum, FSC_A_5_sum, FSC_A_6_sum, FSC_A_7_sum,\n",
        "                      FSC_H_1_sum, FSC_H_2_sum, FSC_H_3_sum, FSC_H_4_sum, FSC_H_5_sum, FSC_H_6_sum, FSC_H_7_sum,\n",
        "                      FSC_W_1_sum, FSC_W_2_sum, FSC_W_3_sum, FSC_W_4_sum, FSC_W_5_sum, FSC_W_6_sum, FSC_W_7_sum,\n",
        "                      SSC_A_1_sum, SSC_A_2_sum, SSC_A_3_sum, SSC_A_4_sum, SSC_A_5_sum, SSC_A_6_sum, SSC_A_7_sum,\n",
        "                      SSC_H_1_sum, SSC_H_2_sum, SSC_H_3_sum, SSC_H_4_sum, SSC_H_5_sum, SSC_H_6_sum, SSC_H_7_sum,\n",
        "                      SSC_W_1_sum, SSC_W_2_sum, SSC_W_3_sum, SSC_W_4_sum, SSC_W_5_sum, SSC_W_6_sum, SSC_W_7_sum,\n",
        "                      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfwLh0tltpu-"
      },
      "outputs": [],
      "source": [
        "# building a new dfs of the grouped data\n",
        "pop_2_test_phys_log_grouped = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_2_test_phys_log_grouper_phys)\n",
        "pop_2_test_phys_log_grouped['Cell_line'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eapBhN8X61Bg"
      },
      "outputs": [],
      "source": [
        "# merging the testing data of the two populations\n",
        "populations_df_test_phys_list = [pop_1_test_phys_log_grouped, pop_2_test_phys_log_grouped]\n",
        "Merge_population_test_phys = pd.DataFrame()\n",
        "for df in populations_df_test_phys_list:\n",
        "    Merge_population_test_phys = pd.concat([Merge_population_test_phys, df], ignore_index=True)\n",
        "\n",
        "Merge_population_test_phys = Merge_population_test_phys.drop('population_number', axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model"
      ],
      "metadata": {
        "id": "75E00jFz266T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFU5o1eN61Bg"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the lables\n",
        "cols = [col for col in Merge_population_phys.columns if col not in ['Cell_line']]\n",
        "population_data_phys = Merge_population_phys[cols] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_phys = Merge_population_phys['Cell_line'] # labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if the data includes only one sample per population\n",
        "\n",
        "## split to training and testing\n",
        "#X_train, X_test, y_train, y_test = train_test_split(All_data_train, target, test_size=0.3, random_state=42) # make sure to change names as needed"
      ],
      "metadata": {
        "id": "_VjHQazWtltl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjJrihRzuyM1"
      },
      "outputs": [],
      "source": [
        "# Do no run if data includes onnly one samlpe per population\n",
        "# separating the training features from the lables\n",
        "cols_test = [col for col in Merge_population_test_phys.columns if col not in ['Cell_line']]\n",
        "population_data_test_phys = Merge_population_test_phys[cols_test] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_test_phys = Merge_population_test_phys['Cell_line'] # labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "rmkol9qFAaCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_phys =  RandomForestClassifier(n_jobs=-1)\n",
        "param_grid = {\n",
        "'n_estimators': [500, 1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_phys,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(population_data_phys, target_phys)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "ZRSC-cgsM9hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model using the best parameters results"
      ],
      "metadata": {
        "id": "mnUoCo3YAwNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation using the training data\n",
        "random_forest_grouped_phys = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 500, n_jobs=-1)\n",
        "scores = cross_val_score(random_forest_grouped_phys, population_data_phys, target_phys, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "j56tyPECFbey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-xZAnZaug1s"
      },
      "outputs": [],
      "source": [
        "# fitting the model o all the training data\n",
        "my_model_RF_grouped_phys = random_forest_grouped_phys.fit(population_data_phys, target_phys)\n",
        "print ('Random Forest training Score: {0:2.5f}'.format(my_model_RF_grouped_phys.score(population_data_phys, target_phys)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-clS3JT361Bh"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_RF_grouped_phys = my_model_RF_grouped_phys.predict(population_data_test_phys)\n",
        "heatconmat(target_test_phys,pred_RF_grouped_phys)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_phys.score(population_data_test_phys, target_test_phys)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a8FmbrX61Bh"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find feature importance\n",
        "perm = PermutationImportance(my_model_RF_grouped_phys).fit(population_data_phys, target_phys)\n",
        "eli5.show_weights(perm, feature_names = population_data_phys.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_rf_phys_500' # name the model\n",
        "pickle.dump(my_model_RF_grouped_phys, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "nCEHBxcGFpAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XBGoost"
      ],
      "metadata": {
        "id": "AG6MGU8LB2SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparamaters analysis"
      ],
      "metadata": {
        "id": "ppCQISdMB26K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSwxfe49KANM"
      },
      "outputs": [],
      "source": [
        "xgboost_classification = XGBClassifier(n_jobs=-1)\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(population_data_phys,target_phys)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model using the parameters best results"
      ],
      "metadata": {
        "id": "EypGHOWBCMX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation on the training data\n",
        "xgboost_classification = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "scores = cross_val_score(xgboost_classification, population_data_phys, target_phys, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "YRjkugFpJFa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1oJVjGy61Bh"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_xgboost_grouped_phys = xgboost_classification.fit(population_data_phys, target_phys)\n",
        "print ('xgboost training Score: {0:2.5f}'.format(my_model_xgboost_grouped_phys.score(population_data_phys, target_phys)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvZDUIax61Bh"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost_phys = my_model_xgboost_grouped_phys.predict(population_data_test_phys)\n",
        "heatconmat(target_test_phys,pred_xgboost_phys)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_grouped_phys.score(population_data_test_phys, target_test_phys)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5abkmNi61Bi"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find feature importance\n",
        "perm_xgboost_phys = PermutationImportance(my_model_xgboost_grouped_phys, random_state=1).fit(population_data_phys, target_phys)\n",
        "eli5.show_weights(perm_xgboost_phys, feature_names = population_data_phys.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_xgboost_phys_500' # name the model\n",
        "pickle.dump(my_model_xgboost_grouped_phys, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "y235TFdhJlDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "IrVtnAlVDXho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter analysis"
      ],
      "metadata": {
        "id": "wSF4N5EADZVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(population_data_phys, target_phys)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "9kpaPFUSKiQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSH4nxbyLKlZ"
      },
      "outputs": [],
      "source": [
        "# cross validation on the training data\n",
        "SVM_phys = svm.SVC(C = 0.001, gamma = 0.001, kernel = 'linear')\n",
        "scores = cross_val_score(SVM_phys, population_data_phys, target_phys, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNNoxm8D61Bi"
      },
      "outputs": [],
      "source": [
        "# Fitting the model on all the training data\n",
        "my_model_SVM_grouped_phys = SVM_phys.fit(population_data_phys, target_phys)\n",
        "print ('SVM Score: {0:2.5f}'.format(my_model_SVM_grouped_phys.score(population_data_phys, target_phys)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnnvlzMDwFi0"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM_phys = my_model_SVM_grouped_phys.predict(population_data_test_phys)\n",
        "heatconmat(target_test_phys,pred_SVM_phys)\n",
        "print ('SVM testing Score: {0:2.5f}'.format(my_model_SVM_grouped_phys.score(population_data_test_phys, target_test_phys)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUHA7FQQ61Bj"
      },
      "outputs": [],
      "source": [
        "# permutaion analysis to find feature importance\n",
        "perm_SVM = PermutationImportance(my_model_SVM_grouped_phys, random_state=1).fit(population_data_phys, target_phys)\n",
        "eli5.show_weights(perm_SVM, feature_names = population_data_phys.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_SVM_phys_500' # name the model\n",
        "pickle.dump(my_model_SVM_grouped_phys, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "N1ISE6dvlxyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opening the saved models\n",
        "\n",
        "# RF\n",
        "model_name = 'H460_groupkfold_populations_rf_phys_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_rf_phys = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# xgboost\n",
        "model_name = 'H460_groupkfold_populations_xgboost_phys_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_xgboost_phys = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# SVM\n",
        "model_name = 'H460_groupkfold_populations_SVM_phys_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_SVM_phys = pickle.load(a_file)\n",
        "a_file.close()"
      ],
      "metadata": {
        "id": "6omnOyFA9Zvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcEKO_I161Bj"
      },
      "source": [
        "# new experiment testing - Batch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP0HUPiC61Bj"
      },
      "outputs": [],
      "source": [
        "# reading the batch data- for this example the batch was already preprocessed\n",
        "Merge_population_batch_phys_filtered = pd.read_csv(data_path +'Merge_population_H460_test_phys_500.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PomsphMN61Bj"
      },
      "outputs": [],
      "source": [
        "# separating the data to features and lables\n",
        "cols_batch = [col for col in Merge_population_batch_phys_filtered.columns if col not in ['Cell_line']]\n",
        "population_data_batch_phys = Merge_population_batch_phys_filtered[cols_batch]\n",
        "# assigning the cell identity column as target\n",
        "target_batch_phys_bins = Merge_population_batch_phys_filtered['Cell_line']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "3H6Xj0qOFdhA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h8K2WlX61Bj"
      },
      "outputs": [],
      "source": [
        "# prediction and visualiization\n",
        "pred_random_forest_phys_bins_batch = my_model_RF_grouped_phys.predict(population_data_batch_phys)\n",
        "heatconmat(target_batch_phys_bins,pred_random_forest_phys_bins_batch)\n",
        "accuracy = accuracy_score(pred_random_forest_phys_bins_batch.astype(int), target_batch_phys_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "n-XPPvhAFsHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQTkhMQr61Bj"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost_phys_bins_batch = my_model_xgboost_grouped_phys.predict(population_data_batch_phys)\n",
        "heatconmat(target_batch_phys_bins,pred_xgboost_phys_bins_batch)\n",
        "accuracy = accuracy_score(pred_xgboost_phys_bins_batch.astype(int), target_batch_phys_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "WmaissYUGAX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87aCOzK061Bk"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM_phys_bins_batch = my_model_SVM_grouped_phys.predict(population_data_batch_phys)\n",
        "heatconmat(target_batch_phys_bins,pred_SVM_phys_bins_batch)\n",
        "accuracy = accuracy_score(pred_SVM_phys_bins_batch.astype(int), target_batch_phys_bins.astype(int))\n",
        "aprint(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimention reduction - PCA"
      ],
      "metadata": {
        "id": "UY5HS1PWGXai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZrnP0yjUSFX"
      },
      "outputs": [],
      "source": [
        "# rescale the data\n",
        "train_data_rescaled_phys = scaler.fit_transform(population_data_phys)\n",
        "test_data_rescaled_phys = scaler.fit_transform(population_data_test_phys)\n",
        "batch_data_rescaled_phys = scaler.fit_transform(population_data_batch_phys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4v7en0CUSFX"
      },
      "outputs": [],
      "source": [
        "# displaying the componantes and the data varience they includes for choosing the number of componnets to reduced to\n",
        "cov_mat = np.cov(train_data_rescaled_phys.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "# plot explained variances\n",
        "plt.bar(range(1,43), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,43), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9vCFj_USFX"
      },
      "outputs": [],
      "source": [
        "pca_phys = PCA(n_components=3) # choosing the number of dimentions\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca_phys = pca_phys.fit_transform(train_data_rescaled_phys)\n",
        "X_test_pca_phys = pca_phys.transform(test_data_rescaled_phys)\n",
        "X_batch_pca_phys = pca_phys.transform(batch_data_rescaled_phys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk8YtIbcENZT"
      },
      "outputs": [],
      "source": [
        "# How much each original feature contribute to the components\n",
        "print(pd.DataFrame(np.abs(pca_phys.components_),columns=population_data_phys.columns ,index = ['PC-1', 'PC-2', 'PC-3'])) # adjust the number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzAqQT9xENZU"
      },
      "outputs": [],
      "source": [
        "# printing the most contributing original feature for each component\n",
        "most_important = np.abs(pca_phys.components_).argmax(axis=1)\n",
        "initial_feature_names = population_data_phys.columns\n",
        "most_important_names = initial_feature_names[most_important]\n",
        "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(3)} # adjust the number of components\n",
        "pca_results = pd.DataFrame(dic.items())\n",
        "print(pca_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "-uwWq8UJGy9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "lbv6ZEFEJX55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_phys_pca= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_phys_pca,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(X_train_pca_phys, target_phys)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "QgLaZNYk32zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "oIn8kKBgHkbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the modeln and evaluating its performance on the testing data\n",
        "random_forest_grouped_phys_no_bins_pca = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "my_model_RF_PCA_phys = random_forest_grouped_phys_no_bins_pca.fit(X_train_pca_phys, target_phys)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_PCA_phys.score(X_test_pca_phys, target_test_phys)))"
      ],
      "metadata": {
        "id": "QuU56-JC32zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uR68OQYUSFY"
      },
      "outputs": [],
      "source": [
        "# evaluating the model peerformance on the Batch data\n",
        "pred_random_forest_phys_PCA_batch = my_model_RF_PCA_phys.predict(X_batch_pca_phys)\n",
        "accuracy = accuracy_score(pred_random_forest_phys_PCA_batch.astype(int), target_batch_phys_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "iZ-XuJXaJEqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "XSl8NDUlJPPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVlp65Dn7_OT"
      },
      "outputs": [],
      "source": [
        "xgboost_classification = XGBClassifier(n_jobs=-1)\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(X_train_pca_phys,target_phys)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "OQ__2ANrJpJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "xgboost_classification_phys_pca = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "my_model_xgboost_PCA_phys = xgboost_classification_phys_pca.fit(X_train_pca_phys, target_phys)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_PCA_phys.score(X_test_pca_phys, target_test_phys)))"
      ],
      "metadata": {
        "id": "MRpynVI47_OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVjVjhSpUSFY"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_xgboost_phys_PCA_batch = my_model_xgboost_PCA_phys.predict(X_batch_pca_phys)\n",
        "accuracy = accuracy_score(pred_xgboost_phys_PCA_batch.astype(int), target_batch_phys_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "IhTDIKI2K0yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "M1AmvwnMLD2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(X_train_pca_phys,target_phys)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "862HzjoONJBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "gJ_-ie8ILRje"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o7V8QaRNJBY"
      },
      "outputs": [],
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "SVM_phys_pca = svm.SVC(C = 0.001, gamma = 0.001, kernel = 'linear')\n",
        "my_model_SVM_PCA_phys = SVM_phys_pca.fit(X_train_pca_phys, target_phys)\n",
        "print ('SVM Score: {0:2.5f}'.format(my_model_SVM_PCA_phys.score(X_test_pca_phys, target_test_phys)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DAb9C9TUSFY"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_SVM_phys_PCA_batch = my_model_SVM_PCA_phys.predict(X_batch_pca_phys)\n",
        "accuracy = accuracy_score(pred_SVM_phys_PCA_batch.astype(int), target_batch_phys_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCnjcqh61Bk"
      },
      "source": [
        "# Physiological features without bins"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 1 - Training data"
      ],
      "metadata": {
        "id": "xyQfzp_BMqQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfReW7MV61Bl"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 1 train FSC and SSC values\n",
        "grouper_phys_no_bins = Total_pop_1_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_mean = grouper_phys_no_bins['FSC_A'].mean().to_frame(name='mean_FSC_A').reset_index()\n",
        "FSC_A_median = grouper_phys_no_bins['FSC_A'].median().to_frame(name='median_FSC_A').reset_index()\n",
        "\n",
        "FSC_H_mean = grouper_phys_no_bins['FSC_H'].mean().to_frame(name='mean_FSC_H').reset_index()\n",
        "FSC_H_median = grouper_phys_no_bins['FSC_H'].median().to_frame(name='median_FSC_H').reset_index()\n",
        "\n",
        "FSC_W_mean = grouper_phys_no_bins['FSC_W'].mean().to_frame(name='mean_FSC_W').reset_index()\n",
        "FSC_W_median = grouper_phys_no_bins['FSC_W'].median().to_frame(name='median_FSC_W').reset_index()\n",
        "\n",
        "SSC_A_mean = grouper_phys_no_bins['SSC_A'].mean().to_frame(name='mean_SSC_A').reset_index()\n",
        "SSC_A_median = grouper_phys_no_bins['SSC_A'].median().to_frame(name='median_SSC_A').reset_index()\n",
        "\n",
        "SSC_H_mean = grouper_phys_no_bins['SSC_H'].mean().to_frame(name='mean_SSC_H').reset_index()\n",
        "SSC_H_median = grouper_phys_no_bins['SSC_H'].median().to_frame(name='median_SSC_H').reset_index()\n",
        "\n",
        "SSC_W_mean = grouper_phys_no_bins['SSC_W'].mean().to_frame(name='mean_SSC_W').reset_index()\n",
        "SSC_W_median = grouper_phys_no_bins['SSC_W'].median().to_frame(name='median_SSC_W').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdDa2NE861Bl"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "pop_1_log_grouper_phys_no_bins = [FSC_A_mean, FSC_H_mean, FSC_W_mean, SSC_A_mean, SSC_H_mean, SSC_W_mean, FSC_A_median,\n",
        "                          FSC_H_median, FSC_W_median, SSC_A_median, SSC_H_median, SSC_W_median\n",
        "                      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhNagxgb61Bl"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_1_log_grouped_phys_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_1_log_grouper_phys_no_bins)\n",
        "pop_1_log_grouped_phys_no_bins['Cell_line'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 2 - Training data"
      ],
      "metadata": {
        "id": "mwLYIV9SNZiP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmBQFQnZ61Bk"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 1 train FSC and SSC values\n",
        "grouper_phys_no_bins = Total_pop_2_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_mean = grouper_phys_no_bins['FSC_A'].mean().to_frame(name='mean_FSC_A').reset_index()\n",
        "FSC_A_median = grouper_phys_no_bins['FSC_A'].median().to_frame(name='median_FSC_A').reset_index()\n",
        "\n",
        "FSC_H_mean = grouper_phys_no_bins['FSC_H'].mean().to_frame(name='mean_FSC_H').reset_index()\n",
        "FSC_H_median = grouper_phys_no_bins['FSC_H'].median().to_frame(name='median_FSC_H').reset_index()\n",
        "\n",
        "FSC_W_mean = grouper_phys_no_bins['FSC_W'].mean().to_frame(name='mean_FSC_W').reset_index()\n",
        "FSC_W_median = grouper_phys_no_bins['FSC_W'].median().to_frame(name='median_FSC_W').reset_index()\n",
        "\n",
        "SSC_A_mean = grouper_phys_no_bins['SSC_A'].mean().to_frame(name='mean_SSC_A').reset_index()\n",
        "SSC_A_median = grouper_phys_no_bins['SSC_A'].median().to_frame(name='median_SSC_A').reset_index()\n",
        "\n",
        "SSC_H_mean = grouper_phys_no_bins['SSC_H'].mean().to_frame(name='mean_SSC_H').reset_index()\n",
        "SSC_H_median = grouper_phys_no_bins['SSC_H'].median().to_frame(name='median_SSC_H').reset_index()\n",
        "\n",
        "SSC_W_mean = grouper_phys_no_bins['SSC_W'].mean().to_frame(name='mean_SSC_W').reset_index()\n",
        "SSC_W_median = grouper_phys_no_bins['SSC_W'].median().to_frame(name='median_SSC_W').reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FfhNh5h61Bk"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "pop_2_log_grouper_phys_no_bins = [FSC_A_mean,FSC_H_mean, FSC_W_mean, SSC_A_mean, SSC_H_mean, SSC_W_mean ,FSC_A_median,\n",
        "                          FSC_H_median, FSC_W_median, SSC_A_median, SSC_H_median, SSC_W_median\n",
        "                      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-HFvR4a61Bk"
      },
      "outputs": [],
      "source": [
        "# building a new dfs of the grouped data\n",
        "pop_2_log_grouped_phys_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_2_log_grouper_phys_no_bins)\n",
        "pop_2_log_grouped_phys_no_bins['Cell_line'] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 1 - Testing data"
      ],
      "metadata": {
        "id": "jNt0kPNWNs98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWHRXduux-9U"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 1 test FSC and SSC values\n",
        "grouper_phys_no_bins = Total_pop_1_test_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_mean = grouper_phys_no_bins['FSC_A'].mean().to_frame(name='mean_FSC_A').reset_index()\n",
        "FSC_A_median = grouper_phys_no_bins['FSC_A'].median().to_frame(name='median_FSC_A').reset_index()\n",
        "\n",
        "FSC_H_mean = grouper_phys_no_bins['FSC_H'].mean().to_frame(name='mean_FSC_H').reset_index()\n",
        "FSC_H_median = grouper_phys_no_bins['FSC_H'].median().to_frame(name='median_FSC_H').reset_index()\n",
        "\n",
        "FSC_W_mean = grouper_phys_no_bins['FSC_W'].mean().to_frame(name='mean_FSC_W').reset_index()\n",
        "FSC_W_median = grouper_phys_no_bins['FSC_W'].median().to_frame(name='median_FSC_W').reset_index()\n",
        "\n",
        "SSC_A_mean = grouper_phys_no_bins['SSC_A'].mean().to_frame(name='mean_SSC_A').reset_index()\n",
        "SSC_A_median = grouper_phys_no_bins['SSC_A'].median().to_frame(name='median_SSC_A').reset_index()\n",
        "\n",
        "SSC_H_mean = grouper_phys_no_bins['SSC_H'].mean().to_frame(name='mean_SSC_H').reset_index()\n",
        "SSC_H_median = grouper_phys_no_bins['SSC_H'].median().to_frame(name='median_SSC_H').reset_index()\n",
        "\n",
        "SSC_W_mean = grouper_phys_no_bins['SSC_W'].mean().to_frame(name='mean_SSC_W').reset_index()\n",
        "SSC_W_median = grouper_phys_no_bins['SSC_W'].median().to_frame(name='median_SSC_W').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVOot9Rxx-9U"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "pop_1_test_log_grouper_phys_no_bins = [FSC_A_mean, FSC_H_mean, FSC_W_mean, SSC_A_mean, SSC_H_mean, SSC_W_mean, FSC_A_median,\n",
        "                          FSC_H_median, FSC_W_median, SSC_A_median, SSC_H_median, SSC_W_median]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKGLpIDux-9U"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_1_test_log_grouped_phys_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_1_test_log_grouper_phys_no_bins)\n",
        "pop_1_test_log_grouped_phys_no_bins['Cell_line'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pop 2 - Testing data"
      ],
      "metadata": {
        "id": "YHere4zmOgnO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MZgtOaVx-9V"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 1 test FSC and SSC values\n",
        "grouper_phys_no_bins = Total_pop_2_test_log.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "FSC_A_mean = grouper_phys_no_bins['FSC_A'].mean().to_frame(name='mean_FSC_A').reset_index()\n",
        "FSC_A_median = grouper_phys_no_bins['FSC_A'].median().to_frame(name='median_FSC_A').reset_index()\n",
        "\n",
        "FSC_H_mean = grouper_phys_no_bins['FSC_H'].mean().to_frame(name='mean_FSC_H').reset_index()\n",
        "FSC_H_median = grouper_phys_no_bins['FSC_H'].median().to_frame(name='median_FSC_H').reset_index()\n",
        "\n",
        "FSC_W_mean = grouper_phys_no_bins['FSC_W'].mean().to_frame(name='mean_FSC_W').reset_index()\n",
        "FSC_W_median = grouper_phys_no_bins['FSC_W'].median().to_frame(name='median_FSC_W').reset_index()\n",
        "\n",
        "SSC_A_mean = grouper_phys_no_bins['SSC_A'].mean().to_frame(name='mean_SSC_A').reset_index()\n",
        "SSC_A_median = grouper_phys_no_bins['SSC_A'].median().to_frame(name='median_SSC_A').reset_index()\n",
        "\n",
        "SSC_H_mean = grouper_phys_no_bins['SSC_H'].mean().to_frame(name='mean_SSC_H').reset_index()\n",
        "SSC_H_median = grouper_phys_no_bins['SSC_H'].median().to_frame(name='median_SSC_H').reset_index()\n",
        "\n",
        "SSC_W_mean = grouper_phys_no_bins['SSC_W'].mean().to_frame(name='mean_SSC_W').reset_index()\n",
        "SSC_W_median = grouper_phys_no_bins['SSC_W'].median().to_frame(name='median_SSC_W').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgIKUN3ox-9V"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "pop_2_test_log_grouper_phys_no_bins = [FSC_A_mean,FSC_H_mean, FSC_W_mean, SSC_A_mean, SSC_H_mean, SSC_W_mean ,FSC_A_median,\n",
        "                          FSC_H_median, FSC_W_median, SSC_A_median, SSC_H_median, SSC_W_median]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab4bgdlcx-9V"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_2_test_log_grouped_phys_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), pop_2_test_log_grouper_phys_no_bins)\n",
        "pop_2_test_log_grouped_phys_no_bins['Cell_line'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_11n8Ipb5kcJ"
      },
      "outputs": [],
      "source": [
        "# merging the training data of the two populations\n",
        "populations_df_list = [pop_1_log_grouped_phys_no_bins, pop_2_log_grouped_phys_no_bins]\n",
        "Merge_population_phys_no_bins = pd.DataFrame()\n",
        "for df in populations_df_list:\n",
        "    Merge_population_phys_no_bins = pd.concat([Merge_population_phys_no_bins, df], ignore_index=True)\n",
        "\n",
        "Merge_population_phys_no_bins.drop('population_number', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnWv7IYM61Bl"
      },
      "outputs": [],
      "source": [
        "# merging the testing data of the two populations\n",
        "populations_test_df_list = [pop_1_test_log_grouped_phys_no_bins, pop_2_test_log_grouped_phys_no_bins]\n",
        "Merge_population_test_phys_no_bins = pd.DataFrame()\n",
        "for df in populations_test_df_list:\n",
        "    Merge_population_test_phys_no_bins = pd.concat([Merge_population_test_phys_no_bins, df], ignore_index=True)\n",
        "\n",
        "Merge_population_test_phys_no_bins.drop('population_number', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the models"
      ],
      "metadata": {
        "id": "70FcdKtDSKvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWBNIEW461Bm"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the lables\n",
        "cols = [col for col in Merge_population_phys_no_bins.columns if col not in ['Cell_line']]\n",
        "population_data_phys_no_bins = Merge_population_phys_no_bins[cols]\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_phys_no_bins = Merge_population_phys_no_bins['Cell_line']\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if the data includes only one sample per population\n",
        "\n",
        "## split to training and testing\n",
        "#X_train, X_test, y_train, y_test = train_test_split(All_data_train, target, test_size=0.3, random_state=42) # make sure to change names as needed"
      ],
      "metadata": {
        "id": "MObbxVfPvA7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsdJWkQzy76I"
      },
      "outputs": [],
      "source": [
        "# Do not run if the data includes only one sample per population\n",
        "# separating the testing features from the lables\n",
        "cols_test = [col for col in Merge_population_test_phys_no_bins.columns if col not in ['Cell_line']]\n",
        "population_data_test_phys_no_bins = Merge_population_test_phys_no_bins[cols_test]\n",
        "# assigning the cell identity column as target\n",
        "target_test_phys_no_bins = Merge_population_test_phys_no_bins['Cell_line']\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "8cTFvfikS39-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "Ez6VS7WES7J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_phys_no_bins= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_phys_no_bins,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "oR9x5HEWQKW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using the best parameters results"
      ],
      "metadata": {
        "id": "DYz2bA3YUbAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "random_forest_grouped_phys_no_bins = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "scores = cross_val_score(random_forest_grouped_phys_no_bins, population_data_phys_no_bins, target_phys_no_bins, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "5YfkAZiMQKW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coX8I_lSzX-l"
      },
      "outputs": [],
      "source": [
        "# fitting the model using all the training data\n",
        "my_model_RF_grouped_phys_no_bins = random_forest_grouped_phys_no_bins.fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "print ('Random Forest training Score: {0:2.5f}'.format(my_model_RF_grouped_phys_no_bins.score(population_data_phys_no_bins, target_phys_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcAdEpn861Bm"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_RF_grouped_phys_no_bins = my_model_RF_grouped_phys_no_bins.predict(population_data_test_phys_no_bins)\n",
        "heatconmat(target_test_phys_no_bins,pred_RF_grouped_phys_no_bins)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_phys_no_bins.score(population_data_test_phys_no_bins, target_test_phys_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKe9KDT861Bm"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find feature importance\n",
        "perm = PermutationImportance(my_model_RF_grouped_phys_no_bins).fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "eli5.show_weights(perm, feature_names = population_data_phys_no_bins.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_rf_phys_no_bins_500' # name the model\n",
        "pickle.dump(my_model_RF_grouped_phys_no_bins, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "GvMlzOtvktjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "D3EGmu-9VanE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "LCJ7YzVqV7Cd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiKHtpBGUfUS"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_phys_no_bins = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_phys_no_bins,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(population_data_phys_no_bins,target_phys_no_bins)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using the best parameters results"
      ],
      "metadata": {
        "id": "J3wxx8ywYI58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# crosss validation on the training data\n",
        "xgboost_classification_phys_no_bins = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 1, n_jobs=-1)\n",
        "scores = cross_val_score(xgboost_classification_phys_no_bins, population_data_phys_no_bins, target_phys_no_bins, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "flPzykKLmJaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3CGfLp161Bn"
      },
      "outputs": [],
      "source": [
        "# Fitting the model on all the training data\n",
        "my_model_xgboost_grouped_phys_no_bins = xgboost_classification_phys_no_bins.fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "print ('xgboost training Score: {0:2.5f}'.format(my_model_xgboost_grouped_phys_no_bins.score(population_data_phys_no_bins, target_phys_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmtn-UsB61Bn"
      },
      "outputs": [],
      "source": [
        "# prediction and validation\n",
        "pred_xgboost_phys_no_bins = my_model_xgboost_grouped_phys_no_bins.predict(population_data_test_phys_no_bins)\n",
        "heatconmat(target_test_phys_no_bins,pred_xgboost_phys_no_bins)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_grouped_phys_no_bins.score(population_data_test_phys_no_bins, target_test_phys_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q6iwo7S61Bn"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_xgboost_phys_no_bins = PermutationImportance(my_model_xgboost_grouped_phys_no_bins, random_state=1).fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "eli5.show_weights(perm_xgboost_phys_no_bins, feature_names = population_data_phys_no_bins.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_xgboost_phys_no_bins_500' # name the model\n",
        "pickle.dump(my_model_xgboost_grouped_phys_no_bins, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "HFnQoHdanNV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "Shj9qQlRZKm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "ijWbFVcJZOJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "bcdi0lsPnZgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using the best parameters results"
      ],
      "metadata": {
        "id": "KR9OHOS0ZdK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orrCYIoInZgj"
      },
      "outputs": [],
      "source": [
        "# cross validation analysis\n",
        "SVM_phys_no_bins = svm.SVC(C = 0.001, gamma = 0.01, kernel = 'poly')\n",
        "scores = cross_val_score(SVM_phys_no_bins, population_data_phys_no_bins, target_phys_no_bins, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA0Qvf4g61Bn"
      },
      "outputs": [],
      "source": [
        "# Fitting the model on all the training data\n",
        "my_model_SVM_grouped_phys_no_bins = SVM_phys_no_bins.fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "print ('SVM training Score: {0:2.5f}'.format(my_model_xgboost_grouped_phys_no_bins.score(population_data_phys_no_bins, target_phys_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4WEAicI0n4Q"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM_phys_no_bins = my_model_SVM_grouped_phys_no_bins.predict(population_data_test_phys_no_bins)\n",
        "heatconmat(target_test_phys_no_bins,pred_xgboost_phys_no_bins)\n",
        "print ('SVM testing Score: {0:2.5f}'.format(my_model_SVM_grouped_phys_no_bins.score(population_data_test_phys_no_bins, target_test_phys_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M6HH0EO61Bo"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_SVM = PermutationImportance(my_model_SVM_grouped_phys_no_bins, random_state=1).fit(population_data_phys_no_bins, target_phys_no_bins)\n",
        "eli5.show_weights(perm_SVM, feature_names = population_data_phys_no_bins.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_SVM_phys_no_bins_500' # name the model\n",
        "pickle.dump(my_model_SVM_grouped_phys_no_bins, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "bIWfjNADWZWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bf6_yT461Bo"
      },
      "source": [
        "# new experiment testing - Batch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsBnAfjf61Bo"
      },
      "outputs": [],
      "source": [
        "# Reading the Batch data\n",
        "Merge_population_batch_phys_no_bins = pd.read_csv(data_path +'Merge_population_H460_test_phys_no_bins_500.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KypKOuc61Bo"
      },
      "outputs": [],
      "source": [
        "# separating the data to features and lables\n",
        "cols_batch = [col for col in Merge_population_batch_phys_no_bins.columns if col not in ['Cell_line']]\n",
        "population_data_batch_phys_no_bins = Merge_population_batch_phys_no_bins[cols]\n",
        "# assigning the cell identity column as target\n",
        "target_batch_phys_no_bins = Merge_population_batch_phys_no_bins['Cell_line']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "Gujt7nnpbU-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpAHGBSl61Bo"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_phys_no_bins_rf_batch = my_model_RF_grouped_phys_no_bins.predict(population_data_batch_phys_no_bins)\n",
        "heatconmat(target_batch_phys_no_bins, pred_phys_no_bins_rf_batch)\n",
        "accuracy = accuracy_score(pred_phys_no_bins_rf_batch.astype(int), target_batch_phys_no_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "lsq8SfWAbnJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l0H6mRT61Bp"
      },
      "outputs": [],
      "source": [
        "pred_phys_no_bins_xgboost_batch = my_model_xgboost_grouped_phys_no_bins.predict(population_data_batch_phys_no_bins)\n",
        "heatconmat(target_batch_phys_no_bins,pred_phys_no_bins_xgboost_batch)\n",
        "accuracy = accuracy_score(pred_phys_no_bins_xgboost_batch.astype(int), target_batch_phys_no_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "P8nTSuG9bzOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UStstbm61Bp"
      },
      "outputs": [],
      "source": [
        "pred_phys_no_bins_SVM_batch = my_model_SVM_grouped_phys_no_bins.predict(population_data_batch_phys_no_bins)\n",
        "heatconmat(target_batch_phys_no_bins, pred_phys_no_bins_SVM_batch)\n",
        "accuracy = accuracy_score(pred_phys_no_bins_SVM_batch.astype(int), target_batch_phys_no_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimention reduction - PCA"
      ],
      "metadata": {
        "id": "vST6DSsvcAzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gCPJgHTgXJK"
      },
      "outputs": [],
      "source": [
        "# rescale the data for pca\n",
        "train_data_rescaled_phys_no_bins = scaler.fit_transform(population_data_phys_no_bins)\n",
        "test_data_rescaled_phys_no_bins = scaler.fit_transform(population_data_test_phys_no_bins)\n",
        "batch_data_rescaled_phys_no_bins = scaler.fit_transform(population_data_batch_phys_no_bins)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3pX4xjGgXJK"
      },
      "outputs": [],
      "source": [
        "# displaying the componantes and the data varience they includes for choosing the number of componnets to reduced to\n",
        "cov_mat = np.cov(train_data_rescaled_phys_no_bins.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "# plot explained variances\n",
        "plt.bar(range(1,13), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,13), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axj0jUTTgXJK"
      },
      "outputs": [],
      "source": [
        "pca_phys_no_bins = PCA(n_components=2) # choosing the number of dimentions\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca_phys_no_bins = pca_phys_no_bins.fit_transform(train_data_rescaled_phys_no_bins)\n",
        "X_test_pca_phys_no_bins = pca_phys_no_bins.transform(test_data_rescaled_phys_no_bins)\n",
        "X_batch_pca_phys_no_bins = pca_phys_no_bins.transform(batch_data_rescaled_phys_no_bins)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9iBAlWBKjyu"
      },
      "outputs": [],
      "source": [
        "# How much each original feature contribute to the components\n",
        "\n",
        "print(pd.DataFrame(np.abs(pca_phys_no_bins.components_),columns=population_data_phys_no_bins.columns ,index = ['PC-1', 'PC-2'])) # adjust the number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdqIpvWsKjyu"
      },
      "outputs": [],
      "source": [
        "# printing the most contributing original feature for each component\n",
        "most_important = np.abs(pca_phys_no_bins.components_).argmax(axis=1)\n",
        "initial_feature_names = population_data_phys_no_bins.columns\n",
        "most_important_names = initial_feature_names[most_important]\n",
        "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(2)} # adjust the number of components\n",
        "pca_results = pd.DataFrame(dic.items())\n",
        "print(pca_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "rfnjknSocdSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "CW19bGmQci1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_phys_no_bins= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_phys_no_bins,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(X_train_pca_phys_no_bins, target_phys_no_bins)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "Pqz5oyH6X9dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using the best parameters results"
      ],
      "metadata": {
        "id": "H5EwVAfmc7Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "random_forest_grouped_phys_no_bins = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 5, n_estimators = 1000, n_jobs=-1)\n",
        "my_model_RF_PCA_phys_no_bins = random_forest_grouped_phys_no_bins.fit(X_train_pca_phys_no_bins, target_phys_no_bins)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_PCA_phys_no_bins.score(X_test_pca_phys_no_bins, target_test_phys_no_bins)))"
      ],
      "metadata": {
        "id": "_zu86vHjX9da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWbV26TSgXJK"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_random_forest_phys_no_bins_PCA_batch = my_model_RF_PCA_phys_no_bins.predict(X_batch_pca_phys_no_bins)\n",
        "accuracy = accuracy_score(pred_random_forest_phys_no_bins_PCA_batch.astype(int), target_batch_phys_no_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "hPF6qS0FdUQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07rxYxtPYXXr"
      },
      "outputs": [],
      "source": [
        "xgboost_classification = XGBClassifier(n_jobs=-1)\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(X_train_pca_phys_no_bins, target_phys_no_bins)\n",
        "\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running model using the best paramaters results"
      ],
      "metadata": {
        "id": "qyIN6aD0dj8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "xgboost_classification_phys_no_bins_pca = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "my_model_xgboost_PCA_phys_no_bins = xgboost_classification_phys_no_bins_pca.fit(X_train_pca_phys_no_bins, target_phys_no_bins)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_PCA_phys_no_bins.score(X_test_pca_phys_no_bins, target_test_phys_no_bins)))"
      ],
      "metadata": {
        "id": "dAj_v6EaYXXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIZd3a5agXJL"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_xgboost_phys_no_bins_PCA_batch = my_model_xgboost_PCA_phys_no_bins.predict(X_batch_pca_phys_no_bins)\n",
        "accuracy = accuracy_score(pred_xgboost_phys_no_bins_PCA_batch.astype(int), target_batch_phys_no_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "eBXdYMM0eIIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "_DA16dVveNlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(X_train_pca_phys_no_bins, target_phys_no_bins)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "sKKtJ-jjYpKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "EPlxHlERefWO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10sCSnBcYpKY"
      },
      "outputs": [],
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "SVM_phys_no_bins = svm.SVC(C = 0.001, gamma =0.001, kernel = 'linear')\n",
        "my_model_SVM_PCA_phys_no_bins = SVM_phys_no_bins.fit(X_train_pca_phys_no_bins, target_phys_no_bins)\n",
        "print ('SVM Score: {0:2.5f}'.format(my_model_SVM_PCA_phys_no_bins.score(X_test_pca_phys_no_bins, target_test_phys_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un-kbTdbgXJL"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_SVM_phys_no_bins_PCA_batch = my_model_SVM_PCA_phys_no_bins.predict(X_batch_pca_phys_no_bins)\n",
        "accuracy = accuracy_score(pred_SVM_phys_no_bins_PCA_batch.astype(int), target_batch_phys_no_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0b3LKqJ61Bp"
      },
      "source": [
        "# Physiological - FSC only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SstqnX0c61Bq"
      },
      "outputs": [],
      "source": [
        "# setting the training dataset\n",
        "Merge_population_FSC = Merge_population_phys_no_bins.drop(['mean_SSC_A', 'mean_SSC_H', 'mean_SSC_W',\n",
        "                                               'median_SSC_A','median_SSC_H','median_SSC_W'], axis = 1)\n",
        "\n",
        "Merge_population_FSC.to_csv('H460_Merge_population_FSC.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH2A5-xk16qB"
      },
      "outputs": [],
      "source": [
        "# setting the testing dataset\n",
        "Merge_population_test_FSC = Merge_population_test_phys_no_bins.drop(['mean_SSC_A', 'mean_SSC_H', 'mean_SSC_W',\n",
        "                                               'median_SSC_A','median_SSC_H','median_SSC_W'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmu7DmPE61Bq"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the lables\n",
        "cols = [col for col in Merge_population_FSC.columns if col not in ['Cell_line']]\n",
        "population_data_FSC = Merge_population_FSC[cols] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_FSC = Merge_population_FSC['Cell_line'] # lables\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if the data includes only one sample per population\n",
        "\n",
        "## split to training and testing\n",
        "#X_train, X_test, y_train, y_test = train_test_split(All_data_train, target, test_size=0.3, random_state=42) # make sure to change names as needed"
      ],
      "metadata": {
        "id": "NYD1OK3_v2c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGIfSNjI2IvC"
      },
      "outputs": [],
      "source": [
        "# do not run if the data includes only one sample per population\n",
        "\n",
        "# separating the testing features from the lables\n",
        "cols_test = [col for col in Merge_population_test_FSC.columns if col not in ['Cell_line']]\n",
        "population_data_test_FSC = Merge_population_test_FSC[cols_test] # features\n",
        "\n",
        "#assigning the Oppurtunity Result column as target\n",
        "target_test_FSC = Merge_population_test_FSC['Cell_line'] # labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "tO4c_5fKiRtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "3WNzYubtidXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_FSC= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_FSC,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(population_data_FSC, target_FSC)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "WndYC9R9txmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "JaGEhTJqi1zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "random_forest_grouped_FSC = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "scores = cross_val_score(random_forest_grouped_FSC, population_data_FSC, target_FSC, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validataion Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "PFPevDnDtxmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iMzh8Mp2Xle"
      },
      "outputs": [],
      "source": [
        "# fitting the model fon all the training data\n",
        "my_model_RF_grouped_FSC = random_forest_grouped_FSC.fit(population_data_FSC, target_FSC)\n",
        "print ('Random Forest training Score: {0:2.5f}'.format(my_model_RF_grouped_FSC.score(population_data_FSC, target_FSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmBOEKje61Bq"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_RF_grouped_FSC = my_model_RF_grouped_FSC.predict(population_data_test_FSC)\n",
        "heatconmat(target_test_FSC,pred_RF_grouped_FSC)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_FSC.score(population_data_test_FSC, target_test_FSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_B1kqgB61Bq"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find feature importance\n",
        "perm = PermutationImportance(my_model_RF_grouped_FSC).fit(population_data_FSC, target_FSC)\n",
        "eli5.show_weights(perm, feature_names = population_data_FSC.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_rf_FSC_500' # name the model\n",
        "pickle.dump(my_model_RF_grouped_FSC, open(data_path + model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "L52D9ggMsN76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "jH7eHUl7jsDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "CF30e5r1jw6I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhNSpV7csN77"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_FSC = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_FSC,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(population_data_FSC,target_FSC)\n",
        "\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "xgboost_classification_FSC = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "scores = cross_val_score(xgboost_classification_FSC, population_data_FSC, target_FSC, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "pqIdbBKTsN78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FMnh26E61Br"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_xgboost_grouped_FSC = xgboost_classification.fit(population_data_FSC, target_FSC)\n",
        "print ('xgboost training Score: {0:2.5f}'.format(my_model_xgboost_grouped_FSC.score(population_data_FSC, target_FSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x91t9ng561Br"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost_FSC = my_model_xgboost_grouped_FSC.predict(population_data_test_FSC)\n",
        "heatconmat(target_test_FSC,pred_xgboost_FSC)\n",
        "print('xgboost testing\n",
        "Score: {0:2.5f}'.format(my_model_xgboost_grouped_FSC.score(population_data_test_FSC, target_test_FSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ZPdeoc61Bs"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_xgboost_FSC = PermutationImportance(my_model_xgboost_grouped_FSC, random_state=1).fit(population_data_FSC, target_FSC)\n",
        "eli5.show_weights(perm_xgboost_FSC, feature_names = population_data_FSC.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_xgboost_FSC_500' # name the model\n",
        "pickle.dump(my_model_xgboost_grouped_FSC, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "dtrEi1kPuKdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "Uf8LQ4Askys1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "6hw0Iggqk2SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(population_data_FSC, target_FSC)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "ByuFe_uFs9DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the models using best parameters results"
      ],
      "metadata": {
        "id": "KdyAkusSlJKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prbqX28R61Bs"
      },
      "outputs": [],
      "source": [
        "# cross validation on the training data\n",
        "SVM_FSC = svm.SVC(C = 0.001, gamma = 0.1, kernel='poly')\n",
        "scores = cross_val_score(SVM_FSC, population_data_FSC, target_FSC, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_vYVspl61Bs"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_SVM_grouped_FSC = SVM_FSC.fit(population_data_FSC, target_FSC)\n",
        "print ('SVM training Score: {0:2.5f}'.format(my_model_SVM_grouped_FSC.score(population_data_FSC, target_FSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeoU7CHp3OdZ"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM_FSC = my_model_SVM_grouped_FSC.predict(population_data_test_FSC)\n",
        "heatconmat(target_test_FSC,pred_SVM_FSC)\n",
        "print ('SVM testing Score: {0:2.5f}'.format(my_model_SVM_grouped_FSC.score(population_data_test_FSC, target_test_FSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-76Y49gn61Bt"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_SVM = PermutationImportance(my_model_SVM_grouped_FSC, random_state=1).fit(population_data_FSC, target_FSC)\n",
        "eli5.show_weights(perm_SVM, feature_names = population_data_FSC.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_SVM_FSC_500' # name the model\n",
        "pickle.dump(my_model_SVM_grouped_FSC, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "aJRqPw8o3FRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Godwyn8J61Bt"
      },
      "source": [
        "# new experiment testing - Batch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHnqJ2_-61Bt"
      },
      "outputs": [],
      "source": [
        "# reading the batch data\n",
        "Merge_population_batch_FSC = pd.read_csv(data_path +'Merge_population_H460_test_FSC_500.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYl0cJ8J61Bt"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the lables\n",
        "cols_batch = [col for col in Merge_population_batch_FSC.columns if col not in ['Cell_line']]\n",
        "population_data_FSC_batch = Merge_population_batch_FSC[cols] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_batch_FSC = Merge_population_batch_FSC['Cell_line'] # labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "gfNbdXE8tP9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7od-D_3w61Bt"
      },
      "outputs": [],
      "source": [
        "pred_batch_FSC_rf = my_model_RF_grouped_FSC.predict(population_data_FSC_batch)\n",
        "heatconmat(target_batch_FSC,pred_batch_FSC_rf)\n",
        "accuracy = accuracy_score(pred_batch_FSC_rf.astype(int), target_batch_FSC.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "gZT_GUm7teak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIpEb9hQ61Bt"
      },
      "outputs": [],
      "source": [
        "pred_batch_FSC_xgboost = my_model_xgboost_grouped_FSC.predict(population_data_FSC_batch)\n",
        "heatconmat(target_batch_FSC,pred_batch_FSC_xgboost)\n",
        "accuracy = accuracy_score(pred_batch_FSC_xgboost.astype(int), target_batch_FSC.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "8fo7cC0ztt02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH8RSgi161Bu"
      },
      "outputs": [],
      "source": [
        "pred_batch_FSC_SVM = my_model_SVM_grouped_FSC.predict(population_data_FSC_batch)\n",
        "heatconmat(target_batch_FSC,pred_batch_FSC_SVM)\n",
        "accuracy = accuracy_score(pred_batch_FSC_SVM.astype(int), target_batch_FSC.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimention reduction - PCA"
      ],
      "metadata": {
        "id": "6PCUh6_ft9YP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfGSDE3djxmo"
      },
      "outputs": [],
      "source": [
        "# rescale the data for pca\n",
        "train_data_rescaled_FSC = scaler.fit_transform(population_data_FSC)\n",
        "test_data_rescaled_FSC = scaler.fit_transform(population_data_test_FSC)\n",
        "batch_data_rescaled_FSC = scaler.fit_transform(population_data_FSC_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYrN_3zGjxmp"
      },
      "outputs": [],
      "source": [
        "# displaying the componantes and the data varience they includes for choosing the number of componnets to reduced to\n",
        "cov_mat = np.cov(train_data_rescaled_FSC.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "# plot explained variances\n",
        "plt.bar(range(1,7), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,7), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAPk6fmljxmp"
      },
      "outputs": [],
      "source": [
        "pca_FSC = PCA(n_components=2) # choosing the number of dimentions\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca_FSC = pca_FSC.fit_transform(train_data_rescaled_FSC)\n",
        "X_test_pca_FSC = pca_FSC.transform(test_data_rescaled_FSC)\n",
        "X_batch_pca_FSC = pca_FSC.transform(batch_data_rescaled_FSC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8QLVE4FLAr3"
      },
      "outputs": [],
      "source": [
        "# How much each original feature contribute to the components\n",
        "\n",
        "print(pd.DataFrame(np.abs(pca_FSC.components_),columns=population_data_FSC.columns ,index = ['PC-1', 'PC-2'])) # adjust the number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUAecSvbLAr4"
      },
      "outputs": [],
      "source": [
        "# printing the most contributing original feature for each component\n",
        "most_important = np.abs(pca_FSC.components_).argmax(axis=1)\n",
        "initial_feature_names = population_data_FSC.columns\n",
        "most_important_names = initial_feature_names[most_important]\n",
        "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(2)} # adjust the number of components\n",
        "pca_results = pd.DataFrame(dic.items())\n",
        "print(pca_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "QumKQaPvuWtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "1ztw1k2cuap6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_FSC_pca= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_FSC_pca,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(X_train_pca_FSC, target_FSC)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "BW1Tibm73aOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running the model using best parameters results"
      ],
      "metadata": {
        "id": "SD9ZjB6Mu4t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "random_forest_grouped_FSC_pca = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "my_model_RF_PCA_FSC = random_forest_grouped_FSC_pca.fit(X_train_pca_FSC, target_FSC)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_PCA_FSC.score(X_test_pca_FSC, target_test_FSC)))"
      ],
      "metadata": {
        "id": "jvsInBEN3aOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvajyW54jxmq"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_random_forest_FSC_PCA_batch = my_model_RF_PCA_FSC.predict(X_batch_pca_FSC)\n",
        "accuracy = accuracy_score(pred_random_forest_FSC_PCA_batch.astype(int), target_batch_FSC.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "n_zrFIeYvTHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "iWqwZD4rvWo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH0xYwiz6hbh"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_FSC_pca = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_FSC_pca,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(X_train_pca_FSC, target_FSC)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using the best parameters results"
      ],
      "metadata": {
        "id": "bvmdiEMlvw9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "xgboost_classification_FSC_pca = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.75, n_jobs=-1)\n",
        "my_model_xgboost_PCA_FSC = xgboost_classification_FSC_pca.fit(X_train_pca_FSC, target_FSC)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_PCA_FSC.score(X_test_pca_FSC, target_test_FSC)))"
      ],
      "metadata": {
        "id": "utvVSkH86hbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur9YINSljxmq"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_xgboost_FSC_PCA_batch = my_model_xgboost_PCA_FSC.predict(X_batch_pca_FSC)\n",
        "accuracy = accuracy_score(pred_xgboost_FSC_PCA_batch.astype(int), target_batch_FSC.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "fCtCv3uCvp3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "LMZsDLzQwnjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(X_train_pca_FSC, target_FSC)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "yjFa0kG17Zd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "x9xjoZ8ZwfAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "SVM_FSC = svm.SVC(kernel='linear')\n",
        "my_model_SVM_PCA_FSC = SVM_FSC.fit(X_train_pca_FSC, target_FSC)\n",
        "print ('SVM Score: {0:2.5f}'.format(my_model_SVM_PCA_FSC.score(X_test_pca_FSC, target_test_FSC)))"
      ],
      "metadata": {
        "id": "SpJPAHK771Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkjloRgTjxmr"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_SVM_FSC_PCA_batch = my_model_SVM_PCA_FSC.predict(X_batch_pca_FSC)\n",
        "accuracy = accuracy_score(pred_SVM_FSC_PCA_batch.astype(int), target_batch_FSC.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Physiological- Only SSC features"
      ],
      "metadata": {
        "id": "mYXDIlgRxC_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzCzn7Ik61Bw"
      },
      "outputs": [],
      "source": [
        "# setting the training dataset\n",
        "Merge_population_SSC = Merge_population_phys_no_bins.drop(['mean_FSC_A', 'mean_FSC_H', 'mean_FSC_W',\n",
        "                                               'median_FSC_A','median_FSC_H','median_FSC_W'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBbBPJ-76Fic"
      },
      "outputs": [],
      "source": [
        "# setting the testing data set\n",
        "Merge_population_test_SSC = Merge_population_test_phys_no_bins.drop(['mean_FSC_A', 'mean_FSC_H', 'mean_FSC_W',\n",
        "                                               'median_FSC_A','median_FSC_H','median_FSC_W'], axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LJmY5hW61Bw"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the labels\n",
        "cols = [col for col in Merge_population_SSC.columns if col not in ['Cell_line']]\n",
        "population_data_SSC = Merge_population_SSC[cols] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_SSC = Merge_population_SSC['Cell_line'] # labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if the data includes only one sample per population\n",
        "\n",
        "## split to training and testing\n",
        "#X_train, X_test, y_train, y_test = train_test_split(All_data_train, target, test_size=0.3, random_state=42) # make sure to change names as needed"
      ],
      "metadata": {
        "id": "1SMiVSBIwlry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaLunFab6Qgs"
      },
      "outputs": [],
      "source": [
        "# do not run if the data includes only one sample per population\n",
        "\n",
        "# separating the testing features from the labels\n",
        "cols_test = [col for col in Merge_population_test_SSC.columns if col not in ['Cell_line']]\n",
        "population_data_test_SSC = Merge_population_test_SSC[cols_test] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_test_SSC = Merge_population_test_SSC['Cell_line'] # labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "jYd8X0SLx1V6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "uzq05s7yx5BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_SSC= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_SSC,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(population_data_SSC, target_SSC)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "9xnYxi-F8Tp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "kbX8_xq9ziY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "random_forest_grouped_SSC = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "scores = cross_val_score(random_forest_grouped_SSC, population_data_SSC, target_SSC, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Croos validation  Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "u_0ejwfN8Tp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9cctito6c-O"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_RF_grouped_SSC = random_forest_grouped_SSC.fit(population_data_SSC, target_SSC)\n",
        "print ('Random Forest training Score: {0:2.5f}'.format(my_model_RF_grouped_SSC.score(population_data_SSC, target_SSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zWQH9DW61Bx"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_RF_grouped_SSC = my_model_RF_grouped_SSC.predict(population_data_test_SSC)\n",
        "heatconmat(target_test_SSC,pred_RF_grouped_SSC)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_SSC.score(population_data_test_SSC, target_test_SSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-In0G6q61Bx"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm = PermutationImportance(my_model_RF_grouped_SSC).fit(population_data_SSC, target_SSC)\n",
        "eli5.show_weights(perm, feature_names = population_data_SSC.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_rf_SSC_500' # name the model\n",
        "pickle.dump(my_model_RF_grouped_SSC, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "vzR9x6K7C10M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "Ks0ujgJPHcIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "0KAgpJ0UH9F9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leXNGxQhC10N"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_SSC = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_SSC,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(population_data_SSC,target_SSC)\n",
        "\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "bYOA5tteIRiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "xgboost_classification_SSC = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "scores = cross_val_score(xgboost_classification_SSC, population_data_SSC, target_SSC, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "o4EtDM1jC10P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvN6_16o61Bx"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_xgboost_grouped_SSC = xgboost_classification_SSC.fit(population_data_SSC, target_SSC)\n",
        "print('xgboost training Score: {0:2.5f}'.format(my_model_xgboost_grouped_SSC.score(population_data_SSC, target_SSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVeFZ4dJ61Bx"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost_SSC = my_model_xgboost_grouped_SSC.predict(population_data_test_SSC)\n",
        "heatconmat(target_test_SSC,pred_xgboost_SSC)\n",
        "print('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_grouped_SSC.score(population_data_test_SSC, target_test_SSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA1t38X361Bx"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find feature importance\n",
        "perm_xgboost_SSC = PermutationImportance(my_model_xgboost_grouped_SSC, random_state=1).fit(population_data_SSC, target_SSC)\n",
        "eli5.show_weights(perm_xgboost_SSC, feature_names = population_data_SSC.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_xgboost_SSC_500' # name the model\n",
        "pickle.dump(my_model_xgboost_grouped_SSC, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "t7sdlA_gAJBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "t_qiBDrVJUkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "6L8TjX37JXrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrPGQsss61Bx"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(population_data_SSC, target_SSC)\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model with the best parameters results"
      ],
      "metadata": {
        "id": "s3GhU0LBJyXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "SVM_SSC = svm.SVC(C = 0.001, gamma = 0.01, kernel='poly')\n",
        "scores = cross_val_score(SVM_SSC, population_data_SSC, target_SSC, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "a8RXV76Q5Mg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgRHS2bb61By"
      },
      "outputs": [],
      "source": [
        "# fitting the data on all the training data\n",
        "my_model_SVM_grouped_SSC = SVM_SSC.fit(population_data_SSC, target_SSC)\n",
        "print('SVM training Score: {0:2.5f}'.format(my_model_SVM_grouped_SSC.score(population_data_SSC, target_SSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx1m5rzk61B1"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM_SSC = my_model_SVM_grouped_SSC.predict(population_data_test_SSC)\n",
        "heatconmat(target_test_SSC,pred_SVM_SSC)\n",
        "print('SVM Score: {0:2.5f}'.format(my_model_SVM_grouped_SSC.score(population_data_test_SSC, target_test_SSC)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYzed3JZ61B2"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_SVM = PermutationImportance(my_model_SVM_grouped_SSC, random_state=1).fit(population_data_SSC, target_SSC)\n",
        "eli5.show_weights(perm_SVM, feature_names = population_data_SSC.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_SVM_SSC_500' # name the model\n",
        "pickle.dump(my_model_SVM_grouped_SSC, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "7sQ5fkey40-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opening the saved models\n",
        "\n",
        "# RF\n",
        "model_name = 'H460_groupkfold_populations_rf_SSC_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_rf_SSC = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# xgboost\n",
        "model_name = 'H460_groupkfold_populations_xgboost_SSC_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_xgboost_SSC = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# SVM\n",
        "model_name = 'H460_groupkfold_populations_SVM_SSC_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_SVM_SSC = pickle.load(a_file)\n",
        "a_file.close()"
      ],
      "metadata": {
        "id": "SHimMpQk50B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#models performance evaluation on new unseen data using the Batch dataset"
      ],
      "metadata": {
        "id": "rM-CQR24LsZn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZEiYEOU61B2"
      },
      "outputs": [],
      "source": [
        "# reading the Batch data (for thhis example batch data was already preprocessed)\n",
        "Merge_population_batch_SSC = pd.read_csv(data_path +'Merge_population_H460_test_SSC_500.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKBVHpAi61B2"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the labels\n",
        "cols_batch = [col for col in Merge_population_batch_SSC.columns if col not in ['Cell_line']]\n",
        "population_data_SSC_batch = Merge_population_batch_SSC[cols_batch]# features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_batch_SSC = Merge_population_batch_SSC['Cell_line'] # labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest performance"
      ],
      "metadata": {
        "id": "vbQXOx7hMOfr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT2mU2W_61B2"
      },
      "outputs": [],
      "source": [
        "pred_batch_SSC_rf = my_model_population_rf_SSC.predict(population_data_SSC_batch)\n",
        "heatconmat(target_batch_SSC,pred_batch_SSC_rf)\n",
        "accuracy = accuracy_score(pred_batch_SSC_rf.astype(int), target_batch_SSC.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost performance"
      ],
      "metadata": {
        "id": "SaoPsg-3Mldn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nptx7Zw861B2"
      },
      "outputs": [],
      "source": [
        "pred_batch_SSC_xgboost = my_model_population_xgboost_SSC.predict(population_data_SSC_batch)\n",
        "heatconmat(target_batch_SSC,pred_batch_SSC_xgboost)\n",
        "accuracy = accuracy_score(pred_batch_SSC_xgboost.astype(int), target_batch_SSC.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM performance"
      ],
      "metadata": {
        "id": "sxbJKuvgMrtE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03FJLr-F61B3"
      },
      "outputs": [],
      "source": [
        "pred_batch_SSC_SVM = my_model_population_SVM_SSC.predict(population_data_SSC_batch)\n",
        "heatconmat(target_batch_SSC,pred_batch_SSC_SVM)\n",
        "accuracy = accuracy_score(pred_batch_SSC_SVM.astype(int), target_batch_SSC.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimention reduction - PCA"
      ],
      "metadata": {
        "id": "qVA7Z369NTdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing the transformed data"
      ],
      "metadata": {
        "id": "5gQpql2DN1Jy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qqc8aSImQ31"
      },
      "outputs": [],
      "source": [
        "# rescale the data\n",
        "train_data_rescaled_SSC = scaler.fit_transform(population_data_SSC)\n",
        "test_data_rescaled_SSC = scaler.fit_transform(population_data_test_SSC)\n",
        "batch_data_rescaled_SSC = scaler.fit_transform(population_data_SSC_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TjYjbaImQ31"
      },
      "outputs": [],
      "source": [
        "# displaying the componantes and the data varience they includes for choosing the number of componnets to reduced to\n",
        "cov_mat = np.cov(train_data_rescaled_SSC.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "# plot explained variances\n",
        "plt.bar(range(1,7), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,7), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp9U3WkbmQ32"
      },
      "outputs": [],
      "source": [
        "pca_SSC = PCA(n_components=2) # choosing the number of dimentions\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca_SSC = pca_SSC.fit_transform(train_data_rescaled_SSC)\n",
        "X_test_pca_SSC = pca_SSC.transform(test_data_rescaled_SSC)\n",
        "X_batch_pca_SSC = pca_SSC.transform(batch_data_rescaled_SSC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNwpOMhTNJQf"
      },
      "outputs": [],
      "source": [
        "# How much each original feature contribute to the components\n",
        "\n",
        "print(pd.DataFrame(np.abs(pca_SSC.components_),columns=population_data_SSC.columns ,index = ['PC-1', 'PC-2'])) # adjust the number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhm2eU55NJQf"
      },
      "outputs": [],
      "source": [
        "# printing the most contributing original feature for each component\n",
        "most_important = np.abs(pca_SSC.components_).argmax(axis=1)\n",
        "initial_feature_names = population_data_SSC.columns\n",
        "most_important_names = initial_feature_names[most_important]\n",
        "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(2)} # adjust the number of components\n",
        "pca_results = pd.DataFrame(dic.items())\n",
        "print(pca_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random fores"
      ],
      "metadata": {
        "id": "285j3vBsOINm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "LKfnX4X3OKLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_SSC_pca = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [500, 1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_SSC_pca,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(X_train_pca_SSC, target_SSC)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "BA-cDUei7h_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "random_forest_SSC_pca = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators =500, n_jobs=-1)\n",
        "my_model_RF_PCA_SSC = random_forest_SSC_pca.fit(X_train_pca_SSC, target_SSC)\n",
        "print('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_PCA_SSC.score(X_test_pca_SSC, target_test_SSC)))"
      ],
      "metadata": {
        "id": "3DL74poE7v6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMOFa3bfmQ32"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_random_forest_SSC_PCA_batch = my_model_RF_PCA_SSC.predict(X_batch_pca_SSC)\n",
        "accuracy = accuracy_score(pred_random_forest_SSC_PCA_batch.astype(int), target_batch_SSC.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "edO0mZxMPTT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameteres optimization"
      ],
      "metadata": {
        "id": "0WhmGh5QPugM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJCS5kUsJ-5O"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_SSC_pca = XGBClassifier(n_jobs=-1)\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_SSC_pca,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(X_train_pca_SSC, target_SSC)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "xgboost_classification_SSC_pca = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "my_model_xgboost_PCA_SSC = xgboost_classification_SSC_pca.fit(X_train_pca_SSC, target_SSC)\n",
        "print('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_PCA_SSC.score(X_test_pca_SSC, target_test_SSC)))"
      ],
      "metadata": {
        "id": "jcILTPEZJ-5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTypBHepmQ32"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_xgboost_SSC_PCA_batch = my_model_xgboost_PCA_SSC.predict(X_batch_pca_SSC)\n",
        "accuracy = accuracy_score(pred_xgboost_SSC_PCA_batch.astype(int), target_batch_SSC.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "f8t92DfVQIud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "i2HOaDivQTQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(X_train_pca_SSC, target_SSC)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "0R3qOPAQM1lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "SVM_SSC = svm.SVC(kernel='linear')\n",
        "my_model_SVM_PCA_SSC = SVM_SSC.fit(X_train_pca_SSC, target_SSC)\n",
        "print ('SVM testing Score: {0:2.5f}'.format(my_model_SVM_PCA_SSC.score(X_test_pca_SSC, target_test_SSC)))"
      ],
      "metadata": {
        "id": "vewmpD-3NDpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUsW46WymQ32"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_SVM_SSC_PCA_batch = my_model_SVM_PCA_SSC.predict(X_batch_pca_SSC)\n",
        "accuracy = accuracy_score(pred_SVM_SSC_PCA_batch.astype(int), target_batch_SSC.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uptake features without bins only"
      ],
      "metadata": {
        "id": "prmdimawRQv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing the data"
      ],
      "metadata": {
        "id": "IbgAwyPtRvlZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jYMfXBeBSYH"
      },
      "outputs": [],
      "source": [
        "# train dfs\n",
        "Total_pop_1_uptake_no_bins = prefeatured_pop_1\n",
        "Total_pop_2_uptake_no_bins = prefeatured_pop_2\n",
        "\n",
        "# test dfs (scilence if there is one sample per population only )\n",
        "Total_pop_1_test_uptake_no_bins = prefeatured_pop_1_test\n",
        "Total_pop_2_test_uptake_no_bins = prefeatured_pop_2_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data"
      ],
      "metadata": {
        "id": "ICvT83IQR3e_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1h8Ic0u61B-"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 1 train uptake values\n",
        "grouper_pop_1_uptake_no_bins = Total_pop_1_uptake_no_bins.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "BV421_A_mean = grouper_pop_1_uptake_no_bins['BV421_A'].mean().to_frame(name='mean_BV421_A').reset_index()\n",
        "BV421_A_median = grouper_pop_1_uptake_no_bins['BV421_A'].median().to_frame(name='median_BV421_A').reset_index()\n",
        "\n",
        "FITC_A_mean = grouper_pop_1_uptake_no_bins['FITC_A'].mean().to_frame(name='mean_FITC_A').reset_index()\n",
        "FITC_A_median = grouper_pop_1_uptake_no_bins['FITC_A'].median().to_frame(name='median_FITC_A').reset_index()\n",
        "\n",
        "PerCP_A_mean = grouper_pop_1_uptake_no_bins['PerCP_A'].mean().to_frame(name='mean_PerCP_A').reset_index()\n",
        "PerCP_A_median = grouper_pop_1_uptake_no_bins['PerCP_A'].median().to_frame(name='median_PerCP_A').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_mean = grouper_pop_1_uptake_no_bins['PE_Texas_Red_A'].mean().to_frame(name='mean_PE_Texas_Red_A').reset_index()\n",
        "PE_Texas_Red_A_median = grouper_pop_1_uptake_no_bins['PE_Texas_Red_A'].median().to_frame(name='median_PE_Texas_Red_A').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_mean = grouper_pop_1_uptake_no_bins['Alexa_Fluor_700_A'].mean().to_frame(name='mean_Alexa_Fluor_700_A').reset_index()\n",
        "Alexa_Fluor_700_A_median = grouper_pop_1_uptake_no_bins['Alexa_Fluor_700_A'].median().to_frame(name='median_Alexa_Fluor_700_A').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfyS-6ae61B-"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "grouper_pop_1_uptake_no_bins = [BV421_A_mean,BV421_A_median, FITC_A_mean, FITC_A_median, PerCP_A_mean, PerCP_A_median ,\n",
        "                          PE_Texas_Red_A_mean, PE_Texas_Red_A_median, Alexa_Fluor_700_A_mean, Alexa_Fluor_700_A_median]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLioSZ5l61B-"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_1_log_grouped_uptake_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), grouper_pop_1_uptake_no_bins)\n",
        "pop_1_log_grouped_uptake_no_bins['Cell_line'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtHJBdc261B_"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 2 train uptake values\n",
        "grouper_pop_2_uptake_no_bins = Total_pop_2_uptake_no_bins.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "BV421_A_mean = grouper_pop_2_uptake_no_bins['BV421_A'].mean().to_frame(name='mean_BV421_A').reset_index()\n",
        "BV421_A_median = grouper_pop_2_uptake_no_bins['BV421_A'].median().to_frame(name='median_BV421_A').reset_index()\n",
        "\n",
        "FITC_A_mean = grouper_pop_2_uptake_no_bins['FITC_A'].mean().to_frame(name='mean_FITC_A').reset_index()\n",
        "FITC_A_median = grouper_pop_2_uptake_no_bins['FITC_A'].median().to_frame(name='median_FITC_A').reset_index()\n",
        "\n",
        "PerCP_A_mean = grouper_pop_2_uptake_no_bins['PerCP_A'].mean().to_frame(name='mean_PerCP_A').reset_index()\n",
        "PerCP_A_median = grouper_pop_2_uptake_no_bins['PerCP_A'].median().to_frame(name='median_PerCP_A').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_mean = grouper_pop_2_uptake_no_bins['PE_Texas_Red_A'].mean().to_frame(name='mean_PE_Texas_Red_A').reset_index()\n",
        "PE_Texas_Red_A_median = grouper_pop_2_uptake_no_bins['PE_Texas_Red_A'].median().to_frame(name='median_PE_Texas_Red_A').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_mean = grouper_pop_2_uptake_no_bins['Alexa_Fluor_700_A'].mean().to_frame(name='mean_Alexa_Fluor_700_A').reset_index()\n",
        "Alexa_Fluor_700_A_median = grouper_pop_2_uptake_no_bins['Alexa_Fluor_700_A'].median().to_frame(name='median_Alexa_Fluor_700_A').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrrTskeo61B_"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "grouper_pop_2_uptake_no_bins = [BV421_A_mean,BV421_A_median, FITC_A_mean, FITC_A_median, PerCP_A_mean, PerCP_A_median ,\n",
        "                          PE_Texas_Red_A_mean, PE_Texas_Red_A_median, Alexa_Fluor_700_A_mean, Alexa_Fluor_700_A_median]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFMURcrM61B_"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_2_log_grouped_uptake_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), grouper_pop_2_uptake_no_bins)\n",
        "pop_2_log_grouped_uptake_no_bins['Cell_line'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XouTeZ361B_"
      },
      "outputs": [],
      "source": [
        "# merging the training data of the two populations\n",
        "populations_df_list = [pop_1_log_grouped_uptake_no_bins, pop_2_log_grouped_uptake_no_bins]\n",
        "Merge_population_uptake_no_bins = pd.DataFrame()\n",
        "for df in populations_df_list:\n",
        "    Merge_population_uptake_no_bins = pd.concat([Merge_population_uptake_no_bins, df], ignore_index=True)\n",
        "\n",
        "Merge_population_uptake_no_bins.drop('population_number', axis = 1, inplace = True)\n",
        "Merge_population_uptake_no_bins.to_csv('Merge_population_H460_uptake_no_bins.csv') # name the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing data"
      ],
      "metadata": {
        "id": "k40LynO2Wa4T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc_0lfrKEAFe"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 1 test uptake values\n",
        "grouper_pop_1_test_uptake_no_bins = Total_pop_1_test_uptake_no_bins.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "BV421_A_mean = grouper_pop_1_test_uptake_no_bins['BV421_A'].mean().to_frame(name='mean_BV421_A').reset_index()\n",
        "BV421_A_median = grouper_pop_1_test_uptake_no_bins['BV421_A'].median().to_frame(name='median_BV421_A').reset_index()\n",
        "\n",
        "FITC_A_mean = grouper_pop_1_test_uptake_no_bins['FITC_A'].mean().to_frame(name='mean_FITC_A').reset_index()\n",
        "FITC_A_median = grouper_pop_1_test_uptake_no_bins['FITC_A'].median().to_frame(name='median_FITC_A').reset_index()\n",
        "\n",
        "PerCP_A_mean = grouper_pop_1_test_uptake_no_bins['PerCP_A'].mean().to_frame(name='mean_PerCP_A').reset_index()\n",
        "PerCP_A_median = grouper_pop_1_test_uptake_no_bins['PerCP_A'].median().to_frame(name='median_PerCP_A').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_mean = grouper_pop_1_test_uptake_no_bins['PE_Texas_Red_A'].mean().to_frame(name='mean_PE_Texas_Red_A').reset_index()\n",
        "PE_Texas_Red_A_median = grouper_pop_1_test_uptake_no_bins['PE_Texas_Red_A'].median().to_frame(name='median_PE_Texas_Red_A').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_mean = grouper_pop_1_test_uptake_no_bins['Alexa_Fluor_700_A'].mean().to_frame(name='mean_Alexa_Fluor_700_A').reset_index()\n",
        "Alexa_Fluor_700_A_median = grouper_pop_1_test_uptake_no_bins['Alexa_Fluor_700_A'].median().to_frame(name='median_Alexa_Fluor_700_A').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGAh1HWlEAFf"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "grouper_pop_1_test_uptake_no_bins = [BV421_A_mean,BV421_A_median, FITC_A_mean, FITC_A_median, PerCP_A_mean, PerCP_A_median ,\n",
        "                          PE_Texas_Red_A_mean, PE_Texas_Red_A_median, Alexa_Fluor_700_A_mean, Alexa_Fluor_700_A_median]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwXhTzvuEAFf"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_1_test_log_grouped_uptake_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), grouper_pop_1_test_uptake_no_bins)\n",
        "pop_1_test_log_grouped_uptake_no_bins['Cell_line'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmow_A-xEAFf"
      },
      "outputs": [],
      "source": [
        "# grouping mean and median pop 2 test uptake values\n",
        "grouper_pop_2_test_uptake_no_bins = Total_pop_2_test_uptake_no_bins.groupby(pd.Grouper(key='population_number'))\n",
        "\n",
        "BV421_A_mean = grouper_pop_2_test_uptake_no_bins['BV421_A'].mean().to_frame(name='mean_BV421_A').reset_index()\n",
        "BV421_A_median = grouper_pop_2_test_uptake_no_bins['BV421_A'].median().to_frame(name='median_BV421_A').reset_index()\n",
        "\n",
        "FITC_A_mean = grouper_pop_2_test_uptake_no_bins['FITC_A'].mean().to_frame(name='mean_FITC_A').reset_index()\n",
        "FITC_A_median = grouper_pop_2_test_uptake_no_bins['FITC_A'].median().to_frame(name='median_FITC_A').reset_index()\n",
        "\n",
        "PerCP_A_mean = grouper_pop_2_test_uptake_no_bins['PerCP_A'].mean().to_frame(name='mean_PerCP_A').reset_index()\n",
        "PerCP_A_median = grouper_pop_2_test_uptake_no_bins['PerCP_A'].median().to_frame(name='median_PerCP_A').reset_index()\n",
        "\n",
        "PE_Texas_Red_A_mean = grouper_pop_2_test_uptake_no_bins['PE_Texas_Red_A'].mean().to_frame(name='mean_PE_Texas_Red_A').reset_index()\n",
        "PE_Texas_Red_A_median = grouper_pop_2_test_uptake_no_bins['PE_Texas_Red_A'].median().to_frame(name='median_PE_Texas_Red_A').reset_index()\n",
        "\n",
        "Alexa_Fluor_700_A_mean = grouper_pop_2_test_uptake_no_bins['Alexa_Fluor_700_A'].mean().to_frame(name='mean_Alexa_Fluor_700_A').reset_index()\n",
        "Alexa_Fluor_700_A_median = grouper_pop_2_test_uptake_no_bins['Alexa_Fluor_700_A'].median().to_frame(name='median_Alexa_Fluor_700_A').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYHjVRb0EAFg"
      },
      "outputs": [],
      "source": [
        "# setting a new array of the grouping data\n",
        "grouper_pop_2_test_uptake_no_bins = [BV421_A_mean,BV421_A_median, FITC_A_mean, FITC_A_median, PerCP_A_mean, PerCP_A_median ,\n",
        "                          PE_Texas_Red_A_mean, PE_Texas_Red_A_median, Alexa_Fluor_700_A_mean, Alexa_Fluor_700_A_median]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvroPfsvEAFg"
      },
      "outputs": [],
      "source": [
        "# building a new df of the grouped data\n",
        "pop_2_test_log_grouped_uptake_no_bins = reduce(lambda left, right: pd.merge(left, right, on='population_number'), grouper_pop_2_test_uptake_no_bins)\n",
        "pop_2_test_log_grouped_uptake_no_bins['Cell_line'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ryeW7aEoqj"
      },
      "outputs": [],
      "source": [
        "# merging the testing data of the two populations\n",
        "populations_df_list_test = [pop_1_test_log_grouped_uptake_no_bins, pop_2_test_log_grouped_uptake_no_bins]\n",
        "Merge_population_test_uptake_no_bins = pd.DataFrame()\n",
        "for df in populations_df_list_test:\n",
        "    Merge_population_test_uptake_no_bins = pd.concat([Merge_population_test_uptake_no_bins, df], ignore_index=True)\n",
        "\n",
        "Merge_population_test_uptake_no_bins.drop('population_number', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the models"
      ],
      "metadata": {
        "id": "l9y07t2sXhL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFK-lMP_61CA"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the labels\n",
        "cols = [col for col in Merge_population_uptake_no_bins.columns if col not in ['Cell_line']]\n",
        "population_data_uptake_no_bins = Merge_population_uptake_no_bins[cols] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_uptake_no_bins = Merge_population_uptake_no_bins['Cell_line'] # labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if the data includes only one sample per population\n",
        "\n",
        "## split to training and testing\n",
        "#X_train, X_test, y_train, y_test = train_test_split(All_data_train, target, test_size=0.3, random_state=42) # make sure to change names as needed"
      ],
      "metadata": {
        "id": "2N2PFsevzbx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-kBYFz-61CA"
      },
      "outputs": [],
      "source": [
        "# do not run if the data includes only one sample per population\n",
        "\n",
        "# separating the testing features from the labels\n",
        "cols_test = [col for col in Merge_population_test_uptake_no_bins.columns if col not in ['Cell_line']]\n",
        "population_data_test_uptake_no_bins = Merge_population_test_uptake_no_bins[cols_test] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_test_uptake_no_bins = Merge_population_test_uptake_no_bins['Cell_line'] # labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "7jMC6NpFX6aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "IUP6dHyXYFwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_uptake_no_bins= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_uptake_no_bins,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "r6UHYmN1S0Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "MIhLyupLd2Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "random_forest_grouped_uptake_no_bins = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "scores = cross_val_score(random_forest_grouped_uptake_no_bins, population_data_uptake_no_bins, target_uptake_no_bins, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "HPuHmhHcS0Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmPX6jagFT0X"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_RF_grouped_uptake_no_bins = random_forest_grouped_uptake_no_bins.fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "print('Random Forest trainin gScore: {0:2.5f}'.format(my_model_RF_grouped_uptake_no_bins.score(population_data_uptake_no_bins, target_uptake_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ot3v8Ld61CA"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_RF_grouped_uptake_no_bins = my_model_RF_grouped_uptake_no_bins.predict(population_data_test_uptake_no_bins)\n",
        "heatconmat(target_test_uptake_no_bins,pred_RF_grouped_uptake_no_bins)\n",
        "print('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_uptake_no_bins.score(population_data_test_uptake_no_bins, target_test_uptake_no_bins)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uorF1bc061CA"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm = PermutationImportance(my_model_RF_grouped_uptake_no_bins).fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "eli5.show_weights(perm, feature_names = population_data_uptake_no_bins.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_population_rf_uptake_no_bins_500' # name the model\n",
        "pickle.dump(my_model_RF_grouped_uptake_no_bins, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "P9RO-fWOXtl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "Duazq4QddTqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "mrfJ2XosdZ3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWyX2yfRYB_o"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_uptake_no_bins = XGBClassifier(n_jobs=-1)\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_uptake_no_bins,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "6pu_lrQ0dqmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "xgboost_classification_uptake_no_bins = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "scores = cross_val_score(xgboost_classification_uptake_no_bins, population_data_uptake_no_bins, target_uptake_no_bins, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "ABIK_PBLYB_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DD6R-iM61CB"
      },
      "outputs": [],
      "source": [
        "# fitting thhe model on all thhe training data\n",
        "my_model_xgboost_grouped_uptake_no_bins = xgboost_classification_uptake_no_bins.fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "print('xgboost training Score: {0:2.5f}'.format(my_model_xgboost_grouped_uptake_no_bins.score(population_data_uptake_no_bins, target_uptake_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTch5HCK61CB"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost_uptake_no_bins = my_model_xgboost_grouped_uptake_no_bins.predict(population_data_test_uptake_no_bins)\n",
        "heatconmat(target_test_uptake_no_bins,pred_xgboost_uptake_no_bins)\n",
        "print('xgboost testingScore: {0:2.5f}'.format(my_model_xgboost_grouped_uptake_no_bins.score(population_data_test_uptake_no_bins, target_test_uptake_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtRsWyVF61CB"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_xgboost_uptake_no_bins = PermutationImportance(my_model_xgboost_grouped_uptake_no_bins, random_state=1).fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "eli5.show_weights(perm_xgboost_uptake_no_bins, feature_names = population_data_uptake_no_bins.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_population_xgboost_uptake_no_bins_500' # name the model\n",
        "pickle.dump(my_model_xgboost_grouped_uptake_no_bins, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "wmPhh85dbVIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "SxZ2yHnFeqw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "PilGsPCDesfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "PwHFxo3jZnpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1freJUh_ZnpU"
      },
      "outputs": [],
      "source": [
        "# cross validation on the training data\n",
        "SVM_uptake_no_bins = svm.SVC(C = 0.001, gamma = 0.001, kernel='linear')\n",
        "scores = cross_val_score(SVM_uptake_no_bins, population_data_uptake_no_bins, target_uptake_no_bins, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuFqnMOy61CB"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_SVM_grouped_uptake_no_bins = SVM_uptake_no_bins.fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "print('SVM training Score: {0:2.5f}'.format(my_model_SVM_grouped_uptake_no_bins.score(population_data_uptake_no_bins, target_uptake_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znKkyYsh61CC"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM_uptake_no_bins = my_model_SVM_grouped_uptake_no_bins.predict(population_data_test_uptake_no_bins)\n",
        "heatconmat(target_test_uptake_no_bins,pred_SVM_uptake_no_bins)\n",
        "print('SVM testing Score: {0:2.5f}'.format(my_model_SVM_grouped_uptake_no_bins.score(population_data_test_uptake_no_bins, target_test_uptake_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xctbjTcs61CC"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_SVM = PermutationImportance(my_model_SVM_grouped_uptake_no_bins, random_state=1).fit(population_data_uptake_no_bins, target_uptake_no_bins)\n",
        "eli5.show_weights(perm_SVM, feature_names = population_data_uptake_no_bins.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_population_SVM_uptake_no_bins_500' # name the model\n",
        "pickle.dump(my_model_SVM_grouped_uptake_no_bins, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "ahRWLw9yaWlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opening the saved models\n",
        "\n",
        "# RF\n",
        "model_name = 'H460_groupkfold_population_rf_uptake_no_bins_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_rf_uptake_no_bins = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# xgboost\n",
        "model_name = 'H460_groupkfold_population_xgboost_uptake_no_bins_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_xgboost_uptake_no_bins = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# SVM\n",
        "model_name = 'H460_groupkfold_population_SVM_uptake_no_bins_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_SVM_uptake_no_bins = pickle.load(a_file)\n",
        "a_file.close()"
      ],
      "metadata": {
        "id": "d73lkLnybhis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models performance evaluation on new unseen data using the Batch dataset"
      ],
      "metadata": {
        "id": "FNGFVmphgZxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m50J_F561CC"
      },
      "outputs": [],
      "source": [
        "# reading the Batch data\n",
        "Merge_population_batch_uptake_no_bins = pd.read_csv(data_path +'Merge_population_H460_uptake_test_no_bins_500.csv')\n",
        "Merge_population_batch_uptake_no_bins.drop('Unnamed: 0', axis =1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDeuZi3D61CC"
      },
      "outputs": [],
      "source": [
        "# separating the Batch features from the labels\n",
        "cols_batch = [col for col in Merge_population_batch_uptake_no_bins.columns if col not in ['Cell_line']]\n",
        "population_data_uptake_no_bins_batch = Merge_population_batch_uptake_no_bins[cols_batch]\n",
        "# assigning the cell identity column as target\n",
        "target_batch_uptake_no_bins = Merge_population_batch_uptake_no_bins['Cell_line']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest performance"
      ],
      "metadata": {
        "id": "vimF5rCIgxDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d15N9GSq61CC"
      },
      "outputs": [],
      "source": [
        "pred_batch_uptake_no_bins_rf = my_model_population_rf_uptake_no_bins.predict(population_data_uptake_no_bins_batch)\n",
        "heatconmat(target_batch_uptake_no_bins,pred_batch_uptake_no_bins_rf)\n",
        "accuracy = accuracy_score(pred_batch_uptake_no_bins_rf.astype(int), target_batch_uptake_no_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost performance"
      ],
      "metadata": {
        "id": "aRgVKlPbg9sc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuOAI8CA61CC"
      },
      "outputs": [],
      "source": [
        "pred_batch_uptake_no_bins_xgboost = my_model_population_xgboost_uptake_no_bins.predict(population_data_uptake_no_bins_batch)\n",
        "heatconmat(target_batch_uptake_no_bins,pred_batch_uptake_no_bins_xgboost)\n",
        "accuracy = accuracy_score(pred_batch_uptake_no_bins_xgboost.astype(int), target_batch_uptake_no_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM performance"
      ],
      "metadata": {
        "id": "b2cSvEUshKSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5xvatl861CD"
      },
      "outputs": [],
      "source": [
        "pred_batch_uptake_no_bins_SVM = my_model_population_SVM_uptake_no_bins.predict(population_data_uptake_no_bins_batch)\n",
        "heatconmat(target_batch_uptake_no_bins,pred_batch_uptake_no_bins_SVM)\n",
        "accuracy = accuracy_score(pred_batch_uptake_no_bins_SVM.astype(int), target_batch_uptake_no_bins.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimention reduction -PCA"
      ],
      "metadata": {
        "id": "-szOMS4YhaWZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUDPml3ltwgj"
      },
      "outputs": [],
      "source": [
        "# rescale the data\n",
        "train_data_rescaled_uptake_no_bins = scaler.fit_transform(population_data_uptake_no_bins)\n",
        "test_data_rescaled_uptake_no_bins = scaler.fit_transform(population_data_test_uptake_no_bins)\n",
        "batch_data_rescaled_uptake_no_bins = scaler.fit_transform(population_data_uptake_no_bins_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iurwWL4Ctwgj"
      },
      "outputs": [],
      "source": [
        "# displaying the componantes and the data varience they includes for choosing the number of componnets to reduced to\n",
        "cov_mat = np.cov(train_data_rescaled_uptake_no_bins.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "# plot explained variances\n",
        "plt.bar(range(1,11), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,11), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_EBGKrEtwgj"
      },
      "outputs": [],
      "source": [
        "pca_uptake_no_bins = PCA(n_components=2) # choosing the number of dimentions\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca_uptake_no_bins = pca_uptake_no_bins.fit_transform(train_data_rescaled_uptake_no_bins)\n",
        "X_test_pca_uptake_no_bins = pca_uptake_no_bins.transform(test_data_rescaled_uptake_no_bins)\n",
        "X_batch_pca_uptake_no_bins = pca_uptake_no_bins.transform(batch_data_rescaled_uptake_no_bins)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OPqVd-dU_Xz"
      },
      "outputs": [],
      "source": [
        "# How much each original feature contribute to the components\n",
        "\n",
        "print(pd.DataFrame(np.abs(pca_uptake_no_bins.components_),columns=population_data_uptake_no_bins.columns ,index = ['PC-1','PC-2'])) # adjust the number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS0H58I3U_X0"
      },
      "outputs": [],
      "source": [
        "# printing the most contributing original feature for each component\n",
        "most_important = np.abs(pca_uptake_no_bins.components_).argmax(axis=1)\n",
        "initial_feature_names = population_data_uptake_no_bins.columns\n",
        "most_important_names = initial_feature_names[most_important]\n",
        "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(2)} # adjust the number of components\n",
        "pca_results = pd.DataFrame(dic.items())\n",
        "print(pca_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "3jTi0Ejuiupk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "ME1sGKLNhxTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_uptake_no_bins_pca= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_uptake_no_bins_pca,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(X_train_pca_uptake_no_bins, target_uptake_no_bins)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "GXFueAuLciZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model using best parameters results"
      ],
      "metadata": {
        "id": "dZwB0uTsiIUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the modeln and evaluating its performance on the testing data\n",
        "random_forest_grouped_uptake_no_bins_pca = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "my_model_RF_PCA_uptake_no_bins = random_forest_grouped_uptake_no_bins_pca.fit(X_train_pca_uptake_no_bins, target_uptake_no_bins)\n",
        "print('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_PCA_uptake_no_bins.score(X_test_pca_uptake_no_bins, target_test_uptake_no_bins)))"
      ],
      "metadata": {
        "id": "aJ5KadS1ciZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QQj44O_twgk"
      },
      "outputs": [],
      "source": [
        "# evaluating the model peerformance on the Batch data\n",
        "pred_random_forest_uptake_no_bins_PCA_batch = my_model_RF_PCA_uptake_no_bins.predict(X_batch_pca_uptake_no_bins)\n",
        "accuracy = accuracy_score(pred_random_forest_uptake_no_bins_PCA_batch.astype(int), target_batch_uptake_no_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "SvVlyFLIin9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "X1Q-bj5bi2uq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy22z8OOhOYu"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_uptake_no_bins_pca = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_uptake_no_bins_pca,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(X_train_pca_uptake_no_bins, target_uptake_no_bins)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "emDzUaO_jGfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the modeln and evaluating its performance on the testing data\n",
        "xgboost_classification_uptake_no_bins_pca = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "my_model_xgboost_PCA_uptake_no_bins = xgboost_classification_uptake_no_bins_pca.fit(X_train_pca_uptake_no_bins, target_uptake_no_bins)\n",
        "print('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_PCA_uptake_no_bins.score(X_test_pca_uptake_no_bins, target_test_uptake_no_bins)))"
      ],
      "metadata": {
        "id": "vcEGwKn5hOYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY3xbSNXtwgk"
      },
      "outputs": [],
      "source": [
        "# evaluating the model peerformance on the Batch data\n",
        "pred_xgboost_uptake_no_bins_PCA_batch = xgboost_classification_uptake_no_bins_pca.predict(X_batch_pca_uptake_no_bins)\n",
        "accuracy = accuracy_score(pred_xgboost_uptake_no_bins_PCA_batch.astype(int), target_batch_uptake_no_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "7RDG4yMAjZhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "emkiQXKmjcSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(X_train_pca_uptake_no_bins, target_uptake_no_bins)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "SPMl7Yxoif_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "kN2GHKvajs3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtSRXSczif_j"
      },
      "outputs": [],
      "source": [
        "# training the modeln and evaluating its performance on the testing data\n",
        "SVM_uptake_no_bins = svm.SVC(C = 0.001, gamma = 0.001, kernel='linear')\n",
        "my_model_SVM_PCA_uptake_no_bins = SVM_uptake_no_bins.fit(X_train_pca_uptake_no_bins, target_uptake_no_bins)\n",
        "print('SVM testing Score: {0:2.5f}'.format(my_model_SVM_PCA_uptake_no_bins.score(X_test_pca_uptake_no_bins, target_test_uptake_no_bins)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jikUwsFItwgk"
      },
      "outputs": [],
      "source": [
        "# evaluating the model peerformance on the Batch data\n",
        "pred_SVM_uptake_no_bins_PCA_batch = my_model_SVM_PCA_uptake_no_bins.predict(X_batch_pca_uptake_no_bins)\n",
        "accuracy = accuracy_score(pred_SVM_uptake_no_bins_PCA_batch.astype(int), target_batch_uptake_no_bins.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All Features - includes physiological (A,W,H) and uptkae (A) features (22)"
      ],
      "metadata": {
        "id": "DLesyVIClHPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing the data"
      ],
      "metadata": {
        "id": "LvuW3X5cmDIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training dataset\n",
        "Merge_population_all = Merge_population_uptake_no_bins\n",
        "Merge_population_all.drop('Cell_line', axis = 1, inplace = True)\n",
        "Merge_population_all = pd.concat([Merge_population_all, Merge_population_phys_no_bins], axis =1)"
      ],
      "metadata": {
        "id": "QwYzg1Ceqg0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not run if the data includes only one sample per population\n",
        "# testing dataset\n",
        "Merge_population_test_all = Merge_population_test_uptake_no_bins\n",
        "Merge_population_test_all.drop(['Cell_line'], axis = 1, inplace = True)\n",
        "Merge_population_test_all = pd.concat([Merge_population_test_all, Merge_population_test_phys_no_bins], axis = 1)"
      ],
      "metadata": {
        "id": "c2Ouj5SXqe7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZH3vKwZ61B6"
      },
      "outputs": [],
      "source": [
        "# separating the training features from the labels\n",
        "cols = [col for col in Merge_population_all.columns if col not in ['Cell_line']]\n",
        "population_data_all = Merge_population_all[cols] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_all = Merge_population_all['Cell_line'] # labels\n",
        "target_group = ['0', '1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if the data includes only one sample per population\n",
        "\n",
        "## split to training and testing\n",
        "#X_train, X_test, y_train, y_test = train_test_split(All_data_train, target, test_size=0.3, random_state=42) # make sure to change names as needed"
      ],
      "metadata": {
        "id": "fu6-i1bR1LqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrLUot8g61B6"
      },
      "outputs": [],
      "source": [
        "# do not run if the data includes only one sample per population\n",
        "# separating the testing features from the labels\n",
        "cols_test = [col for col in Merge_population_test_all.columns if col not in ['Cell_line']]\n",
        "population_data_test_all = Merge_population_test_all[cols_test] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_test_all = Merge_population_test_all['Cell_line']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "kSduUaB6mr1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "edCNd9QgmwAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_all= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_all,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(population_data_all, target_all)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "f9vNO3dhPCVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "XibK3l3UnL-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "random_forest_grouped_all = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "scores = cross_val_score(random_forest_grouped_all, population_data_all, target_all, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "iOB6wTdwPCVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE2Vu-7R9uXJ"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_RF_grouped_all = random_forest_grouped_all.fit(population_data_all, target_all)\n",
        "print('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_all.score(population_data_all, target_all)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MwIsAIF61B6"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_RF_grouped_all = my_model_RF_grouped_all.predict(population_data_test_all)\n",
        "heatconmat(target_test_all,pred_RF_grouped_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkSjWast61B6"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "perm = PermutationImportance(my_model_RF_grouped_all).fit(population_data_all, target_all)\n",
        "eli5.show_weights(perm, feature_names = population_data_all.columns.tolist())\n",
        "print('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_grouped_all.score(population_data_test_all, target_test_all)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_rf_all_no_bins_500' # name the model\n",
        "pickle.dump(my_model_RF_grouped_all, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "k3aFiV8bQ4ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "uZ8bDW51nzms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "w6-8XHqbn3L9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4P2bYsIW_MT"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_all = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_all,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(population_data_all, target_all)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "YZ4pVyvTwhQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation on the training data\n",
        "xgboost_classification_all = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "scores_xg = cross_val_score(xgboost_classification_all, population_data_all, target_all, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores_xg)*100, std(scores)*100))"
      ],
      "metadata": {
        "id": "xdaWJxT4W_MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw6csTtM61B6"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_xgboost_grouped_all = xgboost_classification_all.fit(population_data_all, target_all)\n",
        "print ('xgboost training Score: {0:2.5f}'.format(my_model_xgboost_grouped_all.score(population_data_all, target_all)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VphdkC4A61B7"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_xgboost_all = my_model_xgboost_grouped_all.predict(population_data_test_all)\n",
        "heatconmat(target_test_all,pred_xgboost_all)\n",
        "print ('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_grouped_all.score(population_data_test_all, target_test_all)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR78Ezlv61B7"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_xgboost_all = PermutationImportance(my_model_xgboost_grouped_all, random_state=1).fit(population_data_all, target_all)\n",
        "eli5.show_weights(perm_xgboost_all, feature_names = population_data_all.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_xgboost_all_no_bins_500' # name the model\n",
        "pickle.dump(my_model_xgboost_grouped_all, open(data_path+model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "A0AWXELbRQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "FftkV1ABxF0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "S8gYZaCZxH82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(population_data_all, target_all)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "C7cQ5_zcXc2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "BK0KpYHgxYB8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ4yJ33261B7"
      },
      "outputs": [],
      "source": [
        "# cross validation on the training data\n",
        "SVM_all = svm.SVC(C = 0.001, gamma = 0.001, kernel='linear')\n",
        "scores = cross_val_score(SVM_all, population_data_all, target_all, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Cross validation Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q5wKVWS61B7"
      },
      "outputs": [],
      "source": [
        "# fitting the model on all the training data\n",
        "my_model_SVM_grouped_all = SVM_all.fit(population_data_all, target_all)\n",
        "print ('SVM training tScore: {0:2.5f}'.format(my_model_SVM_grouped_all.score(population_data_all, target_all)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWvs2NqB61B7"
      },
      "outputs": [],
      "source": [
        "# prediction and visualization\n",
        "pred_SVM_all = my_model_SVM_grouped_all.predict(population_data_test_all)\n",
        "heatconmat(target_test_all,pred_SVM_all)\n",
        "print('SVM testing Score: {0:2.5f}'.format(my_model_SVM_grouped_all.score(population_data_test_all, target_test_all)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3GPJyUq61B7"
      },
      "outputs": [],
      "source": [
        "# permutation analysis to find features importance\n",
        "perm_SVM = PermutationImportance(my_model_SVM_grouped_all, random_state=1).fit(population_data_all, target_all)\n",
        "eli5.show_weights(perm_SVM, feature_names = population_data_all.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model as a pickle\n",
        "model_name = 'H460_groupkfold_populations_SVM_all_no_bins_500' # name the model\n",
        "pickle.dump(my_model_SVM_grouped_all, open(data_path+ model_name+'.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "zyT6NN19RWvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opening the saved models\n",
        "\n",
        "# RF\n",
        "model_name = 'H460_groupkfold_populations_rf_all_no_bins_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_rf_all = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# xgboost\n",
        "model_name = 'H460_groupkfold_populations_xgboost_all_no_bins_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_populatin_xgboost_all = pickle.load(a_file)\n",
        "a_file.close()\n",
        "\n",
        "# SVM\n",
        "model_name = 'H460_groupkfold_populations_SVM_all_no_bins_500' # name the model\n",
        "a_file = open(data_path+model_name+'.pkl', \"rb\")\n",
        "my_model_population_SVM_all = pickle.load(a_file)\n",
        "a_file.close()"
      ],
      "metadata": {
        "id": "sgDUovQ8Ra38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCw28eGK61B8"
      },
      "source": [
        "# new experiment testing - Batch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om-scm7I61B8"
      },
      "outputs": [],
      "source": [
        "# reading the Batch data (for this example the batch data was already preprocessed)\n",
        "Merge_population_batch_all = pd.read_csv(data_path +'Merge_population_all_H460_batch_no_nps_500.csv')\n",
        "Merge_population_batch_all.drop('Unnamed: 0', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUjCRerQ61B8"
      },
      "outputs": [],
      "source": [
        "# separating the Batch features from the labels\n",
        "cols_batch = [col for col in Merge_population_batch_all.columns if col not in ['Cell_line']]\n",
        "population_data_all_batch = Merge_population_batch_all[cols_batch] # features\n",
        "\n",
        "# assigning the cell identity column as target\n",
        "target_batch_all = Merge_population_batch_all['Cell_line']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest performance"
      ],
      "metadata": {
        "id": "2z8DX3LMy_OE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vYTn60I61B8"
      },
      "outputs": [],
      "source": [
        "pred_batch_all_rf = my_model_population_rf_all.predict(population_data_all_batch)\n",
        "heatconmat(target_batch_all,pred_batch_all_rf)\n",
        "accuracy = accuracy_score(pred_batch_all_rf.astype(int), target_batch_all.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost performance"
      ],
      "metadata": {
        "id": "UoSXbjZMzPgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKfN6b7_61B8"
      },
      "outputs": [],
      "source": [
        "pred_batch_all_xgboost = my_model_populatin_xgboost_all.predict(population_data_all_batch)\n",
        "heatconmat(target_batch_all,pred_batch_all_xgboost)\n",
        "accuracy = accuracy_score(pred_batch_all_xgboost.astype(int), target_batch_all.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM performace"
      ],
      "metadata": {
        "id": "Ug_3XiwJzs38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm32_Qbq61B9"
      },
      "outputs": [],
      "source": [
        "pred_batch_all_SVM = my_model_population_SVM_all.predict(population_data_all_batch)\n",
        "heatconmat(target_batch_all,pred_batch_all_SVM)\n",
        "accuracy = accuracy_score(pred_batch_all_SVM.astype(int), target_batch_all.astype(int))\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dimention reduction -PCA"
      ],
      "metadata": {
        "id": "HL52qxLHz8W-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rBFrM1un5fW"
      },
      "outputs": [],
      "source": [
        "# rescale the data for pca\n",
        "train_data_rescaled_all = scaler.fit_transform(population_data_all)\n",
        "test_data_rescaled_all = scaler.fit_transform(population_data_test_all)\n",
        "batch_data_rescaled_all = scaler.fit_transform(population_data_all_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7x5PuFYn5fX"
      },
      "outputs": [],
      "source": [
        "# displaying the componantes and the data varience they includes for choosing the number of componnets to reduced to\n",
        "cov_mat = np.cov(train_data_rescaled_all.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "# plot explained variances\n",
        "plt.bar(range(1,23), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,23), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q5dLMmOn5fX"
      },
      "outputs": [],
      "source": [
        "pca_all = PCA(n_components=2) # choosing the number of dimentions\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca_all = pca_all.fit_transform(train_data_rescaled_all)\n",
        "X_test_pca_all = pca_all.transform(test_data_rescaled_all)\n",
        "X_batch_pca_all = pca_all.transform(batch_data_rescaled_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGhEKagHPBWm"
      },
      "outputs": [],
      "source": [
        "# How much each original feature contribute to the components\n",
        "\n",
        "print(pd.DataFrame(np.abs(pca_all.components_),columns=population_data_all.columns ,index = ['PC-1', 'PC-2'])) # adjust the number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsEkdu1yPBWm"
      },
      "outputs": [],
      "source": [
        "# printing the most contributing original feature for each component\n",
        "most_important = np.abs(pca_all.components_).argmax(axis=1)\n",
        "initial_feature_names = population_data_all.columns\n",
        "most_important_names = initial_feature_names[most_important]\n",
        "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(2)} # adjust the number of components\n",
        "pca_results = pd.DataFrame(dic.items())\n",
        "print(pca_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "JyN24Azr0V6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "6iYFj5QW0ana"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_grouped_all_pca= RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [1000],\n",
        "'min_samples_leaf': [1, 5, 10],\n",
        "'max_depth': [4, 6, 8, 10, 12],\n",
        "'max_features': ['auto', 'sqrt'],\n",
        "'bootstrap': [True, False]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "model_gridsearch = GridSearchCV(\n",
        "estimator=random_forest_grouped_all_pca,\n",
        "param_grid=param_grid,\n",
        "scoring='accuracy',\n",
        "n_jobs=4,\n",
        "cv=cv,\n",
        "refit=True,\n",
        "return_train_score=True)\n",
        "\n",
        "# Fit the selected model\n",
        "model_gridsearch.fit(X_train_pca_all, target_all)\n",
        "print(model_gridsearch.best_params_)"
      ],
      "metadata": {
        "id": "mpsrqBEsX7-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "FhqI-ndi0zBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "random_forest_grouped_all_pca = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, n_estimators = 1000, n_jobs=-1)\n",
        "my_model_RF_PCA_all = random_forest_grouped_all_pca.fit(X_train_pca_all, target_all)\n",
        "print ('Random Forest testing Score: {0:2.5f}'.format(my_model_RF_PCA_all.score(X_test_pca_all, target_test_all)))"
      ],
      "metadata": {
        "id": "WTvu9qhTX7-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrmDvSWrn5fX"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_random_forest_all_PCA_batch = my_model_RF_PCA_all.predict(X_batch_pca_all)\n",
        "accuracy = accuracy_score(pred_random_forest_all_PCA_batch.astype(int), target_batch_all.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "8n2mVKkA1L7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "fGLI95wm1Piq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O2LlgDWYZck"
      },
      "outputs": [],
      "source": [
        "xgboost_classification_all_pca = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "PARAMETERS = {\"subsample\":[0.5, 0.75, 1],\n",
        "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
        "              \"max_depth\":[2, 6, 12],\n",
        "              \"min_child_weight\":[1,5,15],\n",
        "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
        "              'n_estimator' : [500]}\n",
        "\n",
        "model_gs = GridSearchCV(xgboost_classification_all_pca,param_grid=PARAMETERS,cv=cv,scoring=\"accuracy\")\n",
        "model_gs.fit(X_train_pca_all, target_all)\n",
        "print(model_gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "TP5WFbYK1oiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "xgboost_classification_all_pca = XGBClassifier(colsample_bytree = 0.5, learning_rate = 0.3, max_depth = 2, min_child_weight = 1, n_estimator = 500, subsample = 0.5, n_jobs=-1)\n",
        "my_model_xgboost_PCA_all = xgboost_classification_all_pca.fit(X_train_pca_all, target_all)\n",
        "print('xgboost testing Score: {0:2.5f}'.format(my_model_xgboost_PCA_all.score(X_test_pca_all, target_test_all)))"
      ],
      "metadata": {
        "id": "FZeoW-fnYZcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnOGhHdRn5fX"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_xgboost_all_PCA_batch = my_model_xgboost_PCA_all.predict(X_batch_pca_all)\n",
        "accuracy = accuracy_score(pred_xgboost_all_PCA_batch.astype(int), target_batch_all.astype(int))\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "g_saU6fp19bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "8xN6gc8-2AUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'sigmoid']}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=cv)\n",
        "grid_search.fit(X_train_pca_all, target_all)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "zM6RVMjLP29s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model using best parameters results"
      ],
      "metadata": {
        "id": "cYd5k07p2SPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsU0q1DvP29v"
      },
      "outputs": [],
      "source": [
        "# training the model and evaluating its performance on the testing data\n",
        "SVM_all = svm.SVC(C = 0.001, gamma = 0.001, kernel='linear')\n",
        "my_model_SVM_PCA_all = SVM_all.fit(X_train_pca_all, target_all)\n",
        "print('SVM testing Score: {0:2.5f}'.format(my_model_SVM_PCA_all.score(X_test_pca_all, target_test_all)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgOlu5Mhn5fY"
      },
      "outputs": [],
      "source": [
        "# evaluating the model performance on the Batch data\n",
        "pred_SVM_all_PCA_batch = my_model_SVM_PCA_all.predict(X_batch_pca_all)\n",
        "accuracy = accuracy_score(pred_SVM_all_PCA_batch.astype(int), target_batch_all.astype(int))\n",
        "accuracy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}